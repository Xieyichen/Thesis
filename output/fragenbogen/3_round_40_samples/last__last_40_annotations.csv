,abstract,title,bart_base,bart_cnn,bart_xsum,t5,gpt2,pegasus_xsum
0,"As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",Heterogeneous Graph Neural Networks for Extractive Document Summarization<original>,<TITLE>Heterogeneous Graph-based Neural Network for Extractive Document Summarization<TITLE>Heterogeneous Graph-based Neural Networks for Extractive Document Summarization<TITLE>Heterogeneous Graph-Based Neural Network for Extractive Document Summarization<TITLE>Heterogeneous Graph-Based Neural Networks for Extractive Document Summarization<TITLE>Graph-based Neural Network for Extractive Document Summarization<bart_base>,<TITLE>HETERSUMGRAPH: Heterogeneous Graph-based Neural Network for Extractive Document Summarization with Cross-<TITLE>HETERSUMGRAPH: Heterogeneous Graph-based Neural Network for Extractive Document Summarization with Cross-Sent<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-based Neural Network for Extractive Document Summarization with Cross<TITLE>HETERSUMGRAPH: Heterogeneous Graph-based Neural Network for Extractive Document Summarization with Cross-Sentence<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-based Neural Network for Extractive Document Summarization with Cross-<bart_cnn>,<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-based Neural Network for Extractive Document Summarization<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-Based Neural Network for Extractive Document Summarization<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-based Neural Network for Extractive Document Summarization<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-Based Neural Network for Extractive Document Summarization<TITLE>HETERSUMGRAPH: A Heterogeneous Graph-based Neural Network for Extractive Document Summarization<bart_xsum>,<TITLE>Heterogeneous Graph-Based Neural Network for Extractive Document Summarization<TITLE>Heterogeneous Graph-based Neural Network for Extractive Document Summarization<TITLE>Heterogeneous Graph-Based Neural Networks for Extractive Document Summary<TITLE>Heterogeneous Graph-Based Neural Network for Extractive Document Summary<TITLE>Graph-Based Neural Networks for Extractive Document Summarization<t5_small>,<TITLE> Heterogeneous Graph-based Neural Network for Extractive Document Summarization<TITLE> Heterogeneous Graph-based Neural Network for Extractive Summarization<TITLE> Heterogeneous Graph-based Neural Networks for Extractive Document Summarization<TITLE> Heterogeneous Graph-Based Neural Network for Extractive Document Summarization<TITLE> Heterogeneous Graph-Based Neural Network for Extractive Summarization<gpt2>,<TITLE>Heterogeneous Graph-based Neural Network for Extractive Document Summarization<TITLE>Heterogeneous Graph-Based Neural Network for Extractive Document Summarization<TITLE>Heterogeneous Graph-based Neural Networks for Extractive Document Summarization<TITLE>Heterogeneous Graph-Based Neural Networks for Extractive Document Summarization<TITLE>Heterogeneous Graph-based Neural Network for Extractive Document Segmentation<pegasus_xsum>
1,"Activities and events in our lives are structural, be it a vacation, a camping trip, or a wedding. While individual details vary, there are characteristic patterns that are specific to each of these scenarios. For example, a wedding typically consists of a sequence of events such as walking down the aisle, exchanging vows, and dancing. In this paper, we present a data-driven approach to learning event knowledge from a large collection of photo albums. We formulate the task as constrained optimization to induce the prototypical temporal structure of an event, integrating both visual and textual cues. Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content.",Learning Prototypical Event Structure from Photo Albums<original>,<TITLE>Learning Multimodal Event Structure from Noisy Web Content<TITLE>Learning Multimodal Event Structure from Noisy Photographs<TITLE>Learning Multimodal Event Structure from Photo Albums<TITLE>Learning Multimodal Knowledge of Events from Photographs<TITLE>Learning Multimodal Event Structure from Photographs<bart_base>,<TITLE>Learning Event Knowledge from Noisy Web Content using Prototypical Temporality Constrained Optimization and Data-Driven<TITLE>Learning Event Knowledge from Noisy Web Content using Prototypical Temporality Constrained Optimization and Data-driven Learning<TITLE>Learning Event Knowledge from Noisy Web Content using Prototypical Temporality Constrained Optimization in Photograph Albums.<TITLE>Learning Event Knowledge from Noisy Web Content using Prototypical Temporality Constrained Optimization in Photo Albums.<TITLE>Learning Event Knowledge from Noisy Web Content using Prototypical Temporality Constrained Optimization and Data-driven Struct<bart_cnn>,<TITLE>Learning Temporally Structured Events from Large-Scale Photo Albums<TITLE>Learning Temporally Structured Event Structure from Large-Scale Photographs<TITLE>Learning Temporally Structured Event Structure from Large-Scale Photograph Collection<TITLE>Learning Temporally Structured Event Structure from Large-Scale Photograph Collections<TITLE>Learning Temporally Structured Events from Large-Scale Photo Albums<bart_xsum>,<TITLE>Learning Event Structure from Large Collection of Photo Albums<TITLE>Multimodal Learning of Event Structure from Photo Albums<TITLE>Learning Event Structure from a Large Collection of Photos<TITLE>Data-Driven Learning of Event Structure<TITLE>Learning Event Structure from Large Collection of Photos<t5_small>,<TITLE> Learning Event Knowledge from Photograph Albums<TITLE> Learning Event Knowledge from Photographs<TITLE> Learning Temporal Event Knowledge from Photograph Albums<TITLE> Learning Temporal Event Knowledge from Photographs<TITLE> Learning Event Knowledge from Large Collections of Photo Albums<gpt2>,<TITLE>Learning Multimodal Knowledge of Event Structure from Photo Album Collections<TITLE>Data-Driven Learning of Event Structure from Photo Album Collections<TITLE>Learning Multimodal Knowledge of Event Structure from Photo Album Collection<TITLE>Learning Multimodal Knowledge of Event Structure from Photo Album Content<TITLE>Learning Temporal Structure of Events from Photo Album Collections<pegasus_xsum>
2,"In contrast to the standard classification paradigm where the true class is given to each training pattern, complementary-label learning only uses training patterns each equipped with a complementary label, which only specifies one of the classes that the pattern does not belong to. The goal of this paper is to derive a novel framework of complementary-label learning with an unbiased estimator of the classification risk, for arbitrary losses and models—all existing methods have failed to achieve this goal. Not only is this beneficial for the learning stage, it also makes model/hyper-parameter selection (through crossvalidation) possible without the need of any ordinarily labeled validation data, while using any linear/non-linear models or convex/non-convex loss functions. We further improve the risk estimator by a non-negative correction and gradient ascent trick, and demonstrate its superiority through experiments.",Complementary-Label Learning for Arbitrary Losses and Models<original>,<TITLE>Non-negative Correction and Gradient Ascent for Non-negative Classification<TITLE>Non-negative Correction and Gradient Ascent for Contrastive Label Learning<TITLE>Non-negative Correction and Gradient Ascent for Approximate Classification<TITLE>Non-negative Correction and Gradient ascent for complementary-label learning<TITLE>Non-negative Correction and Gradient ascent for complementary-label classification<bart_base>,<TITLE>Combining Training and Cross-Validation with Unbiased Estimation of the Classification Risk: A Case Study on Complementary<TITLE>Combining Training and Cross-Validation with Unbiased Estimation of the Classification Risk: A Case Study in Complementary<TITLE>Combining Training and Cross-Validation with Unbiased Estimation of the Classification Risk for Losses and Models: A Case<TITLE>Combining Training and Cross-Validation with Unbiased Estimation of the Classification Risk for Losses and Models in Complement<TITLE>Combining Training and Cross-Validation with Unbiased Estimation of the Classification Risk for Losses and Models: An Application<bart_cnn>,<TITLE>An Unbiased Risk Estimation for Complementary-Label Learning<TITLE>An Unbiased Risk Estimation for Complementary-label Learning<TITLE>Risk Estimation for Complementary-Label Learning<TITLE>Risk Estimation for Complementary-label Learning<TITLE>Risk Estimation for Complementary-Label Learning<bart_xsum>,<TITLE>Comparative-Label Learning with a Non-negative Correction and Gradient Ascent<TITLE>Non-negative correction and gradient ascent trick for complementary-label learning.<TITLE>A Non-negative correction and gradient ascent trick for complementary-label learning<TITLE>Non-negative correction and gradient ascent trick for complementary-label learning<TITLE>Non-negative correction and gradient ascent trick in complementary-label learning<t5_small>,<TITLE> A Non-Linear Approach to Complementary-Label Learning<TITLE> A Non-Linear Optimization Framework for Complementary-Label Learning<TITLE> A Non-Linear Model-Based Approach to Complementary-Label Learning<TITLE> A Non-Linear Approach to Complementarity-Label Learning<TITLE> A Non-Linear Approach to Complementary-Label Learning with Optimal Loss Functions<gpt2>,<TITLE>Cross-validation of complementary-label learning with an unbiased risk estimator<TITLE>Cross-validation of complementary-label learning with an unbiased loss estimator<TITLE>Cross-validation of complementary-label learning with an unbiased estimator<TITLE>Risk Estimation in complementary-label learning with crossvalidation<TITLE>Cross-validation of complementary-label learning<pegasus_xsum>
3,"Sequence-to-sequence (seq2seq) models have achieved tremendous success in text generation tasks. However, there is no guarantee that they can always generate sentences without grammatical errors. In this paper, we present a preliminary empirical study on whether and how much automatic grammatical error correction can help improve seq2seq text generation. We conduct experiments across various seq2seq text generation tasks including machine translation, formality style transfer, sentence compression and simplification. Experiments show the state-of-the-art grammatical error correction system can improve the grammaticality of generated text and can bring task-oriented improvements in the tasks where target sentences are in a formal style.",Automatic Grammatical Error Correction for Sequence-to-sequence Text Generation: An Empirical Study<original>,<TITLE>How much Grammatical Error Correction Can Improve Sequence-to-Sequence Text Generation?<TITLE>How much Grammatical Error Correction can Improve Sequence-to-Sequence Text Generation?<TITLE>How Much Grammatical Error Correction Can Improve Sequence-to-Sequence Text Generation?<TITLE>Grammatical Error Correction Improves Sequence-to-Sequence Text Generation Tasks<TITLE>How much Grammatical Error Correction Can Improve Sequence-to-Sequence Generation?<bart_base>,<TITLE>Improving Text Generation via Grammatical Error Correction and Task-Oriented Generation of Sentences without Grammatical Errors Using Se<TITLE>Improving Text Generation via Grammatical Error Correction and Task-Oriented Generation of Sentences without Grammatical Errors Using Automatic<TITLE>Improving Text Generation via Grammatical Error Correction and Task-Oriented Generation of Sentences without Grammatical Signals.<TITLE>Improving Text Generation via Grammatical Error Correction and Task-Oriented Generation of Sentences without Grammatical Errors Using a<TITLE>Improving Text Generation via Grammatical Error Correction and Task-Oriented Generation of Sentences without Grammatical Errors Using Form<bart_cnn>,<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Text Generation<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Text Generation<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Models<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Text Generation<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Text Generation<bart_xsum>,<TITLE>Automatic Grammatical Error Correction for Seq2seq Text Generation<TITLE>Automatic Grammatical Error Correction in Seq2seq Text Generation<TITLE>Automatic Grammatical Error Correction for Seq2Seq Text Generation<TITLE>Grammatical Error Correction for Seq2seq Text Generation<TITLE>Automatic Grammatical Error Correction for Text Generation<t5_small>,<TITLE> Improving Text Generation with Grammatical Error Correction<TITLE> Grammatical Error Correction for Sequence-to-Sequence Text Generation<TITLE> Improving Sentence-to-Sequence Text Generation with Grammatical Error Correction<TITLE> Improving Sequence-to-Sequence Text Generation with Grammatical Error Correction<TITLE> Improving Sentence-to-Sequence Text Generation via Grammatical Error Correction<gpt2>,<TITLE>Empirical Study on Grammatical Error Correction for Sequence-to-Sequence Generation<TITLE>Empirical Study on Grammatical Error Correction in Sequence-to-Sequence Generation<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Text Generation<TITLE>Automatic Grammatical Error Correction for Sequence-to-Sequence Text Generation Tasks<TITLE>Empirical Study on Grammatical Error Correction for Sequence to Sequence Text Generation<pegasus_xsum>
4,"Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning. With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.",Analogical Reasoning on Chinese Morphological and Semantic Relations<original>,<TITLE>CA8: An Analogical Reasoning Task on Chinese<TITLE>CA8: An Analogical Reasoning Task for Chinese<TITLE>Analogical Reasoning for Chinese Word Embeddings<TITLE>Analogical Reasoning on Chinese<TITLE>Analogical Reasoning for Chinese<bart_base>,<TITLE>An Analogical Reasoning Task-Oriented Chinese Word Embeddings with Implicit Morphological and Explicit Semantic Relations<TITLE>An Analogical Reasoning Task-Oriented Chinese Word Embeddings: A Sketch of Implicit Morphological Relations and<TITLE>An Analogical Reasoning Task-Oriented Chinese Word Embeddings: Sketching Implicit Morphological Relations and Sem<TITLE>An Analogical Reasoning Task-Oriented Chinese Word Embeddings: A Sketch of Implicit Morphological and Sem<TITLE>An Analogical Reasoning Task-Oriented Chinese Word Embeddings: A Sketch of Implicit Morphological and Explicit<bart_cnn>,<TITLE>Analogical Reasoning in Chinese: A Case Study on Morphological and Semantic Relations<TITLE>Analogical Reasoning in Chinese: A Case Study on Morphology and Semantics<TITLE>Analogical Reasoning in Chinese: A Case Study on Morphology and Semantics<TITLE>Analogical Reasoning in Chinese<TITLE>Analogical Reasoning for Chinese<bart_xsum>,<TITLE>An Analogical Reasoning Task on Chinese<TITLE>An Analogical Reasoning Task for Chinese<TITLE>An Analogical Reasoning task on Chinese<TITLE>Analogical Reasoning for Chinese<TITLE>Analogical Reasoning on Chinese<t5_small>,<TITLE> Chinese Analogical Reasoning with Vector Representations and Context Features<TITLE> Chinese Word Embeddings as Analogical Reasoning<TITLE> Chinese Analogical Reasoning with Vector Representations<TITLE> Chinese Analogical Reasoning<TITLE> Chinese Word Embeddings as Analogical Reasoning Models<gpt2>,<TITLE>CA8: An Analogical Collocation Task on Chinese<TITLE>CA8: An Analogical Argumentation Task on Chinese<TITLE>CA8: An Analogical Collocation Task for Chinese<TITLE>CA8: An Analogical Reasoning Task on Chinese<TITLE>CA8: An Analogical Argument for Chinese<pegasus_xsum>
5,"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard ""mask-predict"" algorithm, and provide analyses of its behavior on machine translation tasks.",Inference Strategies for Machine Translation with Conditional Masking<original>,<TITLE>Tresholding Conditional Masked Language Model Training for Machine Translation<TITLE>A thresholding strategy for conditional masked language model training in machine translation<TITLE>Conditional Masked Language Model Training for Machine Translation<TITLE>A thresholding strategy for conditional masked language model training<TITLE>A thresholding strategy for conditional masked language models<bart_base>,<TITLE>A Thresholding Approach to Conditional Masked Language Modeling for Semi-Autoregressive Sequence Generation with Conditional In<TITLE>A Thresholding Approach to Conditional Masked Language Modeling for Semi-Autoregressive Sequence Generation with Conditional Prob<TITLE>A Thresholding Approach to Conditional Masked Language Modeling for Semi-Autoregressive and Semi-autoregressive<TITLE>A Thresholding Approach to Conditional Masked Language Modeling for Semi-Autoregressive Sequence Generation: An Analysis of<TITLE>A Thresholding Approach to Conditional Masked Language Modeling for Semi-Autoregressive Sequence Generation with Conditional Random<bart_cnn>,<TITLE>Conditional Masked Language Modeling for Non-Autoregressive Sequence Generation<TITLE>Conditional Masked Language Modeling for Non-autoregressive Sequence Generation<TITLE>Masked Inference for Non-Autoregressive Sequence Generation and Machine Translation<TITLE>Masked Inference for Non-Autoregressive Sequence Generation Tasks<TITLE>Masked Inference for Non-Autoregressive Sequence Generation<bart_xsum>,<TITLE>Conditional Masked Language Model Training for Machine Translation<TITLE>Conditional Masked Language Models for Machine Translation<TITLE>Predicting Conditional Masked Language Models<TITLE>Conditional Masked Language Model Training<TITLE>Conditional Masked Language Models<t5_small>,<TITLE> Conditional Masking for Non-Autoregressive Sequence Generation<TITLE> Factorization of Conditional Masking for Non-Autoregressive Sequence Generation<TITLE> Conditional Masking for Non-Autoregressive Sequential Generation<TITLE> Factorization of Conditional Masked Language Model<TITLE> Factorization of Conditional Masked Language Models<gpt2>,<TITLE>Masked Inference for Non-Autoregressive Sequence Generation.<TITLE>Masked Inference for Non-Autoregressive Sequence Generation<TITLE>On the Inference Strategy for Conditional Masked Language Models.<TITLE>On the Inference Strategy for Conditional Masked Language Models<TITLE>On the Effects of Masked Inference on Sequence Generation<pegasus_xsum>
6,"We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system’s error trends.",Prototype-Driven Learning for Sequence Models<original>,<TITLE>Posterior-Driven Learning for Unsupervised Sequence Models<TITLE>Iterative Learning for Semi-Supervised Sequence Modeling<TITLE> prototype-driven learning for unsupervised sequence modeling<TITLE> prototype-driven learning for unsupervised sequence modeling.<TITLE>Iterative Learning for Unsupervised Sequence Modeling<bart_base>,<TITLE>Prototype-Driven Learning for Part-of-Speech Induction in Unsupervised Sequence Modeling using Sparse<TITLE>Prototype-Driven Learning for Part-of-Speech Induction in Unsupervised Sequence Modeling with Sparse<TITLE>Prototype-Driven Learning for Part-of-Speech Induction in Unsupervised Sequence Modeling using Distributional<TITLE>Prototype-Driven Learning for Part-of-Speech Induction in Unsupervised Sequence Modeling Using Sparse<TITLE>Prototype-Driven Learning for Part-of-Speech Induction with Application to Unsupervised Sequence Modeling.<bart_cnn>,<TITLE>Prototype-Driven Unsupervised Sequence Modeling with Distributional Similarity Features<TITLE>Prototype-Driven Learning for Unsupervised Sequence Modeling and Information Extraction<TITLE>Prototype-Driven Learning for Unsupervised Sequence Modeling<TITLE>Prototype-Driven Learning for Unsupervised Sequence Modeling<TITLE>Prototype-Driven Learning for Unsupervised Sequence Modeling<bart_xsum>,<TITLE>Prototype-Driven Learning for primarily Unsupervised Sequence Modeling<TITLE>Prototype-Driven Learning for primarily unsupervised Sequence Modeling<TITLE>Prototype-Driven Learning for primarily Unsupervised Sequence Models<TITLE>Prototype-Driven Learning for primarily Unsupervised Sequence Modelling<TITLE>Prototype-driven Learning for primarily Unsupervised Sequence Modeling<t5_small>,<TITLE> Speeding Up Prototype-Driven Learning for Unsupervised Sequence Modeling<TITLE> Probabilistic Prototype Learning for Unsupervised Sequence Modeling<TITLE> Unsupervised Sequence Modeling with Prototype Knowledge<TITLE> Probabilistic Prototype-Driven Learning for Unsupervised Sequence Models<TITLE> Probabilistic Prototype-Driven Learning for Sequence Modeling<gpt2>,<TITLE>Prototype-Driven Learning for Unsupervised Sequence Modeling<TITLE>Prototype-driven Learning for Unsupervised Sequence Modeling<TITLE>Prototype-Driven Learning for Unsupervised Sequence Models<TITLE>Prototype-Driven Unsupervised Sequence Modeling<TITLE>Prototype-Driven Sequence Modeling<pegasus_xsum>
7,"Different languages might have different word orders. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.",On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing<original>,<TITLE>Cross-Lingual Transfer and Self-Attentive Models<TITLE>Cross-Lingual Transfer and Self-Attentive Architecture<TITLE>Cross-Lingual Transfer of Words to Distant Languages<TITLE>Cross-Lingual Transfer to Distant Languages<TITLE>Cross-Lingual Transfer for Distant Languages<bart_base>,<TITLE>Cross-Lingual Transfer of Order-Agnostic Dependency Parsers with Recurrent Neural Networks and Modified Self-<TITLE>Cross-Lingual Transfer of Order-Agnostic Dependency Parsers to Distant Languages with Recurrent Neural Networks<TITLE>Cross-Lingual Transfer of Order-Agnostic Dependency Parsers to Distantly-Worlded Languages with Rec<TITLE>Cross-Lingual Transfer of Order-Agnostic Dependency Parsers to Distantly-Worlded Languages using Rec<TITLE>Cross-Lingual Transfer of Order-Agnostic Dependency Parsers to Distant Languages with Recurrent Neural Network<bart_cnn>,<TITLE>Cross-Lingual Transferability of Dependency Parsers with Self-Attentive and RNNs<TITLE>Cross-Lingual Transferability of Dependency Parsers: An Orderagnostic Approach<TITLE>Cross-Lingual Transferability of Dependency Parsers with Orderagnostic Architectures<TITLE>Cross-Lingual Transferability of Dependency Parsers to Distant Languages<TITLE>Cross-Lingual Transferability of Dependency Parsers to Foreign Languages<bart_xsum>,<TITLE>Cross-Lingual Transfer with Orderagnostic Architecture<TITLE>Cross-lingual Transfer with Orderagnostic Models<TITLE>Cross-lingual Transfer with Orderagnostic Architectures<TITLE>Crosslingual Transfer with Orderagnostic Models<TITLE>Crosslingual Transfer with Orderagnostic Architectures<t5_small>,<TITLE> Cross-Lingual Transfer and Perturbation of Dependency Parsers<TITLE> Cross-Lingual Transfer and Perturbation in Dependency Parsing<TITLE> Cross-Lingual Transfer and Perturbation for Distant Languages<TITLE> Cross-Lingual Transfer and Perturbation for Multilingual Parsing<TITLE> Cross-Lingual Transfer and Perturbation for Distant Language<gpt2>,<TITLE>Dependency-Based Cross-Lingual Transfer: An Order Agnostic Approach<TITLE>Dependency-based Cross-Lingual Transfer with Order Agnostic Neural Networks<TITLE>Dependency-based Cross-Lingual Transfer with Recurrent Neural Networks<TITLE>Dependency-based Cross-Lingual Transfer of Word Order Models<TITLE>Cross-Lingual Transfer of Dependency Parsers<pegasus_xsum>
8,"We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension — the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.",Beyond Convexity: Stochastic Quasi-Convex Optimization<original>,<TITLE>The Bayes-Consistency of Multiclass Learning.<TITLE>The Bayes-Consistency of Multiclass Learning<TITLE>The Bayes Consistency of Multiclass Learning.<TITLE>The Bayes Consistency of Multiclass Learning<TITLE>Bayes Consistency in Multiclass Learning<bart_base>,<TITLE>Bayes-Consistency of 1-Nearest Neighbor-based Multiclass Learning with Application to Measure-theoret<TITLE>Bayes-Consistency of 1-Nearest Neighbor-based Multiclass Learning Algorithms with Finite Double<TITLE>Bayes-Consistency of 1-Nearest Neighbor-based Multiclass Learning Algorithms with Finite Dou<TITLE>Bayes-Consistency of 1-Nearest Neighbor-based Multiclass Learning with Application to Measure-Theoret<TITLE>Bayes-Consistency of 1-Nearest Neighbor-based Multiclass Learning Algorithms with Finite-<bart_cnn>,<TITLE>Bayes-Consistency of 1-Nearest-Neighbor Sample Compression for Multiclass Learning<TITLE>Bayes-Consistency of 1-Nearest-Neighbor Sample Compression for Multiclass Learning.<TITLE>Bayes-Consistency of 1-Nearest-Neighbor Sample Compression for Multiclass Learning<TITLE>Bayes-Consistency of 1-Nearest-Neighbor Sample Compression Algorithms<TITLE>Bayes-Consistency of 1-Nearest-Neighbor Sample Compression Algorithms<bart_xsum>,<TITLE>Bayes-consistency of a 1-nearest-neighbor-based multiclass learning algorithm<TITLE>Bayes-consistency of nearest-neighbor Sample Compression<TITLE>Bayes-consistency of nearest-neighbor sample compression algorithms<TITLE>Bayes-consistency of nearest-neighbor sample compression<TITLE>Bayes-Consistent Multiclass Learning<t5_small>,<TITLE> Bayes-Consistency of Multiclass Learning<TITLE> Bayes-Consistency of Multiclass Learning Algorithms<TITLE> Bayes-Consistency of Sample Compression Bounds<TITLE> Bayes-Consistency of Multi-Class Learning<TITLE> Bayes-Consistency of Sample Compression Methods<gpt2>,<TITLE>On the Bayes-Consistency of Nearest-Oriented Multiclass Learning.<TITLE>On the Bayes-Consistency of Nearest-Oriented Multiclass Learning Algorithms<TITLE>Bayes-Consistency of a Nearest-Oriented Multiclass Learning Algorithm.<TITLE>Bayes-Consistency of a Nearest-Oriented Multiclass Learning Algorithm<TITLE>On the Bayes-Consistency of Nearest-Oriented Multiclass Learning<pegasus_xsum>
9,"Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.",Discriminative Gaussian Process Latent Variable Models for Classification<original>,<TITLE>Discriminative Latent Variable Models for Gaussian Process Classification<TITLE>Discriminative Latent Space Learning for Gaussian Process Classification<TITLE>Learning Discriminative Latent Space for Gaussian Process Classification<TITLE>Gaussian Process Latent Variable Models for Classification<TITLE>Gaussian Process Latent Variable Models<bart_base>,<TITLE>Discriminative Manifold Learning with Gaussian Process Priors over the Latent Space for Low-Dimensional Man<TITLE>Discriminative Manifold Learning with Gaussian Process Priors over the Latent Space for Low Dimensional Manif<TITLE>Discriminative Manifold Learning with Gaussian Process Priors over the Latent Space for Low-dimensional Manif<TITLE>Discriminative Manifold Learning with Gaussian Process Priors over the Latent Space for Low-Dimension Man<TITLE>Discriminative Manifold Learning with Gaussian Process Priors over the Latent Space using Discriminatively-<bart_cnn>,<TITLE>Gaussian Process Latent Variable Models for Manifold Learning<TITLE>Gaussian Process Latent Variable Models for Manifold Classification<TITLE>Gaussian Process Latent Variable Models for Manifold Learning<TITLE>Gaussian Process Latent Variable Models for Manifold Classification<TITLE>Gaussian Process Classification with Latent Variable Models<bart_xsum>,<TITLE>Gaussian Process Classification using Latent Variable Models<TITLE>Gaussian Process Classification with Latent Variable Models<TITLE>Gaussian Process Classification via Latent Variable Models<TITLE>Gaussian Process Classification with Discriminative Priors<TITLE>Gaussian Process Classification using Discriminative Priors<t5_small>,<TITLE> Discriminative Latent Variables for Gaussian Process Classification<TITLE> Discriminative Latent Variable Models for Gaussian Process Classification<TITLE> Discriminative Latent Variational Autoencoders<TITLE> Discriminative Latent Variable Models<TITLE> Discriminative Latent Variables for Gaussian Process Learning<gpt2>,<TITLE>A Discriminative Latent Space Model for Supervised Process Classification<TITLE>A Discriminative Latent Space Model for Supervised Manifold Learning<TITLE>A Discriminative Latent Space for Supervised Process Classification.<TITLE>A Discriminative Latent Space for Supervised Process Classification<TITLE>Discriminative Latent Manifold Learning for Supervised Process Classification<pegasus_xsum>
10,"In this paper, we propose to learn word embeddings based on the recent fixed-size ordinally forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence into a fixed-size representation. We use FOFE to fully encode the left and right context of each word in a corpus to construct a novel word-context matrix, which is further weighted and factorized using truncated SVD to generate low-dimension word embedding vectors. We evaluate this alternate method in encoding word-context statistics and show the new FOFE method has a notable effect on the resulting word embeddings. Experimental results on several popular word similarity tasks have demonstrated that the proposed method outperforms other SVD models that use canonical count based techniques to generate word context matrices.",Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding<original>,<TITLE>Learning Word Embeddings Based on Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings with Ordinally Forgetting Encoding.<TITLE>Learning Word Embeddings with Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings by Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings using Ordinally Forgetting Encoding<bart_base>,<TITLE>FOFE: Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding and Untruncated SVD<TITLE>FOFE: Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding for Improved Word Similarity Tasks<TITLE>FOFE: Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding of Word Context Matrices for Improved<TITLE>FOFE: Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding of Word Context Matrices for Word<TITLE>FOFE: Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding and Untruncated SVM<bart_cnn>,<TITLE>Learning Word Embeddings Based on Fixed-Size Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings Using Fixed-Size Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings Using Fixed-Size Ordinally Forgetting Encoding Method<TITLE>Learning Word Embeddings Using Fixed-Size Ordinally Forgetting Encoding<bart_xsum>,<TITLE>Learning Word Embeddings Using Fixed-Size Ordinally forgetting Encoding<TITLE>Learning Word Embeddings using Fixed-Size Ordinally forgetting Encoding Method<TITLE>Learning Word Embeddings using Fixed-Size Ordinally forgetting Encoding<TITLE>Learning Word Embeddings from Fixed-Size Ordinally forgetting Encoding<TITLE>Learning Word Embeddings via Fixed-Size Ordinally forgetting Encoding<t5_small>,<TITLE> Learning Word Embeddings with Fixed-Size Ordinally Forgetting Encoding<TITLE> Word Embeddings with Fixed-Size Ordinally Forgetting Encoding<TITLE> Word Embedding with Fixed-Size Ordinally Forgetting Encoding<TITLE> Learning Word Embeddings from Fixed-Size Ordinally Forgetting Encoding<TITLE> Word Embeddings Based on Fixed-Size Ordinally Forgetting Encoding<gpt2>,<TITLE>Learning Word Embeddings Based on Fixed-Size Ordinally Forgetting Parser<TITLE>Learning Word Embeddings Based on Fixed-Size Ordinally Forgetting Annotation<TITLE>Learning Word Embeddings Based on Fixed-Size Ordinally Forgetting Encoding<TITLE>Learning Word Embeddings Using Fixed-Size Ordinally Forgetting Annotation Method<TITLE>Learning Word Embeddings Using Fixed-Size Ordinally Forgetting Parser<pegasus_xsum>
11,"This study aims at identifying when an event written in text occurs. In particular, we classify a sentence for an event into four time-slots; morning, daytime, evening, and night. To realize our goal, we focus on expressions associated with time-slot (time-associated words). However, listing up all the time-associated words is impractical, because there are numerous time-associated expressions. We therefore use a semi-supervised learning method, the Naïve Bayes classifier backed up with the Expectation Maximization algorithm, in order to iteratively extract time-associated words while improving the classifier. We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period. As a result of experiments, the proposed method achieved 0.864 of accuracy and outperformed other methods.",Time Period Identification of Events in Text<original>,<TITLE>Identifying Time-Affected Words Using Naïve Bayes Classifiers<TITLE>Identifying Time-Affected Words Using Naïve Bayes Classifier<TITLE>Identifying Time-Affected Words in Text<TITLE>Identifying Time-Alassociated Words in Text<TITLE>Identifying Time-Aware Words in Text<bart_base>,<TITLE>Semi-supervised Learning for Time-Associative Event Classification using Naïve Bayes Classifier and Expectation Maximization<TITLE>Semi-supervised Learning for Time-Associative Event Recognition Using Naïve Bayes Classifier and Expectation Maxim<TITLE>Semi-supervised Learning for Time-Associative Event Recognition using Naïve Bayes Classifier and Expectation Maxim<TITLE>Semi-supervised Learning for Time-Associative Event Recognition Using Naïve Bayes Classifier and Support Vector Machines<TITLE>Semi-supervised Learning for Time-Associative Event Classification using Naïve Bayes Classifier and Support Vector Machines.<bart_cnn>,<TITLE>Semi-supervised Learning of Time-Slots for Event Detection in Text<TITLE>Semi-Supervised Learning of Time-Slots for Event Detection in Text<TITLE>Semi-supervised Learning of Time-Slots for Event Detection in Text<TITLE>Semi-supervised Learning of Time-Slots for Event Detection<TITLE>Semi-Supervised Learning of Time-Slots for Event Detection<bart_xsum>,<TITLE>A Semi-Supervised Learning Method for Identifying Time-Slot Events<TITLE>A Semi-Supervised Learning Method for Identifying Time-Slots<TITLE>A Semi-Supervised Learning Approach to Time-Slot Detection<TITLE>A Semi-Supervised Learning Approach to Identifying Time-Slots<TITLE>A Semi-Supervised Learning Approach to Identifying Time-Slot Events<t5_small>,<TITLE> Naive Bayes Classifier for Time-Slots<TITLE> Naive Bayes Classifier for Time-Aware Event Detection<TITLE> Semi-Supervised Time-Slots for Event Detection<TITLE> Naive Bayes Classifier for Event Detection<TITLE> Semi-Supervised Learning of Time-Aware Event Detection<gpt2>,<TITLE>Time-Slave Event Classification Using Expectation Maximization<TITLE>A Semi-Supervised Learning Method for Event Classification<TITLE>Identifying Time-Slots in Text<TITLE>Identifying When an Event Takes place<TITLE>Identifying When Events occur<pegasus_xsum>
12,"Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.",A Structural Probe for Finding Syntax in Word Representations<original>,<TITLE>Detecting Syntactic Knowledge in Neural Network Word Representations<TITLE>A Structural Investigation of Syntactic Knowledge in Neural Networks<TITLE>A Structural Exploration of Syntactic Knowledge in Neural Networks<TITLE>A Structural probe of syntax trees in neural networks<TITLE>Detecting Syntactic Knowledge in Neural Networks<bart_base>,<TITLE>A Structural Probabilistic Investigation of Syntax Trees are Embedding Implicitly in Neural Network's Word Represent<TITLE>A Structural Probabilistic Investigation of Syntax Trees are Embedding Implicitly in Neural Network Word Representations<TITLE>A Structural Probabilistic Investigation of Syntax Trees are Embedding Implicitly in Neural Word Representation Space<TITLE>A Structural Probabilistic Investigation of Syntax Trees are Embedding Implicitly in Neural Network Word Representation<TITLE>A Structural Probabilistic Investigation of Syntax Trees are Embedding Implicitly in Neural Word Representation Spaces<bart_cnn>,<TITLE>A Structural Probe for Detecting Syntactic Knowledge in Neural Word Representations<TITLE>A Structural Probe for Detecting Syntactic Knowledge in Word Representations<TITLE>A Structural Probe for Detecting Syntactic Knowledge in Neural Networks<TITLE>A Structural Probe for Detecting Syntactic Knowledge in Neural Networks<TITLE>Detecting Syntactic Knowledge in Neural Word Representations<bart_xsum>,<TITLE>Detecting Linguistic Knowledge in Word Representation Spaces<TITLE>Detecting Syntactic Knowledge in Word Representation Space<TITLE>Detecting Syntax Trees in Word Representation Spaces<TITLE>Detecting Syntactic Knowledge in Word Representations<TITLE>Detecting Linguistic Knowledge in Word Representations<t5_small>,<TITLE> A Structural Investigation of Syntax Trees in Neural Networks<TITLE> A Structural Investigation of Syntax Trees<TITLE> A Structural Exploration of Syntax Trees<TITLE> A Structural Exploration of Neural Networks for Syntactic Knowledge Detection<TITLE> A Structural Exploration of Syntax Trees in Neural Networks<gpt2>,<TITLE>A Structural Investigation of Syntax in Neural Word Representations<TITLE>A Structural Investigation of Syntax Trees in Deep Neural Networks<TITLE>A Structural Investigation of Syntax Trees in Neural Networks<TITLE>A Structural Investigation of Syntax in Neural Networks<TITLE>Structural Analysis of Syntax Trees in Neural Networks<pegasus_xsum>
13,"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.",Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering<original>,<TITLE>Knowledge Triplet Learning for Question Answering over Knowledge Graphs<TITLE>Knowledge Triplet Learning over Knowledge Graphs for Question Answering<TITLE>Knowledge Triplet Learning for Zero-Shot Question Answering<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering<TITLE>Knowledge Triplet Learning for Knowledge Graph Question Answering<bart_base>,<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering: Improving Answer Generation by Knowledge Graphs over Knowledge Graph<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering: Improving Knowledge Graph Generation and Zero-Shot Question An<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering: Improving Answer Generation by Knowledge Graphs and Scientific Knowledge<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering: Improving Knowledge Graph Generation and Zero-Shot Answer An<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering: Improving Answer Generation by Knowledge Graphs and Knowledge Graph<bart_cnn>,<TITLE>Knowledge Triplet Learning for Zero-Shot Question Answering<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering<TITLE>Knowledge Triplet Learning for Zero-Shot Question Answering<TITLE>Knowledge Triplet Learning for Zero-Shot Question Answering<TITLE>Knowledge Triplet Learning for Zero-Shot Question Answering<bart_xsum>,<TITLE>Self-Supervised Question Answering with Knowledge Triplet Learning<TITLE>Knowledge Triplet Learning for Non-Shot Question Answering Systems<TITLE>Knowledge Triplet Learning for Non-Shot Question Answering<TITLE>Knowledge Triplet Learning for Quantitative Question Answering<TITLE>Knowledge Triplet Learning for Question Answering<t5_small>,<TITLE> Knowledge Triplet Learning for Commonsense Question Answering<TITLE> Knowledge Triplet Learning for Question Answering<TITLE> Knowledge Triplet Learning for Commonsense and Scientific Knowledge<TITLE> Knowledge Triplet Learning for Commonsense and Scientific Knowledge Graphs<TITLE> Knowledge Triplet Learning<gpt2>,<TITLE>Knowledge Triplet Learning for Zero-Shot Question Answering<TITLE>Knowledge Triplet Learning for Zero-shot Question Answering<TITLE>Zero-Shot Question Answering Using Knowledge Triplet Learning<TITLE>Zero-Shot Question Answering with Knowledge Triplet Learning<TITLE>Zero-Shot Question Answering using Knowledge Triplet Learning<pegasus_xsum>
14,"We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms. The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research. The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.",Learning subjective nouns using extraction pattern bootstrapping<original>,<TITLE>Subjectivity Classification Using Bootstrapping Algorithms and Discourse Features<TITLE>Bootstrapping Subjectivity Classifiers Using Subjective Nouns<TITLE>Subjectivity Classifier Using Bootstrapping Algorithms<TITLE>Subjectivity Classification Using Bootstrapping Algorithms<TITLE>A Bootstrapping Approach to Subjectivity Classification<bart_base>,<TITLE>Learning Subjectivity Classifiers from Subjective Nouns using Bootstrapping Algorithms and Extraction Patterns for Subjectivity<TITLE>Learning Subjectivity Classifiers from Subjective Nouns Using Bootstrapping Algorithms and Extraction Patterns for Subjectivity<TITLE>Learning Subjectivity Classifiers from Subjective Nouns using Bootstrapping Algorithms and Extraction Patterns for Sentence<TITLE>Learning Subjectivity Classifiers from Subjective Nouns Using Bootstrapping Algorithms and Extraction Patterns for Sentence<TITLE>Learning Subjectivity Classifiers from Subjective Nouns using Bootstrapping Algorithms and Extraction Patterns for Subject Classification<bart_cnn>,<TITLE>Learning Subjective Nouns for Subjectivity Classification<TITLE>Learning Subjective Nouns for Subjectivity Classifier<TITLE>Learning Subjective Nouns for Subjectivity Classification<TITLE>Learning Subjective Nouns for Subjectivity Classification<TITLE>Learning Subjective Nouns by Bootstrapping<bart_xsum>,<TITLE>Learning Subjectivity Classifiers with Bootstrapping Algorithms<TITLE>Subjectivity Classifiers with Bootstrapping Algorithms<TITLE>A Subjectivity Classifier for Bootstrapping Algorithms<TITLE>Learning Subjectivity Classifiers from Objection Nouns<TITLE>A Subjectivity Classifier with Subjectivity Nouns<t5_small>,<TITLE> Bootstrapping Subjectivity Classifiers<TITLE> Bootstrapping Objective Nouns for Subjectivity Classification<TITLE> Bootstrapping Subjectivity Classifiers for Bootstrapping<TITLE> Bootstrapping Subjectivity Classifiers Using Discourse Information<TITLE> Bootstrapping Subjectivity Classifiers Using Discourse Features<gpt2>,<TITLE>Exploiting Extraction Patterns to Learn Subjectivity for Classifying Sentences<TITLE>Exploiting Extraction Patterns to Learn Subjectivity from Nouns<TITLE>Exploiting Extraction Patterns to Learn Subjective Nouns<TITLE>Naive Bayes: A Subjectivity Classifier<TITLE>A Subjectivity Classifier Using Extraction Patterns<pegasus_xsum>
15,"We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments. We contrast direct assessment (annotators assign scores to items directly), online pairwise ranking aggregation (scores derive from annotator comparison of items), and a hybrid approach (EASL: Efficient Annotation of Scalar Labels) proposed here. Our proposal leads to increased correlation with ground truth, at far greater annotator efficiency, suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation.",Efficient Online Scalar Annotation with Bounded Support<original>,<TITLE>Efficient Annotation of Scalar Annotations for Dataset Construction and Evaluation<TITLE>Efficient Annotation of Scalar Annotations for Dataset Creation and Evaluation<TITLE>Efficient Annotation of Scalar Annotations for Datasets and System Evaluation<TITLE>Efficient Annotation of Scalar Annotations for Datasets Construction and Evaluation<TITLE>Efficient Annotation of Scalar Annotations for Large-Scale System Evaluation<bart_base>,<TITLE>Efficient Annotation of Scalar Annotations for Dataset Construction and System Quality Estimation by Human Judgments: A Hybrid<TITLE>Efficient Annotation of Scalar Annotations for Dataset Construction and System Quality Estimation by Human Judgments via Direct Assessment<TITLE>Efficient Annotation of Scalar Annotations for Dataset Construction and System Quality Estimation by Human Judgments using Direct Assessment<TITLE>Efficient Annotation of Scalar Annotations for Dataset Construction and System Quality Estimation by Human Judgments using Efficient<TITLE>Efficient Annotation of Scalar Annotations for Dataset Construction and System Quality Estimation by Human Judgments using EAS<bart_cnn>,<TITLE>Efficient Annotation of Scalar Labels by Human Judgments<TITLE>Efficient Annotation of Scalar Labels: A Hybrid Approach<TITLE>Efficient Annotation of Scalar Labels by Human Judgments<TITLE>Efficient Annotation of Scalar Labels by Human Judgments<TITLE>Efficient Annotation of Scalar Labels<bart_xsum>,<TITLE>Efficient Annotation of Scalar Labels for Dataset Construction and System Quality Estimation<TITLE>Efficient Annotation of Scalar Labels for Dataset Generation and System Quality Estimation<TITLE>Efficient Annotation of Scalar Labels for Dataset Design and System Quality Estimation<TITLE>Efficient Annotation of Scalar Labels for Dataset Construction and Quality Estimation<TITLE>Efficient Scalar Annotation for Dataset Construction and System Quality Estimation<t5_small>,<TITLE> Efficient Annotation of Scalar Annotations for Dataset Construction<TITLE> Efficient Annotation of Scalar Annotations<TITLE> Scalar Annotations for Dataset Construction and Quality Estimation<TITLE> Automatically Evaluating Scalar Annotations for Dataset Construction<TITLE> Efficient Annotation of Scalar Annotations for Dataset Creation<gpt2>,<TITLE>Efficient Aggregation of Annotations for Dataset Construction and System Quality Estimation<TITLE>Efficient Aggregation of Human Judgments for Dataset Construction and System Quality Estimation<TITLE>Efficient Aggregation of Annotations for Dataset Construction and Manual System Evaluation<TITLE>Aggregation-Based Annotation for Dataset Construction and System Quality Estimation<TITLE>Efficient Aggregation of Human Judgments for Dataset Construction and Quality Estimation<pegasus_xsum>
16,"Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.",Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach<original>,<TITLE>Relation Extraction from Heterogeneous Information Sources Using Embedding Techniques<TITLE>Relation Extraction from Heterogeneous Information Sources with Embedding Techniques<TITLE>Relation Extraction from Heterogeneous Annotations with Embedding Techniques<TITLE>Relation Extraction with Hierarchical Supervision<TITLE>Relation Extraction Using Hierarchical Supervision<bart_base>,<TITLE>Learning Distributed Representations of Context for Relation Extraction from Heterogeneous Information Sources with REHession and Mutual Enhancement<TITLE>Learning Distributed Representations of Context for Relation Extraction from Heterogeneous Information Sources using REHession and Mutual Enhancement<TITLE>Learning Distributed Representations of Context for Relation Extraction with Heterogeneous Supervision from Noisy Labels using RE<TITLE>Learning Distributed Representations of Context for Relation Extraction with Heterogeneous Supervision from Noisy Labels and Mutual<TITLE>Learning Distributed Representations of Context for Relation Extraction with Heterogeneous Supervision from Noisy Labels and Rel<bart_cnn>,<TITLE>Relation Extraction with Heterogeneous Supervision<TITLE>Relation Extraction with Heterogeneous Supervision<TITLE>Relation Extraction with Heterogeneous Annotations<TITLE>Relation Extraction with Heterogeneous Supervision<TITLE>Relation Extraction with Heterogeneous Supervision<bart_xsum>,<TITLE>Relation Extraction with Annotations from Heterogeneous Information Sources<TITLE>Relation Extraction with Annotations from Heterogeneous Information Source<TITLE>Relation Extraction with Annotations from Heterogeneous Information<TITLE>Relation Extraction with Heterogeneous Annotations<TITLE>REHession: A Framework for Relation Extraction<t5_small>,<TITLE> Semi-supervised Relation Extraction with Heterogeneous Supervision<TITLE> REHession: Relation Extraction with Heterogeneous Supervision<TITLE> Semi-supervised Relation Extraction via Hierarchical Embedding<TITLE> Semi-supervised Relation Extraction with Hierarchical Supervision<TITLE> Semi-supervised Relation Extraction Using Hierarchical Representations<gpt2>,<TITLE>REHession: Relation Extraction with Heterogeneous Supervision<TITLE>REHession: Relation Extraction Using Heterogeneous Supervision<TITLE>Relation Extraction from Heterogeneous Information Sources with Embeddings<TITLE>Relation Extraction with Heterogeneous Supervision<TITLE>Relation Extraction Using Heterogeneous Supervision<pegasus_xsum>
17,"Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.",Opportunistic Decoding with Timely Correction for Simultaneous Translation<original>,<TITLE>Correcting Over-Generated Words for Simultaneous Translation<TITLE>An Opportunistic Decoding Technique for Simultaneous Translation<TITLE>Optimistic Decoding for Simultaneous Translation with Updates<TITLE>Optimistic Decoding for Simultaneous Translation<TITLE>An opportunistic decoding technique for Simultaneous translation<bart_base>,<TITLE>An Opportunistic Decoding Technique for Simultaneous Translation to Chinese-to-Chinese Translation with Timing-Dependent Correction<TITLE>An Opportunistic Decoding Technique for Simultaneous Translation to Chinese-to-Chinese Translation with Timing-Timing-<TITLE>An Opportunistic Decoding Technique for Simultaneous Translation to Chinese-to-Chinese Statistical Machine Translation with Timing-D<TITLE>An Opportunistic Decoding Technique for Simultaneous Translation to Chinese-to-Chinese Machine Translation with Timing-Dependent<TITLE>An Opportunistic Decoding Technique for Simultaneous Translation to Chinese-to-Chinese Statistical Machine Translation with Timing-Tim<bart_cnn>,<TITLE>Accelerated Decoding for Simultaneous Translation with Over-Generation Correction<TITLE>Accelerated Decoding for Simultaneous Translation with Over-Generating Words<TITLE>Accelerated Decoding for Simultaneous Translation with Over-Generation Correction<TITLE>Accelerated Decoding for Simultaneous Statistical Machine Translation<TITLE>Accelerated Decoding for Simultaneous Translation<bart_xsum>,<TITLE>Opportunistic Decoding for Simultaneous Translation<TITLE>Opportunistic decoding for Simultaneous Translation<TITLE>Opportunistic Decoding for Simultaneous Translation<TITLE>Opportunistic Decoding with Timely Correction Caption<TITLE>Opportunistic Decoding with Timely Correction Ability<t5_small>,<TITLE> Acquatic Decoding for Simultaneous Translation<TITLE> Acquatic Decoding for Fast and Accurate Simultaneous Translation<TITLE> Acquatic Decoding for Fast and Accurate Synchronous Translation<TITLE> Acquatic Decoding for Opportunistic Translation<TITLE> Acquisition of Opportunistic Decoding for Fast and Accurate Translation<gpt2>,<TITLE>Opportunistic Decoding for Simultaneous Translation<TITLE>Opportunistic Correction for Simultaneous Machine Translation<TITLE>Opportunistic Correction for Simultaneous Translation<TITLE>Timing-Aware Decoding for Simultaneous Translation<TITLE>Timing Correction for Simultaneous Translation<pegasus_xsum>
18,"We analyze the convergence of the averaged stochastic gradient descent for overparameterized two-layer neural networks for regression problems. It was recently found that a neural tangent kernel (NTK) plays an important role in showing the global convergence of gradient-based methods under the NTK regime, where the learning dynamics for overparameterized neural networks can be almost characterized by that for the associated reproducing kernel Hilbert space (RKHS). However, there is still room for a convergence rate analysis in the NTK regime. In this study, we show that the averaged stochastic gradient descent can achieve the minimax optimal convergence rate, with the global convergence guarantee, by exploiting the complexities of the target function and the RKHS associated with the NTK. Moreover, we show that the target function specified by the NTK of a ReLU network can be learned at the optimal convergence rate through a smooth approximation of a ReLU network under certain conditions.",Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime<original>,<TITLE>Optimal Convergence of a Neural Tangent Kernel for Overparameterized Neural Networks<TITLE>Optimal Convergence of Neural Tangent Kernels for Overparameterized Neural Networks<TITLE>A Neural Tangent Kernel for Overparameterized Two-layer Neural Networks<TITLE>A Neural Tangent Kernel for Overparameterized Two-Layer Neural Networks<TITLE>A Neural Tangent Kernel for Overparameterized Neural Networks<bart_base>,<TITLE>On the Convergence of Overparameterized Two-Layer Neural Networks with a Neural Tangent Kernel Hilbert Space for Regression<TITLE>On the Convergence of Overparameterized Two-Layer Neural Networks for Regression Problems: Theory and Algorithms.<TITLE>On the Convergence of Overparameterized Two-Layer Neural Networks with a Neural Tangent Kernel Hilbert Space for Learning Dynamics<TITLE>On the Convergence of Overparameterized Two-Layer Neural Networks for Regression Problems: Theory and Algorithms <TITLE>On the Convergence of Overparameterized Two-Layer Neural Networks for Regression Problems: Theory and Algorithms with<bart_cnn>,<TITLE>Convergence of Overparameterized Neural Networks with Neural Tangent Kernels<TITLE>Convergence of Overparameterized Neural Networks with Neural Tangent Kernel<TITLE>Convergence of Overparameterized Neural Networks for Regression Problems<TITLE>On the Convergence of Overparameterized Neural Networks for Regression Problems<TITLE>On the Convergence of Overparameterized Neural Networks<bart_xsum>,<TITLE>Convergence of Averaged Stochastic Gradient Descent for Overparameterized Neural Networks<TITLE>The Convergence of the Averaged Stochastic Gradient Descent for Regression Problems<TITLE>Convergence of the Averaged Stochastic Gradient Descent for Regression Problems<TITLE>Convergence of Averaged Stochastic Gradient Descent for Regression Problems<TITLE>Convergence of the Averaged Stochastic Gradient Descent for Regression<t5_small>,<TITLE> On the Convergence of Overparameterized Two-Layer Neural Networks<TITLE> Convergence of Overparameterized Two-Layer Neural Networks<TITLE> On the Convergence of Overparameterized Neural Networks<TITLE> Convergence of Overparameterized Neural Networks<TITLE> On the Convergence of Overparameterized Two-layer Neural Networks<gpt2>,<TITLE>A Convergence Analysis of the Averaged Gradient Descent for ReLU Neural Networks<TITLE>Minimax Optimal Convergence of the Averaged Gradient Descent for ReLU Networks<TITLE>A Convergence Analysis of the Averaged Gradient Descent for ReLU Networks<TITLE>Convergence of the Averaged Gradient Descent for Overparameterized Neural Networks<TITLE>Convergence of the Averaged Gradient Descent for ReLU Networks<pegasus_xsum>
19,"Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.",Markov Chain Monte Carlo and Variational Inference: Bridging the Gap<original>,<TITLE>Stochastic Gradient Variational Inference with Monte Carlo Methods<TITLE>Stochastic Gradient Variational Inference and Monte Carlo Methods<TITLE>Stochastic Gradient Variational Inference via Monte Carlo Methods<TITLE>Stochastic Gradient Variational Inference via Monte Carlo<TITLE>Fast Monte Carlo Methods for Variational Bayesian Inference<bart_base>,<TITLE>Auxiliary Random Variational Bayesian Inference with Fast Posterior Approximation and Monte-Carlo Maximization.<TITLE>Auxiliary Random Variational Bayesian Inference with Fast Posterior Approximation and Monte-Carlo Optimization.<TITLE>Auxiliary Random Variational Bayesian Inference with Fast Posterior Approximation and Monte-Carlo Maximization of<TITLE>Auxiliary Random Variational Bayesian Inference with Fast Posterior Approximation and Monte-Carlo MCMC.<TITLE>Auxiliary Random Variational Bayesian Inference with Fast Posterior Approximation and Monte-Carlo Maximization <bart_cnn>,<TITLE>Variational Bayesian Inference with Auxiliary Random Variables<TITLE>Variational Bayesian Inference with Monte Carlo MCMC<TITLE>Variational Bayesian Inference with Auxiliary Random Variables<TITLE>Variational Bayesian Inference with Monte Carlo Methods<TITLE>Variational Bayesian Inference with Monte Carlo Methods<bart_xsum>,<TITLE>Variational Bayesian Inference with Posterior Approximations<TITLE>Bayesian Variational Inference with Posterior Approximations<TITLE>Variational Bayesian Inference with Posterior Approximation<TITLE>Variational Bayesian Inference with posterior approximations<TITLE>Variational Bayesian inference with posterior approximations<t5_small>,<TITLE> Variational Bayesian Inference with Auxiliary Random Variables<TITLE> Variational Bayesian Inference via MCMC<TITLE> Stochastic Gradient Variational Inference<TITLE> Variational Bayesian Inference with Posterior Approximation<TITLE> Variational Bayesian Inference<gpt2>,<TITLE>Bridging the Gap between Variational Inference and MCMC<TITLE>Bridging the Gap between Variational Inference and Monte Carlo<TITLE>Variational Bayesian Inference with Auxiliary Random Variables<TITLE>Variational Inference and Monte Carlo Methods<TITLE>Variational Inference and Monte Carlo<pegasus_xsum>
20,"The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog.",USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation<original>,<TITLE>An Unsupervised and Reference-Free Evaluation Metric for Dialog<TITLE>Unsupervised and Reference-Free Evaluation Metric for Dialog<TITLE>Unsupervised and Reference-Free Evaluation Metric for Dialog Generation<TITLE>Unsupervised and Reference-Free Evaluation of Dialog Models<TITLE>Unsupervised and Reference-Free Dialog Evaluation Metric<bart_base>,"<TITLE>Unsupervised and Reference-Free Evaluation of Dialog Models: USR, an Unsupervised Metric for Dialog<TITLE>Unsupervised and Reference-Free Evaluation of Dialog Models for Dialog Research: USR, an Unsupervised Met<TITLE>Unsupervised and Reference-Free Evaluation of Dialog Models for Dialog Research: USR, an Unsupervised Approach<TITLE>Unsupervised and Reference-Free Evaluation of Dialog Models: USR, an Unsupervised Approach for Dialog Research<TITLE>Unsupervised and Reference-Free Evaluation of Dialog Models: USR, an Unsupervised Metric for Dialogue Research<bart_cnn>",<TITLE>An Unsupervised Evaluation Metric for Open-Domain Dialog Models<TITLE>Unsupervised and Reference-Free Evaluation Metric for Dialog Models<TITLE>An Unsupervised Metric for Evaluating Open-Domain Dialog<TITLE>An Unsupervised Evaluation Metric for Open-Domain Dialog<TITLE>Unsupervised and Reference-Free Evaluation Metric for Dialog<bart_xsum>,<TITLE>USR: An UnSupervised and Reference-Free Evaluation Metric for Dialog<TITLE>USR: An UnSupervised and Reference-free Evaluation Metric for Dialog<TITLE>UnSupervised and Reference-Free Evaluation Metrics for Dialog<TITLE>UnSupervised and Reference-Free Evaluation Metric for Dialog<TITLE>UnSupervised and Reference-Free Evaluation for Dialog<t5_small>,<TITLE> Unsupervised and Reference-Free Dialog Evaluation Metrics<TITLE> Unsupervised and Reference-free Dialog Evaluation Metrics<TITLE> Unsupervised Dialog Evaluation Metrics for Dialog<TITLE> Unsupervised Dialog Evaluation Metric for Dialog<TITLE> Unsupervised Evaluation of Dialog Models<gpt2>,<TITLE>Unsupervised and Reference-Free Dialog Model Evaluation<TITLE>Unsupervised and Reference-Free Evaluation for Dialog<TITLE>Unsupervised and Reference-Free Evaluation of Dialog<TITLE>Unsupervised and Reference-Free Dialog Evaluation<TITLE>Unsupervised Dialog Model Evaluation<pegasus_xsum>
21,"Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships. Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.",Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering<original>,<TITLE>Unsupervised Question Answering with Pseudo-Training Data<TITLE>Unsupervised Question Answering with Pseudo-training Data<TITLE>Unsupervised Question Answering Using Pseudo-Training Data<TITLE>Unsupervised Question Answering Using Pseudo-training Data<TITLE>Unsupervised QA Training with Pseudo-Training Data<bart_base>,<TITLE>Unsupervised Question Answering with Pseudo-Training Data Generated for Unsupervised QA Training on Task-<TITLE>Unsupervised Question Answering with Pseudo-Training Data Generated for Unsupervised QA Training and Context-<TITLE>Unsupervised Question Answering with Pseudo-Training Data Generated for Unsupervised QA Training and Contextual<TITLE>Unsupervised Question Answering with Pseudo-Training Data Generated for Unsupervised QA Training of Context-<TITLE>Unsupervised Question Answering with Pseudo-Training Data Generated for Unsupervised QA Training of Contextual<bart_cnn>,<TITLE>Unsupervised Question Answering with Pseudo-Training Data<TITLE>Unsupervised Question Answering with Pseudo-training Data<TITLE>Generating Questions for Unsupervised Question Answering<TITLE>Generating Questions for Unsupervised Question Answering<TITLE>Generating Questions for Unsupervised Question Answering<bart_xsum>,<TITLE>Unsupervised Question Answering with Pseudo-Training Data<TITLE>Unsupervised Question Answering with Generated Pseudo-training Data<TITLE>Unsupervised Question Answering with Pseudo-Training<TITLE>Unsupervised Question Answering with Pseudo-training Data<TITLE>Unsupervised Question Answering<t5_small>,<TITLE> Unsupervised Question Answering with Pseudo-Training Data<TITLE> Unsupervised Question Answering Using Pseudo-Training Data<TITLE> Unsupervised Question Answering using Pseudo-Training Data<TITLE> Generating Pseudo-Training Data for Unsupervised QA<TITLE> Generating Pseudo-Training Data for Unsupervised QA Training<gpt2>,<TITLE>Unsupervised Question Answering with Pseudo-Training Data<TITLE>Unsupervised Question Answering Using Pseudo-Training Data<TITLE>Generating Pseudo-Training Data for Unsupervised QA Training<TITLE>Generating Pseudo-Training Data for Question Answering<TITLE>Generating Pseudo-Training Data for Unsupervised QA<pegasus_xsum>
22,"Safe navigation of autonomous agents in human centric environments requires the ability to understand and predict motion of neighboring pedestrians. However, predicting pedestrian intent is a complex problem. Pedestrian motion is governed by complex social navigation norms, is dependent on neighbors’ trajectories, and is multimodal in nature. In this work, we propose SCAN, a Spatial Context Attentive Network that can jointly predict socially-acceptable multiple future trajectories for all pedestrians in a scene. SCAN encodes the influence of spatially close neighbors using a novel spatial attention mechanism in a manner that relies on fewer assumptions, is parameter efficient, and is more interpretable compared to state-of-the-art spatial attention approaches. Through experiments on several datasets we demonstrate that our approach can also quantitatively outperform state of the art trajectory prediction methods in terms of accuracy of predicted intent.",SCAN: A Spatial Context Attentive Network for Joint Multi-Agent Intent Prediction<original>,<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: Spatial Context Attentive Networks for Predicting Pedestrian Intent<TITLE>SCAN: Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Predicting pedestrian Intent<TITLE>A Spatial Context Attentive Network for Predicting Pedestrian Intent<bart_base>,<TITLE>SCAN: Spatial Context Attentive Networks for Pedestrian Motion Prediction using Spatial Attention in Human Centric En<TITLE>SCAN: Spatial Context Attentive Networks for Pedestrian Motion Prediction using Spatial Attention in Human Centric environments<TITLE>SCAN: Spatial Context Attentive Networks for Pedestrian Motion Prediction using Spatial Attention in Human Centric Environment<TITLE>SCAN: Spatial Context Attentive Networks for Pedestrian Motion Prediction using Spatial Attention in Human Centric Systems<TITLE>SCAN: Spatial Context Attentive Networks for Pedestrian Motion Prediction using Spatial Attention in Human Centric Settings<bart_cnn>,<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Pedestrian Intent Prediction<TITLE>SCAN: A Spatial Context Attentive Network<bart_xsum>,<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Socially Acceptable Multiple Future Trajectories<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Socially Acceptable Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Predicting Pedestrian Intention<TITLE>SCAN: A Spatial Context Attentive Network for Pedestrian Motion Prediction<t5_small>,<TITLE> SCAN: Spatial Context Attentive Networks for Predicting Multiple Future Trajectories<TITLE> Scan: Spatial Context Attentive Networks for Predicting Multiple Future Trajectories<TITLE> SCAN: Spatial Context Attentive Networks for Predicting Social Norms<TITLE> SCAN: A Spatial Context Attentive Network for Predicting Multiple Future Trajectories<TITLE> SCAN: Spatial Context Attentive Networks for Predicting Multiple Future Trajectories for All Stops<gpt2>,<TITLE>SCAN: Spatial Context Attentive Networks for Predicting Pedestrian Intent<TITLE>SCAN: Spatial Context Attentive Network for Predicting Pedestrian Intent<TITLE>SCAN: A Spatial Context Attentive Network for Pedestrian Intent Prediction<TITLE>SCAN: Spatial Context Attentive Network for Pedestrian Intent Prediction<TITLE>SCAN: Spatial Context Attentive Networks for Pedestrian Intent Prediction<pegasus_xsum>
23,"Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.",Distant Supervision for Relation Extraction with an Incomplete Knowledge Base<original>,<TITLE>Learning from Only Positive and Unlabeled Labels for Distant Supervision<TITLE>Learning from Only Positive and Unlabeled Labels in Distant Supervision<TITLE>Negative Examples in Distant Supervision for Relation Extraction<TITLE>Negative Examples for Distantly Supervised Relation Extraction<TITLE>Negative Examples in Distantly Supervised Relation Extraction<bart_base>,<TITLE>Learning from Only Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction: A Case Study on<TITLE>Learning from Only Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction: A Case Study in<TITLE>Learning from Only Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction with a Knowledge Base of<TITLE>Learning from Only Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction: A Case Study with<TITLE>Learning from Only Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction: A Case Study of<bart_cnn>,<TITLE>Learning from Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction<TITLE>Learning from Positive and Unlabeled Labels for Distantly-Supervised Relation Extraction<TITLE>Learning from Positive and Unlabeled Data for Distantly-Supervised Relation Extraction<TITLE>Learning from Positive and Unlabeled Data for Distantly-Supervised Relation Extraction<TITLE>Learning from Negative Examples in Distantly-Supervised Relation Extraction<bart_xsum>,<TITLE>Distant Supervision: Learning from Positive and Unlabeled Labels<TITLE>Learning from Positive and Unlabeled Labels for Distant Supervision<TITLE>Distant Supervised Extraction from Positive Labels<TITLE>Learning from Positive and Unlabeled Labels<TITLE>Distant Supervision for Relation Extraction<t5_small>,<TITLE> Negative Examples in Distantly Supervised Relation Extraction<TITLE> Improving Distantly Supervised Relation Extraction with Negative Examples<TITLE> Negative Examples in Distant Supervision for Relation Extraction<TITLE> Improving Distant Supervision for Relation Extraction<TITLE> Improving Relation Extraction with Negative Examples<gpt2>,<TITLE>Learning from Only Positive and Unlabeled Labels in Distant Supervision for Relation Extraction<TITLE>Learning from Positive and Unlabeled Labels in Distant Supervision for Relation Extraction<TITLE>Learning from Only Positive and Unlabeled Labels in Distant Supervision<TITLE>Learning from Positive and Unlabeled Labels in Distant Supervision<TITLE>Learning from Only Positive and Unlabeled Labels<pegasus_xsum>
24,"Online shopping caters the needs of millions of users on a daily basis. To build an accurate system that can retrieve relevant products for a query like “MB252 with travel bags” one requires product and query categorization mechanisms, which classify the text as Home&Garden>Kitchen&Dining>Kitchen Appliances>Blenders. One of the biggest challenges in e-Commerce is that providers like Amazon, e-Bay, Google, Yahoo! and Walmart organize products into different product taxonomies making it hard and time-consuming for sellers to categorize goods for each shopping platform. To address this challenge, we propose an automatic product categorization mechanism, which for a given product title assigns the correct product category from a taxonomy. We conducted an empirical evaluation on 445, 408 product titles and used a rich product taxonomy of 319 categories organized into 6 levels. We compared performance against multiple algorithms and found that the best performing system reaches .88 f-score.",Everyone Likes Shopping! Multi-class Product Categorization for e-Commerce<original>,<TITLE>Automatic Product Categorization Mechanisms for Online Shopping<TITLE>Automatic Product and Query Categorization for Online Shopping<TITLE>Automatic Product Categorization for Online Shopping<TITLE>Automated Product Categorization for Online Shopping<TITLE>Automatic Product Categorization in Online Shopping<bart_base>,<TITLE>Automatic Product Categorization for Shopping Platforms using Home&Garden Appliance and Multi-Level Guidance Mechanisms<TITLE>Automatic Product Categorization for Shopping Platforms Using Home&Garden Appliance and Multi-Level Guidance Mechanisms<TITLE>Automatic Product Categorization for Shopping Platforms using Home&Garden Appliance and Multi-Level Guidance Mechanism<TITLE>Automatic Product Categorization for Shopping Platforms Using Home&Garden Appliance and Multi-Level Supervision Mechanisms<TITLE>Automatic Product Categorization for Shopping Platforms Using Home&Garden Appliance and Multi-Level Guidance Mechanism<bart_cnn>,<TITLE>Automatic Product Categorization for Online Shopping<TITLE>Automatic Product Categorization for E-commerce<TITLE>Automatic Product Categorization for E-Commerce<TITLE>Automatic Product Categorization for Online Shopping<TITLE>Automatic Product Categorization for E-commerce<bart_xsum>,<TITLE>Product Categorization for Online Shopping with Travel Bags<TITLE>Automated Product Categorization for Online Shopping<TITLE>Automatic Product Categorization for Online Shopping<TITLE>Automatic Product Categorization in Online Shopping<TITLE>Product Categorization for Online Shopping<t5_small>,<TITLE> Automatic Product Categorization for Online Shopping Caters<TITLE> Automated Product Categorization for Online Shopping Caters<TITLE> Automatic Product Categorization for Shopping Platforms<TITLE> Automatic Product Categorization for Online Shopping Caters <TITLE> Automated Product Categorization for Online Shopping Caters <gpt2>,<TITLE>Automatic Product Categorization Mechanism for Online Shopping Platforms<TITLE>Automatic Product Categorization for Online Shopping Platforms<TITLE>Automatic Product Categorization Mechanism for Online Shopping<TITLE>Automatic Product Categorization for Online Shopping<TITLE>Product Categorization for Online Shopping<pegasus_xsum>
25,"We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.",Joint Entity and Event Coreference Resolution across Documents<original>,<TITLE>Joint Coreference Resolution with Entity and Event Mention Clusters<TITLE>Joint Entity and Event Coreference Resolution with Structured Clusters<TITLE>Joint Entity and Event Coreference Resolution Using Linear Regression<TITLE>Joint Coreference Resolution with Entity and Event Clusters<TITLE>Joint Coreference Resolution with Entity and Event Mentions<bart_base>,<TITLE>Joint Coreference Resolution of Entities and Events Using Modeling Semantic Role Dependencies using Linear Regression and Iterative<TITLE>Joint Coreference Resolution of Entities and Events Using Modeling Semantic Role Dependencies using Linear Regression and Incremental<TITLE>Joint Coreference Resolution of Entities and Events Using Modeling Semantic Role Dependencies using Linear Regression and Sequential<TITLE>Joint Coreference Resolution of Entities and Events Using Modeling Semantic Role Dependencies using Linear Regression and Interaction<TITLE>Joint Coreference Resolution of Entities and Events Using Modeling Semantic Role Dependencies using Linear Regression and Sequences<bart_cnn>,<TITLE>Joint Entity and Event Coreference Resolution with Semantic Role Dependencies<TITLE>Joint Entity and Event Coreference Resolution with Semantic Role Dependencies<TITLE>Joint Coreference Resolution of Entities and Events<TITLE>Joint Entity and Event Coreference Resolution<TITLE>Joint Entity and Event Coreference Resolution<bart_xsum>,<TITLE>Joint Coreference Resolution for Entity and Events<TITLE>Joint Coreference Resolution for Entity and Event Discovery<TITLE>Joint Coreference Resolution of Entity and Events<TITLE>Joint Coreference Resolution for Entities and Events<TITLE>Joint Coreference Resolution of Entities and Events<t5_small>,<TITLE> Joint Entity and Event Coreference Resolution<TITLE> A Joint Model for Coreference Resolution<TITLE> Joint Entity and Event Coreference Resolution Using Linear Regression<TITLE> Joint Entity and Event Coreference Resolution: An Iterative Approach<TITLE> Joint Entity and Event Coreference Resolution: An Iterative Method<gpt2>,<TITLE>Joint Coreference Resolution with Entity and Event Clusters<TITLE>Joint Coreference Resolution for Nominal and Verbal Events<TITLE>Joint Coreference Resolution of Nominal and Verbal Events<TITLE>Joint Coreference Resolution with Entity and Event Clustering<TITLE>Joint Entity and Event Coreference Resolution<pegasus_xsum>
26,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.",ALBERT: A Lite BERT for Self-supervised Learning of Language Representations<original>,<TITLE>ParameterReduction and Self-Supervised Learning for Neural Machine Translation<TITLE>ParameterReduction and Self-Supervised Training for Neural Machine Translation<TITLE>ParameterReduction and Self-Supervised Learning for Neural Language Processing<TITLE>Parameter Reduction and Self-Supervised Learning for Neural Machine Translation<TITLE>Approximate Parameter Reduction for Neural Machine Translation<bart_base>,<TITLE>ALBERT: Self-Supervised Learning for Pretraining Natural Language Representations with Memory and Self-supervised Loss.<TITLE>ALBERT: Self-Supervised Learning for Pretraining Natural Language Representations with Application to Label Induction and Sentence<TITLE>ALBERT: Self-Supervised Learning for Pretraining Natural Language Representations with Memory and Self-supervised Losss<TITLE>ALBERT: Self-Supervised Learning for Pretraining Natural Language Representations with Memory and Self-supervised Losses<TITLE>ALBERT: Self-Supervised Learning for Pretraining Natural Language Representations with Memory and Self-supervised Training.<bart_cnn>,<TITLE>Parameter Reduction and Self-Supervised Coherence for BERT<TITLE>Parameter Reduction and Self-Supervised Learning for BERT<TITLE>Parameter Reduction and Self-Supervised Loss for BERT<TITLE>Parameter Reduction and Self-Supervised Learning for BERT<TITLE>Parameter Reduction and Self-Supervised Learning for BERT<bart_xsum>,<TITLE>Improved Model Size with Multi-Sentence Coherence<TITLE>Improved Model Size with Multi-Sentence Inputs<TITLE>Improved Model Size for Pretraining Natural Language Representations<TITLE>Improved Model Size with Parameter Reduction<TITLE>Improved Model Size with Parameterreduction<t5_small>,<TITLE> Parameter Reduction for Neural Machine Translation<TITLE> Parameter Reduction Techniques for Neural Machine Translation<TITLE> Parameter Reduction for Natural Language Processing<TITLE> Parameter Reduction Techniques for Natural Language Processing<TITLE> Reducing Memory Usage for Natural Language Processing<gpt2>,<TITLE>Parameter Reduction and Self-Supervised Coherence for BERT<TITLE>Parameter Reduction and Self-supervised Coherence Loss for BERT<TITLE>Parameter Reduction and Self-supervised Coherence Learning for BERT<TITLE>Parameter Reduction and Self-Supervised Loss for BERT<TITLE>Parameter Reduction for BERT<pegasus_xsum>
27,"With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",Curriculum Learning for Natural Language Understanding<original>,<TITLE>Curriculum Learning for Language Models with Difficulty<TITLE>Curriculum Learning for Natural Language Understanding<TITLE>Curriculum Learning for Language Model Understanding<TITLE>Curriculum Learning for Language Modeling<TITLE>Curriculum Learning for Language Models<bart_base>,<TITLE>Curriculum Learning for Natural Language Understanding: An Easy-to-Difficult Curriculum Based on Cross-Lingual<TITLE>Curriculum Learning for Natural Language Understanding: An Easy-to-Difficult Curriculum Based on a Cross-Ling<TITLE>Curriculum Learning for Natural Language Understanding: An Easy-to-Difficult Curriculum Based on the Cross-Ling<TITLE>Curriculum Learning for Natural Language Understanding: An Easy-to-Difficult Curriculum Based on a Cross-Way Approach<TITLE>Curriculum Learning for Natural Language Understanding: An Easy-to-Difficult Curriculum Based on the Cross-Way Training<bart_cnn>,<TITLE>Curriculum Learning for Natural Language Understanding: An Easy-to-Difficult Approach<TITLE>Curriculum Learning: An Easy-to-Difficult Approach for Natural Language Understanding<TITLE>Curriculum Learning for Natural Language Understanding Tasks<TITLE>Curriculum Learning for Natural Language Understanding Tasks<TITLE>Curriculum Learning for Natural Language Understanding Tasks<bart_xsum>,<TITLE>Pretrain-Finetune Curriculum for Natural Language Understanding<TITLE>Pretrain-finetune Curriculum for Natural Language Understanding<TITLE>Curriculum Learning for Natural Language Understanding Tasks<TITLE>Curriculum Learning for Natural Language Understanding<TITLE>Curriculum Learning for Language Models<t5_small>,<TITLE> Curriculum Learning for Natural Language Understanding<TITLE> Curriculum Learning for Language Models<TITLE> Curriculum Learning for Pre-trained Language Models<TITLE> Curriculum Learning for Natural Language Understanding Tasks<TITLE> Curriculum Learning for Language Understanding<gpt2>,<TITLE>Curriculum Learning for Pre-trained Language Models<TITLE>Curriculum Learning for Pre-Trained Language Models<TITLE>Curriculum Learning for Natural Language Understanding Tasks<TITLE>Curriculum Learning for Natural Language Understanding<TITLE>Curriculum Learning for Language Models<pegasus_xsum>
28,"We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.",Higher-Order Correlation Clustering for Image Segmentation<original>,"<TITLE>Adaptive Skills, Adaptive Partitions: An Application to Lifelong Learning<TITLE>Adaptive Skills, Adaptive Partitions for Lifelong Skill Learning<TITLE>Adaptive Skills, Adaptive Partitions for Lifelong Learning<TITLE>Applying Adaptive Skills to Lifelong Learning<TITLE>Adaptive Skills, Adaptive Partitions<bart_base>","<TITLE>Adaptive Skills, Adaptive Partitions and Where to Go From There: A Case Study on Lifelong Learning with Application to<TITLE>Adaptive Skills, Adaptive Partitions and Where to Go From There: A Case Study on Lifelong Learning with Local Optim<TITLE>Adaptive Skills, Adaptive Partitions and Where to Go From There: Achieving Lifelong Learning Tasks with Local<TITLE>Adaptive Skills, Adaptive Partitions and Where to Go From There: A Case Study on Lifelong Learning with Applications to<TITLE>Adaptive Skills, Adaptive Partitions and Where to Go From There: Achieving Lifelong Learning Tasks in a<bart_cnn>","<TITLE>Adaptive Skills, Adaptive Partitions: Where to Learn and Where to Apply Them<TITLE>Adaptive Skills, Adaptive Partitions: Where to Learn and Where to Reuse<TITLE>Adaptive Skills, Adaptive Partitions: Where to Learn and Where to Apply Skills<TITLE>Adaptive Skills, Adaptive Partitions: Where to Learn and Where to Apply Them<TITLE>Adaptive Skills, Adaptive Partitions: Where to Learn and Where to Reuse<bart_xsum>","<TITLE>The Adaptive Skills, Adaptive Partitions Framework<TITLE>ASAP: Adaptive Skills, Adaptive Partitions<TITLE>Adaptive Skills, Adaptive Partitionitions<TITLE>Adaptive Skills, Adaptive Partitions<TITLE>Adaptive Skills and Adaptive Partitions<t5_small>","<TITLE> Adaptive Skills, Adaptive Partitions<TITLE> Adaptive Skills: Learning Where to Apply Them<TITLE> Adaptive Skills: Learning Where To Apply Them<TITLE> Adaptive Skill Learning<TITLE> Adaptive Skills: Learning Temporally Extended Actions or Options<gpt2>","<TITLE>Adaptive Skills, Adaptive Partitions: A General Skill Learning Framework<TITLE>Adaptive Skills, Adaptive Partitions: A Framework for Lifelong Learning<TITLE>Learning Where to Apply: Adaptive Skills, Adaptive Partitions<TITLE>Adaptive Skills, Adaptive Partitions: Learning Where to Apply<TITLE>The Adaptive Skills, Adaptive Partitions Framework<pegasus_xsum>"
29,"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",COMET: A Neural Framework for MT Evaluation<original>,<TITLE>ComET: A Neural Framework for Multilingual Machine Translation Evaluation<TITLE>A Neural Framework for Multilingual MT Evaluation with Human Judgements<TITLE>ComET: A Neural Framework for Multilingual MT Evaluation<TITLE>ComET: Neural Machine Translation Evaluation with Human Judgements<TITLE>A Neural Framework for Multilingual Machine Translation Evaluation<bart_base>,<TITLE>Cross-Lingual Multilingual Machine Translation Evaluation with Human Judgements and Direct Assessments: A Neural Framework for Machine<TITLE>Cross-Lingual Multilingual Machine Translation Evaluation with Human Judgements and Direct Assessments: Neural Framework for Machine Translation<TITLE>Cross-Lingual Multilingual Machine Translation Evaluation with Human Judgements and Direct Assessments: A Case Study on Machine<TITLE>Cross-Lingual Multilingual Machine Translation Evaluation with Human Judgements and Direct Assessments: A Neural Framework for Domain<TITLE>Cross-Lingual Multilingual Machine Translation Evaluation with Human Judgements and Direct Assessments: A Case Study on the<bart_cnn>,<TITLE>ComET: A Neural Framework for Multilingual Machine Translation Evaluation<TITLE>ComET: A Neural Framework for Multilingual Machine Translation Evaluation Models<TITLE>ComET: A Neural Framework for Multilingual Machine Translation Evaluation<TITLE>ComET: A Neural Framework for Multilingual Machine Translation Evaluation Models<TITLE>ComET: A Neural Framework for Multilingual Machine Translation Evaluation<bart_xsum>,<TITLE>COMET: A Neural Framework for Multilingual Machine Translation Evaluation<TITLE>CoMET: A Neural Framework for Multilingual Machine Translation Evaluation<TITLE>COMET: A Multilingual Machine Translation Evaluation Framework<TITLE>COMET: A Multilingual Machine Translation Evaluation Model<TITLE>Learning Multilingual Machine Translation Evaluation Models<t5_small>,<TITLE> Comet: A Neural Framework for Training Multilingual Machine Translation Evaluation Models<TITLE> Comet: A Neural Framework for Machine Translation Evaluation<TITLE> Comet: A Neural Framework for Training Multilingual Machine Translation Evaluation<TITLE> Comet: A Neural Framework for Training Multilingual Machine Translation Evaluation Model<TITLE> Comet: A Neural Framework for Training Multilingual Machine Translation<gpt2>,<TITLE>COMET: A Neural Framework for Training MT Evaluation Models with Human Judgements<TITLE>COMET: A Neural Framework for Training Multilingual Machine Translation Evaluation Models<TITLE>COMET: A Neural Framework for Training Multilingual Machine Translation Evaluation Models<TITLE>COMET: A Neural Framework for Training Multilingual MT Evaluation Models<TITLE>COMET: A Neural Framework for Training MT Evaluation Models<pegasus_xsum>
30,"In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to neural style representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.",Unsupervised Learning of Artistic Styles with Archetypal Style Analysis<original>,<TITLE>Unsupervised Learning of Artistic Style from Collections of Artworks<TITLE>Unsupervised Learning of Artistic Style Representations from Large Collections<TITLE>Unsupervised Learning of Artistic Style from Large Collections<TITLE>Unsupervised Learning of Artistic Style from Abstractions<TITLE>Unsupervised Archetypal Analysis of Artworks<bart_base>,"<TITLE>Unsupervised Style Discovery, Summarization, and Manipulation from Large Collections of Artworks using Archetypal Analysis.<TITLE>Unsupervised Style Discovery, Summarization, and Manipulation from Large Collections of Artworks with Archetypal Analysis.<TITLE>Unsupervised Style Discovery, Summarization, and Manipulation from Large Collections of Artworks using Archetypal Analysis:<TITLE>Unsupervised Style Discovery, Summarization, and Manipulation from Large Collections of Artworks with Archetypal Analysis:<TITLE>Unsupervised Style Discovery, Summarization, and Manipulation from Large Collections of Artworks with Archetypal Analysis Based<bart_cnn>",<TITLE>Unsupervised Style Discovery from Large Artworks via Archetypal Analysis<TITLE>Unsupervised Style Discovery from Large Artworks using Archetypal Analysis<TITLE>Unsupervised Discovery of Artistic Styles from Large Collections of Paintings<TITLE>Unsupervised Style Discovery from Artworks via Archetypal Analysis<TITLE>Unsupervised Style Discovery from Large Collections of Artworks<bart_xsum>,<TITLE>Unsupervised Learning of Artistic Styles from Paintings<TITLE>Unsupervised Learning of Art Styles from Large Collections<TITLE>Unsupervised Learning of Archetypal Styles<TITLE>Unsupervised Learning of Archetypal Analysis<TITLE>Unsupervised Learning of Artistic Styles<t5_small>,<TITLE> Learning Archetypal Style Representations from Artworks<TITLE> Unsupervised Learning of Artistic Style from Large Collections<TITLE> Unsupervised Learning of Artistic Style Representations<TITLE> Unsupervised Learning of Artistic Style<TITLE> Unsupervised Learning of Artistic Style from Fine Artworks<gpt2>,<TITLE>Unsupervised Discovery and Manipulation of Artistic Styles from Artworks<TITLE>Unsupervised Learning of Artistic Style Representations from Artworks<TITLE>Unsupervised Discovery and Manipulation of Artistic Styles from Large Collections<TITLE>Unsupervised Learning of Artistic Styles from Artworks<TITLE>Unsupervised Learning of Artistic Style Representations<pegasus_xsum>
31,"In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our model assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.",Associative Multichannel Autoencoder for Multimodal Word Representation<original>,<TITLE>Learning Multimodal Word Representations with Associative Multichannel Autoencoder<TITLE>Learning Multimodal Word Representations with Associative Multichannel Autoencoders<TITLE>Learning Multimodal Word Representations via Associative Multichannel Autoencoder<TITLE>Learning Multimodal Word Representations by Associative Multichannel Autoencoder<TITLE>A Multichannel Autoencoder for Learning Multimodal Word Representations<bart_base>,<TITLE>Associative Multichannel Autoencoder for Learning Multimodal Word Representations with Attribute Recurrent Neural<TITLE>Associative Multichannel Autoencoder for Learning Multimodal Word Representations with Attribute Recurrent Networks<TITLE>Associative Multichannel Autoencoder for Learning Multimodal Word Representations with Attribute-Gated<TITLE>Associative Multichannel Autoencoder for Learning Multimodal Word Representations with Attribute-Based Associ<TITLE>Associative Multichannel Autoencoder for Learning Multimodal Word Representations with Attribute Recurrent Memory<bart_cnn>,<TITLE>Learning Multimodal Word Representations with Associative Multichannel Autoencoders<TITLE>Learning Multimodal Word Representations with Associative Multichannel Autoencoder<TITLE>Learning Multimodal Word Representations with Associative Autoencoders<TITLE>Learning Multimodal Word Representations with Associative Autoencoder<TITLE>Learning Multimodal Word Representations with Associative Autoencoders<bart_xsum>,"<TITLE>Learning Multimodal Word Representations by Integrating Textual, Visual and Auditive Input<TITLE>Learning Multimodal Word Representations by Combining Textual, Visual and Auditive Input<TITLE>Learning Multimodal Word Representations with Textual, Visual and Auditive Inputs<TITLE>Associative Multichannel Autoencoder for Learning Multimodal Word Representations<TITLE>Associative Multichannel Autoencoders<t5_small>",<TITLE> Learning Multimodal Word Representations with Associative Multichannel Autoencoders<TITLE> Learning Multimodal Word Representations using Associative Multichannel Autoencoders<TITLE> Learning Multimodal Word Representations via Associative Multichannel Autoencoder<TITLE> Learning Multimodal Word Representations via Associative Multichannel Autoencoders<TITLE> Learning Multimodal Word Representations with Associative Multichannel Autoencoder<gpt2>,<TITLE>Associative Multimodal Autoencoder for Concept Similarity<TITLE>A Multimodal Autoencoder Based on Associative Memory<TITLE>A Multimodal Autoencoder Based on Associative Memory<TITLE>Associative Multimodal Autoencoder<TITLE>Associative Multimodal Autoencoder<pegasus_xsum>
32,"When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",Balancing Training for Multilingual Neural Machine Translation<original>,<TITLE>Learning to Weight Training Data for Multilingual Machine Translation<TITLE>Optimizing Training Data for Multilingual Machine Translation<TITLE>Weighted Training of Multilingual Machine Translation Models<TITLE>Learning to Weight Training Data for Multilingual MT<TITLE>Weighted Training for Multilingual Machine Translation<bart_base>,<TITLE>Optimizing Training Data for Multilingual Machine Translation with Imbalanced Training Set Learning and Up-Sample Less Refresourced Languages<TITLE>Optimizing Training Data for Multilingual Machine Translation with Imbalanced Training Set Learning and Up-Sample Less Refranco Sam<TITLE>Optimizing Training Data for Multilingual Machine Translation with Imbalanced Training Set Learning and Up-Sample Less Refranz Languages<TITLE>Optimizing Training Data for Multilingual Machine Translation with Imbalanced Training Set Learning and Up-Sample Less Refranz Sam<TITLE>Optimizing Training Data for Multilingual Machine Translation with Imbalanced Training Set Learning and Up-Sample Less Refrancoage<bart_cnn>,<TITLE>Learning to Optimize Training Data for Multilingual Machine Translation<TITLE>Learning to Weight Training Data for Multilingual Machine Translation Models<TITLE>Automatic Weighting of Training Data for Multilingual MT<TITLE>Learning to Weight Training Data for Multilingual Machine Translation<TITLE>Automatic Weighting for Multilingual Machine Translation Models<bart_xsum>,<TITLE>Learning to Weight Multilingual Machine Translation with Data Scorers<TITLE>Learning to Weight Training Data for Multilingual Machine Translation Models<TITLE>Learning to Weight Training Data through a Data Scorer<TITLE>Learning to Weight Training Data for Multilingual Machine Translation<TITLE>Learning to Weight Multilingual Machine Translation Models<t5_small>,<TITLE> Learning to Weight Training Data for Multilingual Machine Translation<TITLE> Learning to Weight Multilingual Machine Translation<TITLE> Improving Multilingual Machine Translation by Weighting Training Data<TITLE> Optimizing for Multilingual Machine Translation<TITLE> Learning to Weight Multilingual Machine Translation via Data Scoring<gpt2>,<TITLE>Automatically Learning to Weight Training Data for Multi-lingual Machine Translation<TITLE>Learning to Weight Training Data for Multi-Lingual MT<TITLE>Learning to Weight Training Data for Multi-lingual Machine Translation<TITLE>Learning to Weight Training Data for Multi-lingual Machine Translation Models<TITLE>Learning to Weight Training Data for Multi-Language Machine Translation<pegasus_xsum>
33,"A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.",Hierarchical Phrase-Based Translation with Suffix Arrays<original>,<TITLE>Hierarchical Phrase-Based Translation via On-the-fly lookup<TITLE>Lookup Algorithms for Hierarchical Phrase-Based Translation<TITLE>Hierarchical Phrase-Based Translation with Senses with Gaps<TITLE>Lookup Algorithms for Hierarchical Phrase-based Translation<TITLE>Hierarchical Phrase-Based Translation with Gaps<bart_base>,<TITLE>Lookup Algorithms for Hierarchical Phrase-Based Translation with Gaps: An Efficient Lookup Algorithm<TITLE>Lookup Algorithms for Hierarchical Phrase-Based Translation with Gaps: An Effective Lookup Algorithm and<TITLE>Lookup Algorithms for Hierarchical Phrase-Based Translation with Gaps: An Effective Lookup Algorithm Using<TITLE>Lookup Algorithms for Hierarchical Phrase-Based Translation with Gaps: An Efficient Approach to Fly-<TITLE>Lookup Algorithms for Hierarchical Phrase-Based Translation with Gaps: An Efficient Lookup on the<bart_cnn>,<TITLE>Hierarchical Phrase-Based Translation with Gaps: Faster Lookup Algorithms<TITLE>Hierarchical Phrase-Based Translation with Gaps<TITLE>Hierarchical Phrase-based Translation with Gaps<TITLE>Hierarchical Phrase-Based Translation with Gaps<TITLE>Hierarchical Phrase-Based Translation with Gaps<bart_xsum>,<TITLE>Hierarchical Phrase-Based Translation with Gaps<TITLE>Lookup for Hierarchical Phrase-Based Translation<TITLE>Hierarchical Phrase-Based Translation<TITLE>Hierarchical Phrase-based Translation<TITLE>Hierarchical Phrase Based Translation<t5_small>,<TITLE> Lookup Algorithms for Hierarchical Phrase-Based Translation<TITLE> Lookup Algorithms for Hierarchical Phrase-based Translation<TITLE> Hierarchical Phrase-Based Translation Algorithms<TITLE> Hierarchical Phrase-Based Translation<TITLE> Hierarchical Phrase-Based Translation with Gap Search<gpt2>,<TITLE>Hierarchical Phrase-Based Translation with Gap Indexing<TITLE>Hierarchical Phrase-based Translation with Gap Indexing<TITLE>Hierarchical Phrase-Based Translation with Gap Index Rules<TITLE>Hierarchical Phrase-Based Translation with Gaps<TITLE>Hierarchical Phrase-based Translation with Gaps<pegasus_xsum>
34,"We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.",Hybrid Simplification using Deep Semantics and Machine Translation<original>,<TITLE>A Hybrid Approach to Sentence Simplification with Deep Semantics and Monolingual Translation<TITLE>A Hybrid Approach to Sentence Simplification with Deep Semantics and Monolingual MT<TITLE>Simple Sentence Simplification with Deep Semantics and Monolingual Machine Translation<TITLE>Sentence Simplification with Deep Semantics and Monolingual Machine Translation<TITLE>A Hybrid Approach to Sentence Simplification<bart_base>,"<TITLE>A Hybrid Approach to Simple Sentence Simplification with Deep Semantic and Monolingual Translation Model for Phrase Substitution and<TITLE>A Hybrid Approach to Simple Sentence Simplification with Deep Semantic and Monolingual Machine Translation for Phrase Substitution and<TITLE>A Hybrid Approach to Simple Sentence Simplification with Deep Semantic and Monolingual Machine Translation Model for Phrase Substitution<TITLE>A Hybrid Approach to Simple Sentence Simplification with Deep Semantic and Monolingual Translation Model for Complex Sentence One-<TITLE>A Hybrid Approach to Simple Sentence Simplification with Deep Semantic and Monolingual Translation Model for Phrase Substitution,<bart_cnn>",<TITLE>A Hybrid Approach to Sentence Simplification Using Deep Semantics and Monolingual Machine Translation<TITLE>A Hybrid Approach to Sentence Simplification with Deep Semantics and Monolingual Machine Translation<TITLE>A Hybrid Approach to Sentence Simplification Based on Deep Semantics and Monolingual Translation<TITLE>A Hybrid Approach to Sentence Simplification Using Deep Semantics and Monolingual Machine Translation<TITLE>A Hybrid Approach to Sentence Simplification Using Deep Semantics and Monolingual Translation<bart_xsum>,<TITLE>Combining Deep Semantic and Monolingual Machine Translation for Sentence Simplification<TITLE>Combining Deep Semantics and Monolingual Machine Translation for Sentence Simplification<TITLE>Hybrid Sentence Simplification with Deep Semantics<TITLE>A Hybrid Approach to Sentence Simplification<TITLE>Hybrid Approach to Sentence Simplification<t5_small>,<TITLE> A Hybrid Approach to Sentence Simplification<TITLE> Simple Sentence Simplification Using Deep Semantics and Monolingual Machine Translation<TITLE> Simple Sentence Simplification using Deep Semantics and Monolingual Machine Translation<TITLE> Simple Sentence Simplification Using Deep Semantics and Monolingual Translation<TITLE> Simple Sentence Simplification Using Deep Semantics and Monolingual Translation Models<gpt2>,<TITLE>A Hybrid Approach to Sentence Simplification with Deep Semantics and Monolingual Translation<TITLE>Combining Deep Semantics and Monolingual Machine Translation for Simple Sentence Simplification<TITLE>Combining Deep Semantics and Monolingual Machine Translation for Sentence Simplification<TITLE>Sentence Simplification with Deep Semantics and Monolingual Machine Translation<TITLE>A Hybrid Approach to Sentence Simplification<pegasus_xsum>
35,"Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.",A Topic Similarity Model for Hierarchical Phrase-based Translation<original>,<TITLE>Topic Similarity Modeling for Hierarchical Phrase-Based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-Based Machine Translation<TITLE>Topic Similarity Modeling for Hierarchical Phrase-based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-based Translation<bart_base>,<TITLE>A Topic Similarity Model for Hierarchical Phrase-Based Statistical Machine Translation Using Synchronous Rule Level for SMT<TITLE>A Topic Similarity Model for Hierarchical Phrase-Based Statistical Machine Translation Using Synchronous Rule Level for Word-<TITLE>A Topic Similarity Model for Hierarchical Phrase-based Statistical Machine Translation Using Synchronous Rule Level for Word-<TITLE>A Topic Similarity Model for Hierarchical Phrase-Based Statistical Machine Translation using Synchronous Rule Level for Word-<TITLE>A Topic Similarity Model for Hierarchical Phrase-Based Statistical Machine Translation Using Synchronous Rule Level and Word Distribution<bart_cnn>,<TITLE>A Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE>A Topic Similarity Model for Hierarchical Phrase-based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-based Translation<bart_xsum>,<TITLE>Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-based Translation<TITLE>Topic Similarity for Hierarchical Phrase-Based Translation<TITLE>Topic Similarity Model for Hierarchical Phrase Based Translation<TITLE>Topic Similarity Model for Statistical Machine Translation<t5_small>,<TITLE> A Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE> A Topic Model for Hierarchical Phrase-Based Translation<TITLE> A Topic Model for Hierarchical Phrase-Based Statistical Machine Translation<TITLE> Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE> A Topic Similarity Model for Hierarchical Phrase-based Translation<gpt2>,<TITLE>Topic Similarity Modeling for Hierarchical Phrase-Based Statistical Machine Translation<TITLE>Topic Similarity Model for Hierarchical Phrase-Based Statistical Machine Translation<TITLE>A Topic Similarity Model for Hierarchical Phrase-Based Translation<TITLE>A Topic Similarity Model for Hierarchical Phrase-based Translation<TITLE>Topic Similarity Modeling for Hierarchical Phrase-Based Translation<pegasus_xsum>
36,"This paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision. The approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts, which are automatically extracted via a language-independent bootstrapping process. The induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data. Performance is evaluated based on both a test set of handlabeled multi-referent personal names and via automatically generated pseudonames.",Unsupervised Personal Name Disambiguation<original>,<TITLE>Unsupervised Clustering of Personal Names with Multiple Real Referents<TITLE>Unsupervised Clustering of Personal Name with Multiple Real Referents<TITLE>Unsupervised Clustering of Personal Names with Multiple Real referents<TITLE>Unsupervised Clustering of Personal Names Using Biographic Facts<TITLE>Multi-Referent Personal Names with Multiple Real Referents<bart_base>,<TITLE>Unsupervised Personal Name Extraction from Multi-Referent Data using Pseudo-on-line Clustering of Ent<TITLE>Unsupervised Personal Name Extraction from Multi-Referent Data using Pseudo-on-line Clustering of Text<TITLE>Unsupervised Personal Name Extraction from Multi-Referent Data using Pseudo-on-line Bootstrapping and Un<TITLE>Unsupervised Personal Name Extraction from Multi-Referent Data using Pseudo-on-line Clustering and Boot<TITLE>Unsupervised Personal Name Extraction from Multi-Referent Data using Pseudo-on-line Clustering of Names<bart_cnn>,<TITLE>Unsupervised Multi-Referent Personal Name Clustering with Biographic Data<TITLE>Unsupervised Multi-Referent Personal Name Recognition with Biographic Data<TITLE>Unsupervised Multi-Referent Personal Name Recognition with Biographic Data<TITLE>Unsupervised Clustering of Multi-Referent Personal Names<TITLE>Unsupervised Multi-Referent Personal Name Recognition<bart_xsum>,<TITLE>Unsupervised Clustering of Personal Names with Multiple Real Referents<TITLE>Discriminating Personal Names with Multiple Real Referents in Text<TITLE>Identifying Personal Names with Multiple Real Referents in Text<TITLE>Learning to distinguish Personal Names with Multiple Real Referents in Text<TITLE>Learning to distinguish personal names with multiple real referents in text<t5_small>,<TITLE> Identifying Personal Names with Multiple Real Referents<TITLE> A Clustering Approach to Identifying Personal Names with Multiple Real Referents<TITLE> Automatic Identification of Personal Names with Multiple Real Referents<TITLE> Identifying Personal Names with Multiple Real Referents<TITLE> Identifying Personal Names with Multiple Real Referents in Text<gpt2>,<TITLE>Unsupervised Clustering of Personal Names with Multiple Real Referents<TITLE>Unsupervised Multi-referent Personal Name Disambiguation<TITLE>Unsupervised Clustering of Multi-referent Personal Names<TITLE>Unsupervised Clustering of Personal Names over Rich Feature Space<TITLE>Unsupervised Multi-referent Personal Name Identification<pegasus_xsum>
37,"Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.",Grammar as a Foreign Language<original>,<TITLE>A Domain-Agnostic Attention-enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>A Domain-Agnostic Attention-Enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>A Domain-Agnostic Attention-Enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>A Domain-Agnostic Attention-enhanced Sequence-to-Sequence Model for Syntactic Constituent Parsing<TITLE>A Domain-Agnostic Attention-Enhanced Sequence-to-Sequence Model for Syntactic Constituent Parsing<bart_base>,<TITLE>Attention-enhanced Sequence-to-Sequence Parsing for Syntactic Constituency Parsing with Domain Agnostic<TITLE>Attention-enhanced Sequence-to-Sequence Parsing with Domain Agnostic Attention for Syntactic Constituency Pars<TITLE>Attention-enhanced Sequence-to-Sequence Parsing for Syntactic Constituency Parsing using Domain Agnostic<TITLE>Attention-enhanced Sequence-to-Sequence Parsing for Syntactic Constituency Parsing with Domain-Ag<TITLE>Attention-enhanced Sequence-to-Sequence Parsing for Syntactic Constituency Parsing with Domain-A<bart_cnn>,<TITLE>Domain Agnostic Attention-enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>Domain Agnostic Attention-enhanced Sequence-to-Sequence Models for Syntactic Constituency Parsing<TITLE>Domain Agnostic Attention-enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>Domain Agnostic Attention-enhanced Sequence-to-Sequence Parsing<TITLE>Domain Agnostic Attention-enhanced Sequence-to-Sequence Parsing<bart_xsum>,<TITLE>Domain agnostic Attention-Enhanced Sequence-to-Sequence Parsing<TITLE>Domain Agnostic Attention-Enhanced Sequence-to-Sequence Parsing<TITLE>Domain Agnostic Attention Enhanced Sequence-to-Sequence Parsing<TITLE>Domain Attention-Enhanced Sequence-to-Sequence Parsing<TITLE>Domain Agnostic Attention-Enhanced Semantic Parsing<t5_small>,<TITLE> Domain-Agnostic Attention-Enhanced Sequence-to-Sequence Models for Syntactic Constituency Parsing<TITLE> Domain-Agnostic Attention-Enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE> Domain Agnostic Attention-Enhanced Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE> Domain Adaptation for Syntactic Constituency Parsing<TITLE> Attention-enhanced Sequence-to-Sequence Model for Domain-Agnostic Constituency Parsing<gpt2>,<TITLE>A Sequence-to-Sequence Model for Syntactic Constituency Parsing with Attention<TITLE>An Efficient Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>A Sequence-to-Sequence Model for Syntactic Constituency Parsing<TITLE>Domain Attention-Enhanced Sequence-to-Sequence Parsing<TITLE>Syntactic Constituency Parsing with Domain Attention<pegasus_xsum>
38,"Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite number of loss functions on a Riemannian manifold. The present paper proposes a Riemannian stochastic recursive gradient algorithm (R-SRG), which does not require the inverse of retraction between two distant iterates on the manifold. Convergence analyses of R-SRG are performed on both retractionconvex and non-convex functions under computationally efficient retraction and vector transport operations. The key challenge is analysis of the influence of vector transport along the retraction curve. Numerical evaluations reveal that R-SRG competes well with state-of-the-art Riemannian batch and stochastic gradient algorithms.",Riemannian Stochastic Recursive Gradient Algorithm<original>,<TITLE>Riemannian Stochastic Recursive Gradient Algorithm<TITLE>Riemannian Stochastic Recursive Gradient Algorithms<TITLE>Riemannian Stochastic Recursive Gradient Algorithm.<TITLE>Riemannian Stochastic Residual Gradient Algorithm<TITLE>Riemannian stochastic recursive gradient algorithms.<bart_base>,<TITLE>Riemannian stochastic gradient descent for vector transport and R-SRG with distant iterates on the manifold.<TITLE>Riemannian stochastic gradient descent for vector transport and R-SRG with distant iterates on Manifolds<TITLE>Riemannian stochastic gradient descent for vector transport and R-SRG with distant iterates on Manifold<TITLE>Riemannian stochastic gradient descent for vector transport and R-SRG with distant iterates on manifolds.<TITLE>Riemannian stochastic gradient descent for vector transport and R-SRG with distant iterates on a manifold.<bart_cnn>,<TITLE>Riemannian Stochastic Recursive Gradient Algorithms<TITLE>Riemannian Stochastic Recursive Gradient Algorithm<TITLE>Riemannian Stochastic Recursive Gradient Algorithm<TITLE>Riemannian Stochastic Recursive Gradient Algorithm<TITLE>Riemannian Stochastic Recursive Gradient<bart_xsum>,<TITLE>A Riemannian Stochastic Recursive Gradient Algorithm<TITLE>Riemannian Stochastic Recursive Gradient Algorithms<TITLE>Riemannian Stochastic Recursive Gradient Algorithm<TITLE>Riemannian Stochastic Variance Reduced Algorithms<TITLE>Stochastic Variance Recursive Gradient Algorithms<t5_small>,<TITLE> Riemannian Stochastic Recursive Gradient Algorithm<TITLE> Riemannian Stochastic Recursive Gradient<TITLE> Riemannian Stochastic Gradient Algorithms<TITLE> Riemannian Stochastic Variance Reduction Algorithms<TITLE> Riemannian Variance Reduction Algorithms<gpt2>,<TITLE>Riemannian Gradient Algorithms for Stochastic Variance Reduction.<TITLE>Riemannian Gradient Algorithms for Stochastic Variance Reduction<TITLE>Riemannian Gradient Algorithms for Statistical Variance Reduction<TITLE>Riemannian Gradient Algorithms for Variance Reduction<TITLE>Riemannian Gradient Algorithms<pegasus_xsum>
39,"This paper presents a novel Gaussian process (GP) approach to regression with inputdependent noise rates. We follow Goldberg et al.’s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. In contrast to Goldberg et al., however, we do not use a Markov chain Monte Carlo method to approximate the posterior noise variance but a most likely noise approach. The resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard GPs such as sparse approximations. Extensive experiments on both synthetic and real-world data, including a challenging perception problem in robotics, show the effectiveness of most likely heteroscedastic GP regression.",Most Likely Heteroscedastic Gaussian Process Regression<original>,<TITLE>Heteroscedastic Gaussian Process Regression with Noise Variance<TITLE>A Gaussian Process Approach to Regression with Input-Dependent Noise<TITLE>Gaussian Process Regression with Input-Dependent Noise Variance<TITLE>Gaussian Process Regression with Input-Dependent Noise Rates<TITLE>Gaussian Process Regression with Input-Dependent Noise<bart_base>,<TITLE>Gaussian Process Gaussian Process Regression with Input-dependent Noise Rate Estimation for Heteroscedastic GP Regression<TITLE>Gaussian Process Gaussian Process Regression with Input-dependent Noise Rate Estimation for Heteroscedastic Gaussians<TITLE>Gaussian Process Gaussian Process Regression with Input-dependent Noise Rate Estimation for Heteroscedastic Gaussian<TITLE>Gaussian Process Gaussian Process Regression with Input-dependent Noise Rate for Heteroscedastic Noisy PCA.<TITLE>Gaussian Process Gaussian Process Regression with Input-dependent Noise Rate Estimation for Heteroscedastic Gaussian Systems<bart_cnn>,<TITLE>Gaussian Processes with Input-Dependent Noisy Rates<TITLE>Gaussian Processes with Input-Dependent Noise Rates<TITLE>Gaussian Processes with Input-Dependent Noise<TITLE>Gaussian Processes with Input-Dependent Noise<TITLE>Gaussian Processes with Input-Dependent Noise<bart_xsum>,<TITLE>Gaussian Process Regression with Input-Dependent Noise Rates<TITLE>Gaussian Process GP Regression with Input-dependent Noise Rates<TITLE>Gaussian Process Regression with Input-dependent Noise Rates<TITLE>Gaussian Processes for Noise Rate Regression<TITLE>Gaussian Process Regression with Noise Rates<t5_small>,<TITLE> Gaussian Process Regression with Inputdependent Noise Rates<TITLE> Gaussian Process Regression with Input-dependent Noise Rates<TITLE> Gaussian Process Regression with Input Dependent Noise Rates<TITLE> Gaussian Process Regression with Inputdependent Noise Rate<TITLE> Gaussian Process Regression with Input-dependent Noise Rate<gpt2>,<TITLE>Most likely Heteroscedastic Regression with input-dependent noise<TITLE>Most likely Heteroscedastic Regression with Noise<TITLE>Most likely heteroscedastic GP regression<TITLE>Most likely heteroscedastic GP Regression<TITLE>Most likely Heteroscedastic GP Regression<pegasus_xsum>
