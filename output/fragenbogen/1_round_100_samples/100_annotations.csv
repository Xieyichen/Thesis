,title,abstract,bart_base,bart_cnn,bart_xsum,t5_small,gpt2,pegasus_xsum
0,Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations,"Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.",<TITLE>Quantitatively Analyzing Language Interactions for Zero-Shot Neural MT<TITLE>Quantitatively Analyzing Language Interactions in Zero-Shot Translation<TITLE>Zero-Shot Neural Machine Translation with Language Interactions<TITLE>Zero-Shot Neural Machine Translation with Mutual Information<TITLE>Zero-Shot Neural Machine Translation with Language Identification,<TITLE>Zero-Shot Neural Machine Translation as a Pivot-Based Degeneration of Spurious Correlation between Source and Target Decoding<TITLE>Zero-Shot Neural Machine Translation as a Pivot-Based Degeneration of Spurious Correlation between Source and Target Sentences<TITLE>Zero-Shot Neural Machine Translation as a Pivot-Based Degeneration of Spurious Correlation between Language Sources and Sentences<TITLE>Zero-Shot Neural Machine Translation as a Pivot-Based Degeneration of Spurious Correlation between Source and Target Information with<TITLE>Zero-Shot Neural Machine Translation as a Pivot-Based Degeneration of Spurious Correlation between Source and Target Information.,<TITLE>Pre-training and Back-translation for Zero-Shot Neural Machine Translation in Multilingual Settings<TITLE>Pre-training and Back-translation for Zero-Shot Neural Machine Translation in Multilingual Setting<TITLE>Pre-training and Back-translation for Zero-Shot Neural Machine Translation<TITLE>Pre-training and Back-translation for Zero-Shot Neural Machine Translation<TITLE>Pre-training and Back-translation for Zero-Shot Neural Machine Translation,<TITLE>Null-Shot Translation with Decoder Pre-training and Back-Translation<TITLE>Null-Shot Translation with Decoder Pre-training and Back-translation<TITLE>Null-Shot Translation via Decoding Pre-training and Back-Translation.<TITLE>Null-Shot Translation via Decoding Pre-training and Back-Translation<TITLE>Null-Shot Translation with Decoding Pre-training and Back-Translation,<TITLE> Neural Machine Translation with Arbitrary Correlations<TITLE> Neural Machine Translation with Arbitrary Correlation<TITLE> Neural Machine Translation with No-Pivot Training<TITLE> Neural Machine Translation with No-Pivot Pre-training<TITLE> Neural Machine Translation with Pseudo Pivot-based Training,<TITLE>Decoupling the Source Language and the Target Language for Zero-Shot Neural Machine Translation<TITLE>Decoupling the Source Language and the Target Language for Zero-shot Neural Machine Translation<TITLE>Improving Zero-Shot Neural Machine Translation with Decoding and Back-Translation Methods<TITLE>Improving Zero-Shot Neural Machine Translation with Decoding and Back-translation Methods<TITLE>Improving Zero-Shot Neural Machine Translation with Decoding and Back-Translation
1,Biomedical Entity Representations with Synonym Marginalization,"Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.",<TITLE>Modeling Biomedical Named Entity Normalization with Incomplete Synonyms<TITLE>Modeling Biomedical Named Entity Normalization from Incomplete Synonyms<TITLE>Modeling Biomedical Entity Normalization with Synonyms of Entities<TITLE>Modeling Biomedical Named Entity Normalization with Synonym Selection<TITLE>Modeling Biomedical Named Entity Normalization with Synonyms,<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood Maximization<TITLE>Learning Representations of Biomedical Named Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood Maxim<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood for Text<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximizing Marginal Likelihood for<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood for Normal,<TITLE>Biomedical Entity Normalization based on the Synonym Selection of Entities<TITLE>Biomedical Entity Normalization Based on the Synonym Selection of Entities<TITLE>Biomedical Entity Normalization Based on the Synonym of Entities<TITLE>Biomedical Entity Normalization Using Model-Based Synonym Selection<TITLE>Biomedical Entity Normalization Using Model-Based Candidate Selection,<TITLE>Learning Representations of Biomedical Entities based on Synonyms<TITLE>Learning Representations of Biomedical Entity based on Synonyms<TITLE>Learning Representations of Biomedical Entities Based on Synonyms<TITLE>Learning Biomedical Entity Representations based on Synonyms<TITLE>Learning Representations of Biomedical Entities from Synonyms,<TITLE> Learning Representations of Biomedical Entities Based on Synonyms<TITLE> Learning Representations of Biomedical Entities Based on Synonym Selection<TITLE> Learning Representations of Biomedical Entities with Synonyms<TITLE> Learning Representations of Biomedical Entities by Maximizing Marginal Likelihood<TITLE> Learning Representations of Biomedical Entities by Maximizing Marginal Probability,<TITLE>Learning Representations of Biomedical Named Entities from Incomplete Synonyms<TITLE>Learning Representations of Biomedical Named Expressions from Incomplete Synonyms<TITLE>Learning Representations of Biomedical Named Entities from Incomplete Synonym<TITLE>Learning Biomedical Entity Normalization from Incomplete Synonyms.<TITLE>Learning Biomedical Entity Normalization from Incomplete Synonyms
2,Shaping Visual Representations with Language for Few-Shot Classification,"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.",<TITLE>Language-shaped Learning for Few-Shot Visual Classification<TITLE>Language-shaped Learning for Few-shot Visual Classification<TITLE>Language-Shape Learning for Few-Shot Visual Classification<TITLE>Language-Shape Learning for Few-shot Visual Classification<TITLE>Language-shaped Learning for Few-Shot Image Classification,<TITLE>Language-Shared Few-shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Abstract<TITLE>Language-Shared Few-Shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Abstract<TITLE>Language-Shared Few-shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Text<TITLE>Language-Shared Few-Shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Text<TITLE>Language-Shared Few-shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Image,<TITLE>Language-Shaped Learning for Few-Shot Visual Classification<TITLE>Language-Shaped Learning for Few-shot Visual Classification<TITLE>Language-Shaped Learning for Few-Shot Classification<TITLE>Language-Shaped Learning for Few-Shot Visual Classification<TITLE>Language-Shaped Learning for Few-Shot Visual Classification,<TITLE>Language-shaped Learning for Few-Shot Visual Classification<TITLE>Language-shaped Learning for Few-shot Visual Classification<TITLE>Language-shaped Learning for Visual Classification<TITLE>Language-shaped Learning to Predict Language<TITLE>Language-shaped Learning,<TITLE> Language-Structure Learning for Few-Shot Visual Classification<TITLE> Language-Structure Learning for Few-shot Visual Classification<TITLE> Language-Oriented Few-Shot Visual Classification<TITLE> Language-Sensitive Few-Shot Visual Classification<TITLE> Language-Structure Learning for Visual Classification,<TITLE>Language-Shaped Learning for Few-Shot Visual Classification<TITLE>Language-Shaped Learning for Few-shot Visual Classification<TITLE>Language-shaped Learning for Few-Shot Visual Classification<TITLE>Language-shaped Learning for Few-shot Visual Classification<TITLE>Language-Shaped Learning for Few-Shot Visual Recognition
3,Ellipsis Resolution with Underspecified Scope,"The paper presents an approach to ellipsis resolution in a framework of scope underspecification (Underspecified Discourse Representation Theory). It is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification (Crouch, 1995; Egg et al., 2001) in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation. Furthermore it is shown that the approach presented can cope with the examples discussed by Dalrymple et al. (1991) as well as a problem noted recently by Erk and Koller (2001).",<TITLE>E ellipsis resolution and scope underspecification in Underspecified Discourse Representations<TITLE>E ellipsis resolution and scope underspecification: an application process based approach<TITLE>E ellipsis resolution in a framework of scope underspecification<TITLE>Unsupervised ellipsis resolution and scope underspecification<TITLE>Applying ellipsis resolution to underspecified discourse representation,<TITLE>Ellipsis Resolution in Underspecified Discourse Representation Theory: An Approach to Ellipsis and Scope Und<TITLE>Ellipsis Resolution in Underspecified Discourse Representation Theory: An Application to Ellipsis and Scope Unders<TITLE>Ellipsis Resolution in Underspecified Discourse Representation Theory: An Approach to Ellipsis Descriptors<TITLE>Ellipsis Resolution in Underspecified Discourse Representation Theory: An Application to Ellipsis and Scope Und<TITLE>Ellipsis Resolution in Underspecified Discourse Representation Theory: An Application Processes and anaphora Resolution with,<TITLE>Ellipsis Resolution in a Scope Underspecification Framework<TITLE>Ellipsis Resolution in a Framework of Scope Underspecification.<TITLE>Ellipsis Resolution in a Scope Underspecified Context<TITLE>Ellipsis Resolution in a Framework of Scope Underspecification<TITLE>Ellipsis Resolution in a Scope Underspecification Framework,<TITLE>A Framework for ellipsis resolution in scope underspecification<TITLE>A Framework for ellipsis resolution with scope underspecification<TITLE>A Framework for ellipsis Resolution in scope underspecification<TITLE>Scope Underspecification for ellipsis resolution<TITLE>Scope Underspecification for ellipsis Resolution,<TITLE> Anaphora Resolution and Scope Underspecification<TITLE> An Ellipsis Resolution Approach to Underspecified Discourse<TITLE> A Framework for Ellipsis Resolution<TITLE> An Ellipsis Resolution Approach<TITLE> Anaphora Resolution and Scope Underspecification in Context,<TITLE>ellipsis Resolution in an Underspecified Discourse Representation<TITLE>ellipsis Resolution with Scope Underspecification<TITLE>ellipsis Resolution and Scope Underspecification<TITLE>ellipsis Resolution and Scope underspecification<TITLE>ellipsis Resolution with Scope Underspecification
4,Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing,"A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times.",<TITLE>Estimating High Utility Measures for Psycholinguistic Modeling<TITLE>Estimating High Utility Measures for Psycholinguistic Models<TITLE>Estimating High Utility Measures from Incremental Parser Output<TITLE>Estimating High Utility Measures from Incremental Parsing Output<TITLE>Estimating High Utility Measures from Incremental Parsers,<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and Entropy Measures from a Single Incremental<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and Entropy Measures for Predicting Human Reading<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser using a Lexicalized PCFG and Ent<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and a Lexicalized PCFG-Based<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and a Lexicalized PCFG-based,<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling and Reading Times<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling and Prediction<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling<TITLE>Incremental Measures of High Utility in Psycholinguistic Modeling<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling,<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures from a Single Incremental Parser<TITLE>Calculing Separate Lexical and Syntactic Surprisal Measures from a Single Incremental Parser<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures from a Single Incentive Parser<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures from a Single Incentives Parser<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures using a Lexicalized PCFG,<TITLE> Incremental Parsing with Lexicalized PCFGs<TITLE> Incremental Parsing Using Lexicalized PCFGs<TITLE> Incremental Parsing with Lexicalized PCFG Measures<TITLE> Incremental Incremental Parsing with Lexicalized PCFG<TITLE> Incremental Incremental Parsing Using Lexicalized PCFG,<TITLE>Lexical and Syntactic Surprisal Measures from an Incremental PCFG<TITLE>Lexical and Syntactic Surprisal Measures for Incremental PCFGs<TITLE>Lexical and Syntactic Surprisal Measures of Incremental PCFG Output<TITLE>Lexical and Syntactic Surprisal Measures for Incremental PCFG Output<TITLE>Lexical and Syntactic Surprisal Measures for Incremental PCFG
5,Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction,"We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.",<TITLE>Learning Probabilistic Grammar Priors with Shared Logistic Normal Distributions<TITLE>Learning Probabilistic Grammars with Shared Logistic Normal Distributions<TITLE>Learning Probabilistic Grammars with Shared Logistic Normal Distributions.<TITLE>Learning Probabilistic Grammars using Shared Logistic Normal Distributions.<TITLE>Learning Probabilistic Grammars using Shared Logistic Normal Distributions,<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Probabilistic Priors over Probabil<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Variational EM Algorithms over Prob<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Probabilistic Priors over Belief Networks<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Probabilistic Priors over Belief Prop<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Variational EM Algorithms over Pri,<TITLE>A Shared Logistic Normal Distribution for Learning Probabilistic Grammar<TITLE>A Shared Logistic Normal Distribution for Grammar Induction<TITLE>A Shared Logistic Normal Distribution for Grammar Induction<TITLE>A Shared Logistic Normal Distribution for Grammar Learning<TITLE>A Shared Logistic Normal Distribution for Grammar Induction,<TITLE>Partitioned Logistic Normal Distribution for Probabilistic Grammars<TITLE>Partitioned Logistic Normal Distribution of Probabilistic Grammars<TITLE>A Variational EM algorithm for learning Probabilistic Grammars<TITLE>Partitioned Logistic Normal Distribution for Probabilistic Grammar Knowledge<TITLE>Partitioned Logistic Normal Distribution for Probabilistic Grammar,<TITLE> Learning Probabilistic Grammars with Shared Logistic Normal Distributions<TITLE> Unsupervised Probabilistic Grammar Induction and Bilingual Learning<TITLE> Unsupervised Probabilistic Grammar Induction<TITLE> Learning Probabilistic Grammars with Shared Logistic Regular Distributions<TITLE> Unsupervised Dependency Grammar Induction and Bilingual Learning,<TITLE>A Shared Logistic Normal Distribution for Learning Probabilistic Grammars<TITLE>A Family of Priors over Probabilistic Grammar Weights<TITLE>A Shared Logistic Normal Distribution for Probabilistic Grammar Induction<TITLE>A Shared Logistic Normal Distribution for Probabilistic Grammars<TITLE>A Shared Logistic Normal Distribution for Unsupervised Grammar Induction
6,Inherent Disagreements in Human Textual Inferences,"We analyze human's disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation ""noise"", but rather persist as we collect more ratings and as we vary the amount of context provided to raters. We further show that the type of uncertainty captured by current state-of-the-art models for natural language inference is not reflective of the type of uncertainty present in human disagreements. We discuss implications of our results in relation to the recognizing textual entailment (RTE)/natural language inference (NLI) task. We argue for a refined evaluation objective that requires models to explicitly capture the full distribution of plausible human judgments.",<TITLE>A Refined Evaluation of Human's Discussions about Natural Language Inference<TITLE>A Refined Evaluation of Human's Discourse about Natural Language Inference<TITLE>The Uncertainty of Natural Language Inference: A Refined Evaluation<TITLE>Understanding Human's Discussions about Natural Language Inference<TITLE>The Uncertainty of Natural Language Inference,<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE/natural language<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based inference<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based Natural<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based natural<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based In,<TITLE>Exploring Human Disagreement about the Validity of Natural Language Inference Tasks<TITLE>Human Disagreement about Natural Language Inference: A Case for Refined Evaluation Objective<TITLE>Exploring Human Disagreement about the Validity of Natural Language Inference<TITLE>Exploring Human Disagreement about Natural Language Inference<TITLE>Human Disagreement about Natural Language Inference,<TITLE>Understanding the Validity of Natural Language Inferences<TITLE>Uncertainty in Natural Language Inferences<TITLE>Uncertainty in Natural Language Inference<TITLE>Understanding the Validity of Natural Language Inference<TITLE>Analyzing Natural Language Inferences,<TITLE> Noise vs. Natural Language Inference: An Empirical Study<TITLE> Noise vs. Confident Natural Language Inference<TITLE> Noise vs. Natural Language Inference: An Empirical Analysis<TITLE> Noise vs. Natural Language Inference<TITLE> Are Natural Language Inference Systems Inconsistent?,<TITLE>Analyzing Human Disagreements about the validity of Natural Language Inference<TITLE>Analyzing Human's Disagreements about Natural Language Inference Models<TITLE>Analyzing Human's Disagreements about Natural Language Inference<TITLE>Analyzing Human Disagreements about Natural Language Inference Models<TITLE>Analyzing Human Disagreements about Natural Language Inference
7,Using Linguistic Principles to Recover Empty Categories,"This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al., 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency. Unlike previous approaches to this task, the current method is not corpus-based, but rather makes use of the principles of early Government-Binding theory (Chomsky, 1981), the syntactic theory that underlies the annotation. Using the evaluation metric proposed by Johnson (2002), this approach outperforms previously published approaches on both detection of empty categories and antecedent identification, given either annotated input stripped of empty categories or the output of a parser. Some problems with this evaluation metric are noted and an alternative is proposed along with the results. The paper considers the reasons a principlebased approach to this problem should outperform corpus-based approaches, and speculates on the possibility of a hybrid approach.",<TITLE>An Evaluation Metric for Empty Node Detection in Penn Treebank<TITLE>An Evaluation Metric for Empty Nodes in Penn Treebank<TITLE>An Evaluation Metric for the Penn Treebank<TITLE>An Evaluation Metric for Empty Nodes Detection<TITLE>An Evaluation Metric for Empty Node Detection,<TITLE>A Corpus-Based Approach to the Automatic Discovery of Empty Categories and Their Antecedents in the Penn Treebank: An<TITLE>A Corpus-Based Approach to the Automatic Discovery of Empty Categories and Their Antecedents in the Penn Treebank for Lex<TITLE>A Corpus-Based Approach to the Automatic Discovery of Empty Categories and Their Antecedents in the Penn Treebank: A<TITLE>A Corpus-Based Approach to the Automatic Discovery of Empty Categories and Their Antecedents in the Penn Treebank: The<TITLE>A Corpus-Based Approach to the Automatic Discovery of Empty Categories and Their Antecedents in the Penn Treebank: Theory,<TITLE>A Principle-Based Approach to Empty Categories in the Penn Treebank<TITLE>A Principle-based Approach to Empty Categories in the Penn Treebank<TITLE>A Principle-Based Approach to Empty Categories in the Penn Treebank<TITLE>A Principle-Based Approach to Empty Categories and Function Tagging<TITLE>A Principle-Based Approach to Empty Categories in Treebanks,<TITLE>Detecting Lethal Nodes in the Penn Treebank<TITLE>Detecting Neural Nodes in the Penn Treebank<TITLE>Detecting empty nodes in the Penn Treebank<TITLE>Detecting Lethal Nodes in Penn Treebank<TITLE>Detecting Nodes in the Penn Treebank,<TITLE> Detecting Empty Categories and Antecedents in the Penn Treebank<TITLE> Identifying Empty Categories and Antecedents in the Penn Treebank<TITLE> Automatic Detection of Empty Categories<TITLE> Automatic Detection of Empty Categories and Antecedents<TITLE> Automatic Identification of Empty Categories,<TITLE>A Principle-Based Approach to Empty Node Detection in the Penn Treebank<TITLE>A Principle-based Approach to Empty Node Detection in the Penn Treebank<TITLE>A Principle-Based Approach to Empty Node Detection in the Penn TreeBank<TITLE>A Principle-Based Approach to Empty Node Detection in the Penn Tree Bank<TITLE>A Principle-based Approach to Empty Node Detection in the Penn TreeBank
8,Domain Adaptation with Active Learning for Word Sense Disambiguation,"When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.",<TITLE>Active Learning for Domain Adaptation of Word Sense Disambiguation Systems<TITLE>Active Learning for Domain Adaptation of Word Sense Disambiguation System<TITLE>Active Learning for Domain Adaptation in Word Sense Disambiguation<TITLE>Active Learning for Domain Adaptation in Word Sense Disambiguation Systems<TITLE>Active Learning of Domain Adaptation for Word Sense Disambiguation,<TITLE>Active Domain Adaptation for Word Sense Disambiguation Using Expectation-Maximization Prediction and Count-Merging Techniques<TITLE>Active Domain Adaptation for Word Sense Disambiguation using Expectation-Maximization Prediction and Count-Merging Techniques<TITLE>Active Domain Adaptation for Word Sense Disambiguation Using Expectation-Maximization Prediction and Count-Merging Technique<TITLE>Active Domain Adaptation for Word Sense Disambiguation Using Expectation-Maximization Prediction and Count-Merging Methods<TITLE>Active Domain Adaptation for Word Sense Disambiguation using Expectation-Maximization Prediction and Count-Merging Technique,<TITLE>Domain Adaptation of Word Sense Disambiguation Systems<TITLE>Domain Adaptation for Word Sense Disambiguation<TITLE>Domain Adaptation for Word Sense Disambiguation<TITLE>Domain Adaptation for Word Sense Disambiguation<TITLE>Domain Adaptation for Word Sense Disambiguation,<TITLE>Active Domain Adaptation for Word Sense Disambiguation<TITLE>Active Learning for Word Sense Disambiguation with Active Learning<TITLE>Domain Adaptation for Word Sense Disambiguation<TITLE>Active Learning for Word Sentiment Disambiguation<TITLE>Active Learning for Word Sense Disambiguation,<TITLE> Domain Adaptation for Word Sense Disambiguation<TITLE> Active Learning for Word Sense Disambiguation<TITLE> Active Learning Improves Word Sense Disambiguation<TITLE> Domain Adaptation of Word Sense Disambiguation<TITLE> Domain Adaptation of Word Sense Disambiguation Systems,<TITLE>Active Learning and Count-merging for Domain Adaptation of Word Sense Disambiguation<TITLE>Active Learning and Count-Mating for Domain Adaptation of Word Sense Disambiguation<TITLE>Active Learning for Domain Adaptation of Word Sense Disambiguation Systems<TITLE>Active Learning for Domain Adaptation of Word Sense Disambiguation<TITLE>Active Learning for Domain Adaptation of Word Sense Disambiguations
9,CancerEmo: A Dataset for Fine-Grained Emotion Detection,"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients. However, progress on this task has been hampered by the absence of large labeled datasets. To this end, we introduce CancerEmo, an emotion dataset created from an online health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best BERT model achieves an average F1 of 71%, which we improve further using domain-specific pre-training.",<TITLE>CancerEmo: A Dataset for Fine-Grained Emotions<TITLE>CancerEmo: A Dataset of Fine-Grained Emotions<TITLE>CancerEmo: An Emotion Dataset for Health and Safety<TITLE>CancerEmo: An Emotion Dataset for Health Domain<TITLE>CancerEmo: A Dataset for Emotion Detection,<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in Online Health Community<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in an Online Health<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in Online Health Communities<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in a Health Community<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in Health Community Communities,<TITLE>CancerEmo: A Large-Scale Dataset for Emotion Detection in the Health Domain<TITLE>CancerEmo: A Dataset for Fine-Grained Emotion Detection in Health Domain<TITLE>CancerEmo: A Large-Scale Dataset for Emotion Detection in the Health Domain<TITLE>CancerEmo: A Dataset for Fine-Grained Emotion Detection in the Health Domain<TITLE>CancerEmo: A Dataset for Fine-grained Emotion Detection in the Health Domain,<TITLE>CancerEmo: An Emotion Dataset for Emotion Detection<TITLE>CancerEmo: A Dataset of Emotions from an Online Health Community<TITLE>CancerEmo: A Dataset for Emotion Detection in Health<TITLE>CancerEmo: An Emotion Dataset created from an Online Health Community<TITLE>CancerEmo: An Emotion Dataset created from an online health community,<TITLE> CancerEmo: A Fine-Grained Emotion Dataset for Healthcare<TITLE> CancerEmo: A Fine-Grained Emotion Dataset for Health Care<TITLE> CancerEmo: A Fine-Grained Emotion Dataset<TITLE> CancerEmo: A Fine-Grained Emotion Dataset for Medical Care<TITLE> CancerEmo: A Corpus-based Emotion Dataset,<TITLE>CancerEmo: A Fine-Grained Dataset for Emotion Detection in Health<TITLE>CancerEmo: A Dataset for Fine-grained Emotion Detection in Health Domains<TITLE>CancerEmo: A Dataset for Fine-grained Emotion Detection in Health<TITLE>CancerEmo: A Dataset for Fine-Grained Emotions in Health<TITLE>CancerEmo: A Health Dataset for Fine-grained Emotion Detection
10,Generalized Character-Level Spelling Error Correction,"We present a generalized discriminative model for spelling error correction which targets character-level transformations. While operating at the character level, the model makes use of wordlevel and contextual information. In contrast to previous work, the proposed approach learns to correct a variety of error types without guidance of manuallyselected constraints or language-specific features. We apply the model to correct errors in Egyptian Arabic dialect text, achieving 65% reduction in word error rate over the input baseline, and improving over the earlier state-of-the-art system.",<TITLE>A Discriminative Model for Spelling Error Correction in Arabic Dialect Text<TITLE>A Discriminative Model for Spelling Error Correction in Arabic Dialects<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>A Discriminative Model for Improved Spelling Error Correction<TITLE>Learning to Correct Spelling Errors in Arabic Dialect Text,<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: An Application<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: Learning Character<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: The Case<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: Learning to<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectal Speech: Learning Character,<TITLE>A Generalized Discriminative Model for Spelling Error Correction with Character-Level Information<TITLE>A Generalized Discriminative Model for Spelling Error Correction with Character-Level Transformations<TITLE>A Generalized Discriminative Model for Spelling Error Correction with Character-Level Transformation<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>A Generalized Discriminative Model for Correcting Spelling Errors,<TITLE>A Generalized Discriminative Model for Structural Error Correction<TITLE>A Generalized Discriminative Model for Correcting Errors in Arabic<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>A Discriminative Model for Correcting Errors in Arabic<TITLE>A Discriminative Model for Correcting Errors,<TITLE> Generalized Discriminative Model for Spelling Error Correction<TITLE> A Generalized Discriminative Model for Spelling Error Correction<TITLE> Generalized Discriminative Models for Spelling Error Correction<TITLE> Generalized Discriminative Modeling for Spelling Error Correction<TITLE> Generalized Discriminative Spelling Error Correction,<TITLE>Generalized Discriminative Model for Character-Level Spelling Error Correction<TITLE>A Generalized Discriminative Model for Character-Level Spelling Correction<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>Generalized Discriminative Model for Spelling Error Correction<TITLE>Character-Level Spelling Error Correction
11,Affinity-Preserving Random Walk for Multi-Document Summarization,"Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods.",<TITLE>Affinity-Preserving Random Walk for Multi-Document Summarization<TITLE>An Affinity-Preserving Random Walk for Multi-Document Summarization<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization<TITLE>Affinity-Preserving Random Walk for Multi-Document Summarization.<TITLE>Averaging Random Walk for Multi-Document Summarization,<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization with Adaptive Random Walk Constraints and Divers<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization with Adaptive Random Walk Constraints and Diversity Con<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization with Adaptive Random Walk Constraints and Group Ranking<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization with Adaptive Random Walk Constraints and the R<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization with Adaptive Random Walk Constraints and Ranking Al,<TITLE>Aging Affinity-Preserving Random Walk for Multi-Document Summarization<TITLE>Aging Affinity-Preserving Random Walk for Multi-document Summarization<TITLE>Aging Random Walk for Multi-Document Summarization<TITLE>Aging Random Walk for Multi-Document Summarization<TITLE>Aging Random Walk for Multi-Document Summarization,<TITLE>Affinity-preserving Random Walk for Multi-Document Summarization<TITLE>Affinity-preserving Random Walk for Multi-Document Summarization.<TITLE>Affiliation-preserving Random Walk for Multi-document Summarization<TITLE>Multi-Document Summarization with Affinity-preserving Random Walk<TITLE>Affinity-preserving Random Walk for Multi-document Summarization,<TITLE> Affinity-Preserving Random Walk for Multi-Document Summarization<TITLE> Affinity-Preserving Random Walks for Multi-Document Summarization<TITLE> Affinity-preserving Random Walk for Multi-Document Summarization<TITLE> Adaptive Affinity-Preserving Random Walk for Multi-Document Summarization<TITLE> Attentive Random Walk for Multi-Document Summarization,<TITLE>Affinity-preserving Random Walk for Multi-document Summarization<TITLE>Affinity-preserving Random Walk for Multi-Document Summarization<TITLE>Affinity-Preserving Random Walk for Multi-Document Summarization<TITLE>Affinity-Preserving Random Walk for Multi-document Summarization<TITLE>Affinity-preserving Random Walk for Multi-document Summarization.
12,Deal or No Deal? End-to-End Learning of Negotiation Dialogues,"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.",<TITLE>Learning End-to-End Models for Negotiations in Human-Human Dialogue<TITLE>Learning End-to-End Models for Negotiations in Natural Language Dialogue<TITLE>Learning End-to-End Models for Human-Human Negotiations<TITLE>Learning End-to-End Models for Multi-Issue Negotiations<TITLE>Training End-to-End Negotiators for Natural Language Dialogue,"<TITLE>Training End-to-End Negotiations for Multi-Issue Bilinguage: A Large Dataset and Training<TITLE>Training End-to-End Negotiations for Multi-Issue Bilinguage: A Large Dataset, Training<TITLE>Training End-to-End Negotiations for Multi-Issue Bilinguage: A Large Dataset and Data<TITLE>Training End-to-End Negotiations for Multi-Issue Bilinguage: A Large Dataset and Goal<TITLE>Training End-to-End Negotiations for Multi-Issue Bilinguage: A Large Dataset of Human",<TITLE>End-to-End Negotiation for Multi-issue Bargaining via Natural Language Dialogue<TITLE>End-to-End Negotiation for Multi-issue Bargaining with Natural Language Dialogue<TITLE>End-to-End Negotiation for Multi-issue Bargaining via Natural Language Conversation<TITLE>End-to-End Negotiation for Multi-issue Bargaining with Natural Language Conversation<TITLE>End-to-End Negotiation of Multi-issue Bargaining in Dialogue,<TITLE>A Large Dataset of Human-Human Negotiations on a Multi-issue Negotiation Task<TITLE>A Large Dataset of Human-Human Negotiations via Natural Language Dialogue<TITLE>Multi-Effect Negotiation via Natural Language Dialogue<TITLE>A Large Dataset of Human-Human Negotiations<TITLE>Multi-issue Negotiation with Natural Language Dialogue,<TITLE> End-to-End Dialogue Learning for Multi-Issue Bargaining<TITLE> End-to-End Dialogue Strategies for Multi-Issue Bargaining<TITLE> End-to-End Dialogue Planning for Multi-Issue Bargaining<TITLE> End-to-End Dialogue Learning for Multi-Issue Basing<TITLE> End-to-End Dialogue Learning for Multi-Issue Bidding,<TITLE>Learning to Negotiation in Semi-Cooperative Dialogue Systems<TITLE>Learning to Negotiation for Semi-Cooperative Dialogue Systems<TITLE>Learning to Negotiation in Semi-Cooperative Dialogue<TITLE>Learning to Negotiation for Semi-Cooperative Dialogue<TITLE>Learning to Negotiation in Semi-Supervised Dialogue
13,A Geometric Method to Construct Minimal Peer Prediction Mechanisms,"Minimal peer prediction mechanisms truthfully elicit private information (e.g., opinions or experiences) from rational agents without the requirement that ground truth is eventually revealed. In this paper, we use a geometric perspective to prove that minimal peer prediction mechanisms are equivalent to power diagrams, a type of weighted Voronoi diagram. Using this characterization and results from computational geometry, we show that many of the mechanisms in the literature are unique up to affine transformations, and introduce a general method to construct new truthful mechanisms.",<TITLE>Minimal Peer Prediction Mechanisms are Equivalent to Power Diagrams<TITLE>A Geometric Perspective on Minimal Peer Prediction via Power Diagrams<TITLE>A Geometric Perspective on Minimal Peer Prediction Mechanisms.<TITLE>A Geometric Perspective on Minimal Peer Prediction Mechanisms<TITLE>A Geometric View of Minimal Peer Prediction Mechanisms,<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited from Rational Agents: Structure and Affine Transformations<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited from Rational Agents: An Affine-Based Approach<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited From Rational Agents: An Affine-Based Approach<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited from Rational Agents: An Affine-Based Perspective<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited From Rational Agents: An Affine-Based Perspective,<TITLE>Minimal Peer Prediction Mechanisms are Weighted Voronoi Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Weighted Power Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Power Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Power Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Power Diagrams,<TITLE>A Geometric Perspective for Minimal Peer Prediction Mechanisms<TITLE>A Geometric Perspective for Mindful Peer Prediction Mechanisms<TITLE>Geometric Perspectives for Minimal Peer Prediction Mechanisms<TITLE>A Geometric Perspective on Minimal Peer Prediction Mechanisms<TITLE>Geometry for Minimal Peer Prediction Mechanisms,<TITLE> Truthful Minimal Peer Prediction Mechanisms<TITLE> A Geometric Perspective on Minimal Peer Prediction Mechanisms<TITLE> A Geometric Perspective to Minimal Peer Prediction Mechanisms<TITLE> Efficient Minimal Peer Prediction Mechanisms<TITLE> Truthful Peer Prediction Mechanisms,<TITLE>Minimal Peer Prediction Mechanisms Are Like Power Diagram<TITLE>Truthful Minimal Peer Prediction via Geometric Power Diagram<TITLE>Minimal Peer Prediction Mechanisms as Power Diagram Models<TITLE>Minimal Peer Prediction Mechanisms as Power Diagram<TITLE>Truthful Minimal Peer Prediction Mechanisms
14,Linearly constrained Bayesian matrix factorization for blind source separation,We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efficient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of nonnegative matrix factorization and factor analysis. The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.,<TITLE>Bayesian Probabilistic Matrix Factorization with Linear Constraints<TITLE>Bayesian Probabilistic Matrix Factorization via Gaussian Inference<TITLE>Bayesian Nonnegative Matrix Factorization with Linear Constraints.<TITLE>Bayesian Nonnegative Matrix Factorization with Linear Constraints<TITLE>Bayesian Nonnegative Matrix Factorization via Gaussian Inference,<TITLE>Bayesian Matrix Factorization with Gaussian Priors and Gibbs Sampling Based on Gaussian Observation for Nonnegative Matrix Factor<TITLE>Bayesian Matrix Factorization with Gaussian Priors and Gibbs Sampling Based on Gaussian Constraints for Nonnegative Matrix<TITLE>Bayesian Matrix Factorization with Gaussian Priors and Gibbs Sampling Based on Gaussian Observation for Nonnegative Multivariate<TITLE>Bayesian Matrix Factorization with Gaussian Priors and Gibbs Sampling Based on Gaussian Constraints for Bayesian Form<TITLE>Bayesian Matrix Factorization with Gaussian Priors and Gibbs Sampling Based on Gaussian Observation for Nonnegative Multil,<TITLE>Bayesian Nonnegative Matrix Factorization with Linear Constraints<TITLE>Bayesian Nonnegative Matrix Factorization with Gaussian Priors<TITLE>Bayesian Nonnegative Matrix Factorization with Gaussian Priors<TITLE>Bayesian Matrix Factorization with Gaussian Priors<TITLE>Bayesian Matrix Factorization with Gaussian Priors,<TITLE>Bayesian Probabilistic Matrix Factorization Subject to Linear Constraints<TITLE>Bayesian Probabilistic Matrix Factorization with Linear Constraints<TITLE>Bayesian Probabilistic Matrix Factorization with Linear Constraints.<TITLE>Bayesian Probabilistic Matrix Factorization Using Gibbs sampling<TITLE>A Bayesian Approach to Probabilistic Matrix Factorization,<TITLE> Bayesian Probabilistic Matrix Factorization<TITLE> Bayesian Matrix Factorization<TITLE> Bayesian Matrix Factorization with Gaussian Priors<TITLE> Bayesian Probabilistic Matrix Factorization with Gaussian Priors<TITLE> Bayesian Matrix Factorization via Gaussian Priors,<TITLE>Probabilistic Matrix Factorization with Linear Constraints<TITLE>Bayesian Nonnegative Matrix Factorization with Linear Constraints.<TITLE>Bayesian Nonnegative Matrix Factorization with Linear Constraints<TITLE>A Bayesian Approach to Probabilistic Matrix Factorization<TITLE>Bayesian Matrix Factorization with Linear Constraints.
15,Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols,"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks. We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.",<TITLE>Temporal Knowledge Base Completion with Missing Entities and Time Intervals<TITLE>Timeline: Joint Temporal Knowledge Base Completion and Time Prediction<TITLE> TIMEPLEX: Joint Temporal Knowledge Base Completion<TITLE>Toward Joint Temporal Knowledge Base Completion and Time Prediction<TITLE>Timeline: Joint Temporal Knowledge Base Completion,<TITLE>Temporal Knowledge Base Completion as Joint Temporal Relational Fact Extraction with TIMEPLEX: A Case Study on<TITLE>Temporal Knowledge Base Completion as Joint Temporal Relational Fact Extraction with TIMEPLEX: A Case Study in<TITLE>Temporal Knowledge Base Completion as Joint Temporal Relational Fact Prediction with TIMEPLEX: A Case Study on Un<TITLE>Temporal Knowledge Base Completion as Joint Temporal Relational Fact Prediction with TIMEPLEX: A Case Study in a<TITLE>Temporal Knowledge Base Completion as Joint Temporal Relational Fact Prediction with TIMEPLEX: A Case Study on T,<TITLE>TIMEPLEX: Joint Entity and Time Prediction for Temporal Knowledge Base Completion<TITLE>TIMEPLEX: Joint Entity and Time Prediction for Temporal Knowledge Bases<TITLE>TIMEPLEX: Joint Entity and Time Prediction for Temporal Knowledge Bases.<TITLE>TIMEPLEX: Joint Entity and Time Prediction for Temporal Knowledge Bases<TITLE>TIMEPLEX: Joint Entity and Time Prediction for Temporal Knowledge Bases,<TITLE>TIMEPLEX: Joint Temporal Knowledge Base Completion<TITLE>Joint Temporal Knowledge Base Completion with Relational Facts<TITLE>TKBC: Joint Temporal Knowledge Base Completion<TITLE>A Joint Temporal Knowledge Base Completion Approach<TITLE>Joint Temporal Knowledge Base Completion,<TITLE> Joint Temporal Knowledge Base Completion and Time Prediction<TITLE> Temporal Knowledge Base Completion<TITLE> Joint Temporal Knowledge Base Completion<TITLE> Joint Temporal Knowledge Base Completion and Link Prediction<TITLE> Improving Temporal Knowledge Base Completion,<TITLE>Time: A Joint Temporal Knowledge Base Completion Method for Link and Time Prediction<TITLE>Time: A Joint Temporal Knowledge Base Completion Method for Link and Time Estimation<TITLE>Time: A Joint Temporal Knowledge Base for Link Prediction and Time Prediction<TITLE>Time: A Joint Temporal Knowledge Base Completion Method<TITLE>Joint Temporal Knowledge Base Completion
16,Faster and Simpler Algorithm for Optimal Strategies of Blotto Game,"In the Colonel Blotto game, which was initially introduced by Borel in 1921, two colonels simultaneously distribute their troops across different battlefields. The winner of each battlefield is determined independently by a winner-take-all rule. The ultimate payoff of each colonel is the number of battlefields he wins. This game is commonly used for analyzing a wide range of applications such as the U.S presidential election, innovative technology competitions, advertisements, etc. There have been persistent efforts for finding the optimal strategies for the Colonel Blotto game. After almost a century Ahmadinejad, Dehghani, Hajiaghayi, Lucier, Mahini, and Seddighin provided a poly-time algorithm for finding the",<TITLE>Finding Optimal Strategies for the Colonel Blotto Game<TITLE>The Colonel Blotto Game: Finding Optimal Strategies<TITLE>Finding Optimal Strategies for the Colonel Blotto Games<TITLE>Optimal Strategies for the Colonel Blotto Game<TITLE>The Colonel Blotto Game,<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Different Batt battlefield<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Multi-Field Games<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Multi-Field Differences<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Different Bounded Games<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Different Battches,<TITLE>A Polynomial-Time Algorithm for the Colonel Blotto Game.<TITLE>A Polynomial-Time Algorithm for the Colonel Blotto Game<TITLE>A Poly-Time Algorithm for the Colonel Blotto Game.<TITLE>A Poly-Time Algorithm for the Colonel Blotto Game<TITLE>Finding Optimal Strategies for the Colonel Blotto Game,<TITLE>The Colonel Blotto Game: A Poly-Time Optimization Tool<TITLE>The Colonel Blotto Game: A Poly-Time Approach<TITLE>The Collectible Blotto Game<TITLE>The Collective Blotto Game<TITLE>The Colonel Blotto Game,<TITLE> Colonel Blotto: A Poly-Time Algorithm for Finding the Optimal Strategies for the Colored General<TITLE> Colonel Blotto: A Poly-Time Algorithm for Finding the Optimal Strategies for the Colored One<TITLE> Colonel Blotto: A Poly-Time Algorithm for Finding Generalized Strategies<TITLE> The Colonel Blotto Game<TITLE> Colonel Blotto: A Game-Theoretic Approach,<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies for the Colonel Blotto Game<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies in the Colonel Blotto Game<TITLE>The Colonel Blotto Game: A Poly-Time Algorithm for Finding the Optimal Strategies<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies of the Colonel Blotto Game<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies for the Colonel Blotto Games
17,A Computational Treatment of Lexical Rules in HPSG as Covariation in Lexical Entries,"This paper proposes a new computational treatment of lexical rules as used in the HPSG framework. A compiler is described which translates a set of lexical rules and their interaction into a definite clause encoding, which is called by the base lexical entries in the lexicon. This way, the disjunctive possibilities arising from lexical rule application are encoded as systematic covariation in the specification of lexical entries. The compiler ensures the automatic transfer of properties not changed by a lexical rule. Program transformation techniques are used to advance the encoding. The final output of the compiler constitutes an efficient computational counterpart of the linguistic generalizations captured by lexical rules and allows on-the-fly application of lexical rules.",<TITLE>A Computational Treatment of Lexical Rules in HPSG<TITLE>A Computational Lexical Rule Compiler for HPSG<TITLE>A Computational Approach to Lexical Rules in HPSG<TITLE>A Computational Approach to Lexical Rule Compilation<TITLE>A Computational Approach to Lexical Rules,<TITLE>A Computational Treatment of Lexical Rules in HPSG-based Lexical Grammars for Linguistic Generalization<TITLE>A Computational Treatment of Lexical Rules in HPSG-based Lexical Grammars: An Application to Lingu<TITLE>A Computational Treatment of Lexical Rules in HPSG-based Lexical Grammars for Linguistic Induction<TITLE>A Computational Treatment of Lexical Rules in HPSG-based Lexical Grammars for Linguistic Inference<TITLE>A Computational Treatment of Lexical Rules in HPSG-based Lexical Grammars for Linguistic Generalizations,<TITLE>A Computational Treatment of Lexical Rules and their Interactions<TITLE>A Computational Treatment of Lexical Rules and their Interaction<TITLE>A Computational Approach to Lexical Rules and their Application<TITLE>A Computational Treatment of Lexical Rules and their Interactions<TITLE>A Computational Treatment of Lexical Rules,<TITLE>A Compiler for Lexical Rules and their Interaction<TITLE>A Compiler for Lexical Rules and Their Interaction<TITLE>Lexical Rules: A Compiler for HPSG<TITLE>A Compiler for Lexical Rule Application<TITLE>A Compiler for Lexical Rules,<TITLE> A Computational Approach to Lexical Rules<TITLE> A Computational Framework for Lexical Rules<TITLE> A Compiler for Lexical Rules<TITLE> A Computational Approach to Lexical Rule Evaluation<TITLE> A Computational Approach to Lexical Rule Analysis,<TITLE>Computational Covariation of Lexical Rules in HPSG<TITLE>Computational Covariation of Lexical Rules for HPSG<TITLE>Computational Co-variation of Lexical Rules<TITLE>Computational Covariation of Lexical Rules<TITLE>Lexical Rule Compilation for HPSG
18,#SupportTheCause: Identifying Motivations to Participate in Online Health Campaigns,"We consider the task of automatically identifying participants’ motivations in the public health campaign Movember and investigate the impact of the different motivations on the amount of campaign donations raised. Our classification scheme is based on the Social Identity Model of Collective Action (van Zomeren et al., 2008). We find that automatic classification based on Movember profiles is fairly accurate, while automatic classification based on tweets is challenging. Using our classifier, we find a strong relation between types of motivations and donations. Our study is a first step towards scaling-up collective action research methods.",<TITLE>Automatic Identification of Participants' Motives in the Public Health Campaign Movember<TITLE>Automatic Classification of Participants' Motives in the Public Health Campaign Movember<TITLE>Automatic Identification of Participants' Motives in the Movember Campaign<TITLE>Identifying Participants' Motives in the Public Health Campaign Movember<TITLE>Automatic Identification of Participants' Motives in Movember,<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Collective Action Research and Social Identity Model of Collective Action<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Collective Action Research and Social Identity Model of Movember<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Social Identity Model of Collective Action Profiles and Twe<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Social Identity Model of Collective Action Profiles and Automatic<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Social Identity Model of Collective Action Profiles and Multi,<TITLE>Automatic classification of Movember participants' motivations and donations<TITLE>Automatic Classification of Movember Participants' Motives<TITLE>Automatic classification of Movember motivations and donations<TITLE>Automatic classification of motivations in Movember<TITLE>Automatic classification of Movember participants' motivations,<TITLE>Identifying Motivations in the Public Health Campaign Movember<TITLE>Identifying Motivations in Public Health Campaign Movember<TITLE>Automatic Identification of Motivations in Public Health Campaign Movember<TITLE>Identifying Motivations for Public Health Campaign Movember<TITLE>Automatic Classification for Public Health Campaign Movember,<TITLE> Automatic Identification of Participants' Motivations in the Campaign<TITLE> Automatic Identification of Participants' Motivations in Campaigns<TITLE> Automatic Identification of Political Motivations in Campaigns<TITLE> Automatic Characterization of Political Campaign Contributions<TITLE> Automatic Identification of Political Campaign Contributions,<TITLE>Identifying Participants' Motivatedness in the Movember Campaign<TITLE>Identifying Participants' Motivatedness in the Movember campaign<TITLE>Identifying Participants'  motivations in the Movember campaign<TITLE>Identifying Participants'  motivations for the Movember campaign<TITLE>Automatic Classification of Movember Participants'  motivations
19,Learning Invariant Representations Of Planar Curves,"We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.",<TITLE>Learning invariant geometric functions of planar curves for Euclidean and Similarity transformations<TITLE>Learning invariant geometric functions of planar curves with convolutional neural networks.<TITLE>Learning invariant geometric functions of planar curves with convolutional neural networks<TITLE>Learning Geometric Functions of Planar Curves with Convolutional Neural Networks<TITLE>Learning invariant geometric functions of planar curves,<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning with Convolutional Neural<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning: A Case Study on<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning: A Convex<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning: A Case Study in<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning with Convolutional Networks,<TITLE>Metric Learning for the Euclidean and Similarity Group of Transformations<TITLE>Metric Metric Learning for the Euclidean and Similarity Group<TITLE>Metric Learning for the Euclidean and Similarity Transformations<TITLE>Metric Learning for the Euclidean and Similarity Transformations<TITLE>Metric Learning of Similarity Metric Functions,<TITLE>Invariant Geometry of Planar Curves for Euclidean and Similarity<TITLE>Multi-Scale Simultaneous Learning for Planar Curves<TITLE>Multi-Scale Simultaneous Learning of Planar Curves<TITLE>Invariant Geometric Functions of Planar Curves<TITLE>Invariant Geometry of Planar Curves,<TITLE> A Metric Learning Framework for Planar Curves<TITLE> Semi-Supervised Learning of Invariant Geometric Functions<TITLE> Semi-Supervised Learning of Equivariant Geometric Functions<TITLE> Semi-supervised Learning of Equivariant Geometric Functions<TITLE> Semi-Supervised Learning of Equivariant Geometry,<TITLE>Similarity Metric Learning with Convolutional Neural Networks.<TITLE>Similarity Metric Learning via Convolutional Neural Networks.<TITLE>Similarity Metric Learning with Convolutional Neural Networks<TITLE>Similarity Metric Learning via Convolutional Neural Networks<TITLE>Metric Learning of Invariant Geometric Functions
20,Semantic parsing of speech using grammars learned with weak supervision,"Semantic grammars can be applied both as a language model for a speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations. Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases. Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision. We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors. Further, we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies, yielding an improvement in parsing performance over applying unweighted grammars.",<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognizer Grammars<TITLE>Induction of Semantic Speech Recognition Grammars under Weak Supervision<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision<TITLE>Weakly Supervised Induction of Semantic Speech Recognizers,<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing of Speech<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing in N<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing with N<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing of Sp<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing in a,<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Semantic Speech Recognition and Parsing,<TITLE>Learning Semantic Speech Recognition Grammars under weak supervision<TITLE>Induction of Semantic Speech Recognition Grammars<TITLE>Semantic Speech Recognition Grammars under weak supervision<TITLE>Learning Semantic Speech Recognition Grammars<TITLE>Semantic Speech Recognition Grammars,<TITLE> Improving Semantic Speech Recognition Grammars with Weak Supervision<TITLE> Improving Semantic Speech Recognition Grammar Induction<TITLE> Improving Semantic Speech Recognition Grammars via Weak Supervision<TITLE> Improving Semantic Speech Recognition Grammars using Weak Supervision<TITLE> Inducing Semantic Speech Recognition Grammars,<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammar<TITLE>Weakly-Supervised Induction of Semantic Speech Recognition Grammar<TITLE>Induction of Semantic Speech Recognition Grammars under Weak Supervision<TITLE>Weakly Supervised Semantic Speech Recognition Grammar Induction<TITLE>Induction of Semantic Speech Recognition Grammars
21,Bilingual Lexicon Generation Using Non-Aligned Signatures,"Bilingual lexicons are fundamental resources. Modern automated lexicon generation methods usually require parallel corpora, which are not available for most language pairs. Lexicons can be generated using non-parallel corpora or a pivot language, but such lexicons are noisy. We present an algorithm for generating a high quality lexicon from a noisy one, which only requires an independent corpus for each language. Our algorithm introduces non-aligned signatures (NAS), a cross-lingual word context similarity score that avoids the over-constrained and inefficient nature of alignment-based methods. We use NAS to eliminate incorrect translations from the generated lexicon. We evaluate our method by improving the quality of noisy Spanish-Hebrew lexicons generated from two pivot English lexicons. Our algorithm substantially outperforms other lexicon generation methods.",<TITLE>Non-Aligned Signatures for Automated Lexicon Generation<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation<TITLE>Neural Lexicon Generation with Non-Aligned Signatures<TITLE>Non-Aligned Signatures for Automatic Lexicon Generation<TITLE>Named Entity Generation for Bilingual Lexicon Generation,<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation from Noisy Corpora using Non-Parallel Corpora and<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation from Noisy Corpora using Non-Parallel Corpora.<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation from Noisy Corpora using Non-Parallel Corpora or<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation from Noisy Corpora using Non-Parallel Corpora (<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation from Noisy Corpora using Non-Parallel Corpora in,<TITLE>Generating Noisy Bilingual Lexicons with Non-aligned Signatures<TITLE>Non-aligned Signatures for Noisy Bilingual Lexicon Generation<TITLE>Generating Noisy Bilingual Lexicons with Non-aligned Signature<TITLE>Non-aligned Signatures for Noisy Bilingual Lexicons<TITLE>Non-aligned Signatures for Noisy Lexicon Generation,<TITLE>Non-Aligned Signatures for Bilingual Lexicons Generation<TITLE>Non-Aligned Signatures for Bilingual Lexicon Generation<TITLE>Non-Aligned Signatures for Bilingual Lexicons<TITLE>Non-Aligned Signs for Bilingual Lexicons Generation<TITLE>Non-Aligned Signs for Bilingual Lexicons,<TITLE> Pivot Language Lexicon Generation with Non-Parallel Corpora<TITLE> Pivot Language Lexicon Generation Using Non-Parallel Corpora<TITLE> Automatic Lexicon Generation from Noisy Pivot Language<TITLE> Automatic Lexicon Generation from a Noisy Pivot Language<TITLE> Automatic Lexicon Generation from Noisy Pivot Languages,<TITLE>Non-aligned Signatures for Noisy Bilingual Lexicon Generation<TITLE>Noisy Bilingual Lexicon Generation Using Non-aligned Signatures<TITLE>Noisy Bilingual Lexicon Generation with Non-aligned Signatures<TITLE>Noisy Bilingual Lexicon Generation Using Non-Aligned Signatures<TITLE>Noisy Bilingual Lexicon Generation with Non-Aligned Signatures
22,Reading between the Lines: Learning to Map High-Level Instructions to Commands,"In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1",<TITLE>Mapping High-Level Instructions to Sequences in an Environment<TITLE>Learning to Map High-Level Instructions to Sequences of Commands<TITLE>Learning to Map High-Level Instructions to Sequences of Instructions<TITLE>Mapping High-Level Instructions to Sequences of Commands<TITLE>Mapping High-Level Instructions to Sequences of Instructions,<TITLE>Mapping Instructions to Sequences of Decoders in an External Environment using a Policy-Gradient Reinforcement Learning Algorithm<TITLE>Mapping Instructions to Sequences of Mappings in an External Environment using a Policy-Gradient Reinforcement Learning Algorithm for<TITLE>Mapping Instructions to Sequences of Mappings in an External Environment using Policy-Gradient Reinforcement Learning Algorithm for Text<TITLE>Mapping Instructions to Sequences of Decoders in an External Environment using Policy-Gradient Reinforcement Learning Algorithm for<TITLE>Mapping Instructions to Sequences of Mappings in an External Environment using a Policy-Gradient Reinforcement Learning Algorithm.,<TITLE>Mapping High-Level Instructions to Sequences of Commands in an Environment<TITLE>Mapping High-Level Instructions to Sequences of Command Sequences<TITLE>Mapping Instructions to Sequences of Commands in an External Environment<TITLE>Mapping Instructions to Sequences of Commands in an External Environment<TITLE>Mapping Instructions to Sequences of Commands in an Environment,<TITLE>Modeling High-Level Instructions to Sequences of Commands<TITLE>Learning High-Level Instructions to Sequences in External Environments<TITLE>Learning High-Level Instructions to Sequences of Commands<TITLE>Modeling High-Level Instructions for Text Interpretation<TITLE>Modeling High-Level Instructions to Sequences,<TITLE> Mapping High-Level Instructions to Sequences of Descriptions<TITLE> Mapping High-Level Instructions to Sequences of Instructions<TITLE> Mapping High-Level Instructions to Sequences of Actions<TITLE> Mapping High-Level Instructions to Sequences<TITLE> Mapping High-Level Instructions to Sequences of Desires,<TITLE>Mapping Instructions to Sequences of Actions in an External Environment<TITLE>Mapping High-Level Instructions to Sequences of Control Tasks<TITLE>Mapping High-Level Instructions to Sequences of Actions<TITLE>Mapping High-Level Instructions to Sequences of Words<TITLE>Mapping Instructions to Sequences in an External Environment
23,How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?,"This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer.",<TITLE>Hidden layers of deep rectifier networks are linearly separable<TITLE>The Universal Classification Power of Deep Rectifier Neural Networks<TITLE>The Universal Classification Power of Deep Rectifier Networks<TITLE>Distance preserving Deep rectifier networks<TITLE>Distance preserving Deep Rectifier Networks,<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via linear<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace using linear<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via is<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via non<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via error,<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets into linearly separable units<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets into linearly separable ones<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets to linearly separable ones<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets to linearly separable patterns<TITLE>Hidden layers of deep rectifier neural networks are capable of linearly separating pattern sets.,<TITLE>Distance Preserving Deep Korrelier Neural Networks<TITLE>Distance preserving Deep Korrelier Neural Networks<TITLE>Distance preserving deep rectifier neural networks with hidden layers<TITLE>Distance preserving deep rectifier neural networks<TITLE>Distance preserving deep rectifier networks,<TITLE> Distance preserving deep rectifier networks<TITLE> On the Generalization Power of Distance Preserving Deep Rectifier Networks<TITLE> On the Generalization Power of Deep Rectifier Networks<TITLE> On the Generalization Power of Deep Separable Pattern Sets<TITLE> On the Generalization Power of Distance Preserving Deep RNNs,<TITLE>Universal Distance Preserving of Deep Neural Networks<TITLE>Universal Distance Preserving in Deep Neural Networks<TITLE>Universal Classification Power of Distance Preserving Networks<TITLE>Distance Preserving in Deep Neural Networks<TITLE>Distance Preserving of Deep Neural Networks
24,Abstracts of Current Literature,"S OF CURRENT LITERATURE For copies of the following papers on Project SEMSYN, write to Frau HOrmann c / o Project SEMSYN Institut for Informatik Universitat Stuttgart Azenbergstrasse 12 D-7000 Stuttgart 1, West Germany When Mariko Talks to Siegfried Experiences from a Japanese /German Machine",<TITLE>Project SEMSYN: A Study of Current Literature in German<TITLE>Project SEMSYN: A Case Study on Current Literature<TITLE>Project SEMSYN: A Case Study of Current Literature<TITLE>Project SEMSYN: A Study of Current Literature<TITLE>Project SEMSYN: The Future of Current Literature,<TITLE>The Current LITERATURE: A Frau Heterosyntactic Corpus for Project Semantics and its Application to the<TITLE>The Current LITERATURE: A Frau Heterosyntactic Corpus for Project Semantics and its Application to German<TITLE>The Current LITERATURE: A Frau Heterosyntactic Corpus for Project Semantics and its Application to Document<TITLE>The Current LITERATURE: A Frau Heterosyntactic Project for Informat and Project SEMMorphology<TITLE>The Current LITERATURE: A Frau Heterosyntactic Corpus for Project Semantics and its Application to Text,<TITLE>Project SEMYN: A Study of Current Literature<TITLE>Project SEMSYN: A Study of Current Literature<TITLE>Project SEMYN: A Study in Current Literature<TITLE>Project SEMYN: A Study of Current Literature<TITLE>Project SEMYN: A Study of Current Literature,<TITLE>Frau HOrmann c /o Project SEMSYN<TITLE>Project SEMSYN Institut for Informatik University Stuttgart<TITLE>Project SEMSYN Institut for Informatik<TITLE>Project SEMSYN<TITLE>SEMSYN,<TITLE> Project Semsyn: An Introduction to the Project<TITLE> Project Semsyn: An Introduction to the German Project<TITLE> Project Semsyn: An Information Extraction Conference<TITLE> Project Semantics: An Introduction to the Project<TITLE> Project Semantics: An Introduction to the Special Issue,"<TITLE>Computational Linguistics, Volume 17, Number 2, June 1991<TITLE>Computational Linguistics, Volume 17, Number 2, June 2001<TITLE>Computational Linguistics, Volume 17, Number 2, June 1996<TITLE>Computational Linguistics, Volume 17, Number 2, June 1990<TITLE>Computational Linguistics, Volume 17, Number 2, June 1997"
25,Structured Learning for Cell Tracking,We study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology. Reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research. Existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search. We propose a structured learning approach that allows to learn optimum parameters automatically from a training set. This allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences.,<TITLE>A Structured Learning Approach for Reliable Cell Tracking in Microscapes<TITLE>A Structured Learning Approach to Reliable Cell Tracking in Microscapes<TITLE>Structured Learning for Reliable Cell Tracking in Microscopic Images<TITLE>Structured Learning for Reliable Cell Tracking in Microscapes<TITLE>A Structured Learning Approach to Cell Tracking,<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes: A Structured Learning Approach<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes: A Structured Approach.<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes using Structured Random Fields.<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes: a Structured Approach.<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes using Structured Random Fields and,<TITLE>Structured Learning for Cell Tracking in Time-Lapse Microscopic Images<TITLE>Structured Learning for Cell Tracking in Time-lapse Microscopic Images<TITLE>Structured Learning for Cell Tracking in Time-Lapse Microscopic Images<TITLE>Structured Learning for Cell Tracking in Time-lapse Microscopic Images<TITLE>Structured Learning for Large-Scale Cell Tracking,<TITLE>Learning to Track a Large Quantity of Homogeneous Objects<TITLE>Learning to Track a Large Quantum of Homogeneous Objects<TITLE>Learning to Track Homogeneous Objects in Cell Culture and Development Biology<TITLE>Learning to Track Homogeneous Objects in Cell Culture and Evolution Biology<TITLE>Learning to Track Homogeneous Objects via Structured Learning,<TITLE> Structured Learning for Cell Tracking in Time-Lapse Microarray Sequences<TITLE> Learning to Track Homogeneous Objects in Time-Lapse Microarray Sequences<TITLE> Learning to Track Homogeneous Objects with Structured Learning<TITLE> Learning to Track Homogeneous Objects in Microarray Sequences<TITLE> Structured Learning for Modeling Homogeneous Object Tracking,<TITLE>Learning to Track Cells in Time-lapse Microarrays<TITLE>Learning to Track Cells in Microscopic Time-lapse Images<TITLE>Learning to Track Cells in Microarray Time-lapses<TITLE>Learning to Track Cells in Microscopic Time-lapses<TITLE>Learning to Track Cells in Time-lapse Microarray Images
26,Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural Machine Translation,"We show that the state-of-the-art Transformer MT model is not biased towards monotonic reordering (unlike previous recurrent neural network models), but that nevertheless, long-distance dependencies remain a challenge for the model. Since most dependencies are short-distance, common evaluation metrics will be little influenced by how well systems perform on them. We therefore propose an automatic approach for extracting challenge sets rich with long-distance dependencies, and argue that evaluation using this methodology provides a complementary perspective on system performance. To support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.",<TITLE>Towards Automatic Evaluation of Machine Translation Systems<TITLE>Compiling Challenge Sets for Machine Translation Evaluation<TITLE>Compiling Challenge Sets for Neural Machine Translation<TITLE>Automatic Evaluation of Machine Translation Challenge Sets<TITLE>Compiling Challenge Sets for Machine Translation,<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-Art Trans<TITLE>Evaluating Machine Translation with Long-distance Dependencies: A Scalable Approach to the State-of-the-Art<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-art Trans<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-Art Transformers<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-Art MT,<TITLE>Automatic Evaluation of Transformer MT on the Long-Tail of Syntactic Phenomena<TITLE>Evaluating Transformer MT on the Long Tail of Syntactic Phenomena Using Challenge Sets<TITLE>Evaluating Transformer MT on the Long Tail of Syntactic Phenomena with Challenge Sets<TITLE>Automatic Evaluation of Transformer MT on the Long-tail of Syntactic Phenomena<TITLE>Automatic Evaluation of Transformer MT on Long-Tail Syntactic Phenomena,<TITLE>Extraction of Challenge Sets with Long-Distance Dependencies<TITLE>Extraction of Long-Distance Dependencies for Transformer MT<TITLE>Evaluating Long-Distance Dependencies for Transformer MT<TITLE>Automatic Extraction of Challenge Sets for Transformer MT<TITLE>Extraction of Challenge Sets for Transformer MT,<TITLE> Extracting Challenge Sets Rich with Long-Distance Dependencies<TITLE> Extracting Challenges Rich with Long-Distance Dependencies<TITLE> Extracting Challenges for Neural Machine Translation<TITLE> Extracting Challenge Sets Rich with Short-Distance Dependencies<TITLE> Extracting Challenge Sets from Recurrent Neural Networks,<TITLE>Automatic Evaluation of Long-Tailed Syntactic Challenges for Transformer MT<TITLE>Automatic Evaluation of Transformer MT with Long-distance Dependencies<TITLE>Automatic Evaluation of Long-distance Dependencies in Transformer MT<TITLE>Automatic Evaluation of Long-distance Dependencies for Transformer MT<TITLE>Automatic Evaluation of Transformer MT
27,Humor as Circuits in Semantic Networks,"This work presents a first step to a general implementation of the Semantic-Script Theory of Humor (SSTH). Of the scarce amount of research in computational humor, no research had focused on humor generation beyond simple puns and punning riddles. We propose an algorithm for mining simple humorous scripts from a semantic network (ConceptNet) by specifically searching for dual scripts that jointly maximize overlap and incongruity metrics in line with Raskin’s Semantic-Script Theory of Humor. Initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles. We evaluate the said metrics through a user-assessed quality of the generated two-liners.",<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor<TITLE>Mining Simple Humor with Semantic-Script Theory of Humor<TITLE>Mining Simple Humor from Semantic-Script Theory of Humor<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor.<TITLE>Mining Simple Humor with Semantic-Script Theory of Humor.,<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor with ConceptNet and Raskin’s Sem<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor via ConceptNet and Raskin’s Sem<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor via ConceptNet and Raskin's Semantic Network<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor from ConceptNet: A Relaxed Two-Line Approach<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor with ConceptNet and Raskin's Semantic Network,<TITLE>Mining Semantic Scripts for Humor Generation<TITLE>Mining Semantic Scripts for Humor Generation<TITLE>Mining Semantic Scripts for Comedy Generation<TITLE>Mining Semantic Scripts for Humor Generation<TITLE>Mining Semantic Scripts for Humor Generation,<TITLE>Semantic-Script Theory of Humor: Exploiting Simple Humor from ConceptNet<TITLE>A Semantic-Script Theory of Humor Approach to Humor Generating<TITLE>A Semantic-Script Theory of Humor Approach to Humor Generation<TITLE>A Semantic-Script Theory of Humor Approach<TITLE>Semantic-Script Theory of Humor,<TITLE> Computational Linguistic Humor Mining<TITLE> Computational Linguistic Humor Mining from ConceptNet<TITLE> Semantic-Script Mining for Humor Generation<TITLE> Computational Linguistic Humor Mining Using ConceptNet<TITLE> Semantic-Script Mining for Generating Humor,<TITLE>Generating Two-liners from a Semantic Network<TITLE>Generating Two-liners from Semantic Networks<TITLE>Generating Two-liners from Semantic Content<TITLE>Mining Humor from a Semantic Network<TITLE>Generating Humor from Semantic Content
28,Reduce & Attribute: Two-Step Authorship Attribution for Large-Scale Problems,"Authorship attribution is an active research area which has been prevalent for many decades. Nevertheless, the majority of approaches consider problem sizes of a few candidate authors only, making them difficult to apply to recent scenarios incorporating thousands of authors emerging due to the manifold means to digitally share text. In this study, we focus on such large-scale problems and propose to effectively reduce the number of candidate authors before applying common attribution techniques. By utilizing document embeddings, we show on a novel, comprehensive dataset collection that the set of candidate authors can be reduced with high accuracy. Moreover, we show that common authorship attribution methods substantially benefit from a preliminary reduction if thousands of authors are involved.",<TITLE>Reducing the Number of Candidate Authors for Authorship Attribution<TITLE>Reducing the Number of Candidate Authors in Authorship Attribution<TITLE>Reducing the Number of Candidate Authors for Article Attribution<TITLE>Reducing the Number of Candidate Authorship Attribution Methods<TITLE>Large-Scale Attribution via Document Embeddings,<TITLE>Authorship Attribution via Document Embeddings and Manifold Representations: A Dataset and Application to Large-Scale<TITLE>Authorship Attribution via Document Embeddings and Manifold Representations: A Large-Scale Study on the Role of Document<TITLE>Authorship Attribution via Document Embeddings and Manifold Representations: A Dataset and Application to Large-scale<TITLE>Authorship Attribution via Document Embeddings and Manifold Representations: A Large-Scale Study on the Role of Auth<TITLE>Authorship Attribution via Document Embeddings and Manifold Representations: A Large-Scale Study on the Role of Common,<TITLE>Reducing the Set of Candidate Authors for Large-Scale Authorship Attribution<TITLE>Reducing the Set of Author Candidates for Authorship Attribution<TITLE>Reducing the Set of Author Candidates for Authorship Attribution<TITLE>Reducing the Set of Candidate Authors for Authorship Attribution<TITLE>Reducing the Set of Candidate Authors for Authorship Attribution,<TITLE>Using Document Embeddings to Reduce Authorship Attribution<TITLE>Using Document Embeddings to Reduce Authorship Adjudication<TITLE>Large-Scale Problems with Common Authorship Attribution<TITLE>Using Document Embeddings to Reduce Authorship Addition<TITLE>Large-Scale Problems for Authorship Attribution,<TITLE> Large-Scale Authorship Attribution Using Document Embeddings<TITLE> Large-Scale Authorship Attribution via Document Embeddings<TITLE> Reducing Large-Scale Authorship Attribution Problems Using Document Embeddings<TITLE> Reducing Large-Scale Authorship Attribution Problems with Document Embeddings<TITLE> Large-Scale Authorship Attribution Using Document Embeddings: A Case Study,<TITLE>Large-Scale Authorship Attribution with Document Embeddings<TITLE>Large-Scale Authorship Attribution via Document Embeddings<TITLE>Reducing the Number of Candidate Authors for Authorship Attribution<TITLE>Large-Scale Authorship Attribution via Document Embedding<TITLE>Reducing the Number of Authorship Attribution Candidates
29,A Discriminative Training Procedure for Continuous Translation Models,"Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inference is to use them in an N -best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines.",<TITLE>Neural Rescoring for Continuous-Space Translation<TITLE>Rescoring Neural Networks for Continuous-Space Translation<TITLE>Rescoring Neural Network for Continuous-Space Translation<TITLE>Neural Rescoring for Continuous-space Translation<TITLE>Rescoring Neural Machine Translation Models,<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Resoring Algorithm and an Objective Function<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Resoring Algorithm and an N-<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Rescoring Algorithm and an Objective<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Rescoring Algorithm and a Joint<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Rescoring Algorithm and an N,<TITLE>Rescoring with Joint Neural Networks for Continuous-Space Translation Models<TITLE>Rescoring in Continuous-Space Translation Models with Joint Inference<TITLE>Rescoring in Continuous-Space Translation Models with Joint Neural Networks<TITLE>Rescoring with Joint Neural Networks for Continuous-Space Translation<TITLE>Rescoring with Neural Networks for Continuous-Space Translation Models,<TITLE>Continuous-Space Translation Models in Inference<TITLE>Continuous-Space Translation Models with Objective Function<TITLE>Continuous-Space Translation Models for Inference<TITLE>Continuous-Sspace Translation Models<TITLE>Continuous-Space Translation Models,<TITLE> Jointly Training Neural Networks for Continuous-Space Translation<TITLE> Training Neural Networks for Continuous-Space Translation<TITLE> Joint Training of Neural Networks for Continuous-Space Translation<TITLE> Joint Training of Neural Network Models for Continuous-Space Translation<TITLE> Joint Training of Continuous-Space Translation Models,<TITLE>Jointly Training Neural Networks for Rescoring in Continuous-Space Translation<TITLE>Jointly Training Neural Networks for Improved Rescoring of Machine Translation Models<TITLE>Joint Training of Continuous-Space Translation Models for Improved Rescoring<TITLE>Joint Training of Continuous-Space Neural Networks for Improved Rescoring<TITLE>Jointly Training Neural Networks for Improved Rescoring
30,Interpreting and Boosting Dropout from a Game-Theoretic View,"This paper aims to understand and improve the utility of the dropout operation from the perspective of game-theoretic interactions. We prove that dropout can suppress the strength of interactions between input variables of deep neural networks (DNNs). The theoretic proof is also verified by various experiments. Furthermore, we find that such interactions were strongly related to the over-fitting problem in deep learning. Thus, the utility of dropout can be regarded as decreasing interactions to alleviate the significance of over-fitting. Based on this understanding, we propose an interaction loss to further improve the utility of dropout. Experimental results have shown that the interaction loss can effectively improve the utility of dropout and boost the performance of DNNs.",<TITLE>The Utility of Dropout in Deep Neural Networks via Interaction Loss<TITLE>Improving the Utility of Dropout in Deep Neural Networks<TITLE>Improved the Utility of Dropout in Deep Neural Networks<TITLE>The Utility of Dropout in Deep Neural Networks<TITLE>The Utility of Dropout for Deep Neural Networks,<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for Over-fitting DNN<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for DNNs using Drop<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for DNNs Using Drop<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for Over-fitting Games in<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for DNNs using an,<TITLE>The Utility of Dropout in Deep Neural Networks via Game-Theoretic Interactions<TITLE>The Utility of Dropout in Deep Neural Networks via Game-theoretic Interactions<TITLE>The Utility of Dropout in Deep Learning from the Perspective of Game Theory<TITLE>The Utility of Dropout in Deep Neural Networks via Interaction Loss<TITLE>The Utility of Dropout and Interaction Loss in Deep Neural Networks,<TITLE>Learning the Utility of Dropout from Game-Theoretic Interactions<TITLE>Game-Theoretic Interaction Loss for Deep Neural Networks<TITLE>Learning the Utility of Dropout from Game-Theoretic interactions<TITLE>Game-Theoretic Interaction Loss in Deep Learning<TITLE>Game-Theoretic Interaction Loss for Deep Learning,<TITLE> On the Utility of Dropout in Deep Learning<TITLE> On the Utility of Dropout for Deep Neural Networks<TITLE> On the Utility of Dropout in Deep Neural Networks<TITLE> Improving the Utility of Dropout in Deep Learning<TITLE> Improving the Utility of Dropout for Deep Learning,<TITLE>Understanding and Improving the Utility of Dropout in Deep Neural Networks<TITLE>Understanding and Improving the Utility of Deep Neural Networks via Interaction Loss<TITLE>Understanding and Improving the Utility of Deep Neural Network via Interaction Loss<TITLE>Understanding and Improving the Utility of Dropout in Deep Learning<TITLE>Understanding and Improving the Utility of Deep Neural Network dropout
31,LexSemTm: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning,"There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and optimises it for application to the entire vocabulary of a given language. The optimised method is then used to produce LEXSEMTM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polysemous, English simplex lemmas, which is released as a public resource to the community. Finally, the quality of this data is investigated, and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR, and at least on par with SEMCOR-based distributions otherwise.",<TITLE>LEXSEMTM: An Unsupervised Learning of Sense Distributions<TITLE>LEXSEMTM: An Unsupervised Method for Sense Distribution Learning<TITLE>LEXSEMTM: An Unsupervised Method for Learning Sense Distribution<TITLE>LEXSEMTM: A Large Scale Corpus for Sense Distribution Learning<TITLE>Unsupervised Sense Distribution Learning with LEXSEMTM,<TITLE>Optimising Sense Distributions Using LEXSEMTM: An Empirical Study of the State-of-the<TITLE>Optimising Sense Distributions Using LEXSEMTM: An Empirical Study on the State-of-the<TITLE>Optimising Sense Distributions with LEXSEMTM: An Empirical Study of the State-of-the<TITLE>Optimising Sense Distributions with LEXSEMTM: An Empirical Study on the State-of-the<TITLE>Optimising Sense Distributions Using LEXSEMTM: An Empirical Analysis of the State-of-the,<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning of Sense Distributions for English<TITLE>LEXSEMTM: A Large-Scale Semantic Dataset for Sense Distribution Learning<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning of Sense Distributions for English<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning Method for Sense Distributions<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning Method for Sense Distributions,<TITLE>LEXSEMTM: An Unsupervised Method for Learning Sense Distributions<TITLE>LEXSEMTM: An Unsupervised Method for Learning Sentiment Distribution<TITLE>LEXSEMTM: Unsupervised Learning of Sentiment Distributions<TITLE>LEXSEMTM: Unsupervised Learning of Sense Distributions<TITLE>Unsupervised Learning of Sense Distributions,<TITLE> Unsupervised Learning of Sense Distributions<TITLE> Unsupervised Sense Distribution Learning<TITLE> Unsupervised Learning of Word Sense Distributions<TITLE> Unsupervised Sense Distribution Learning Using Semantic Datasets<TITLE> Unsupervised Learning of Sense Distributions Using WordNet,<TITLE>LEXSEMTM: A Sense Distribution Model for the Entire Vocabulary of a Language<TITLE>LEXSEMTM: A Sense Distribution Dataset for the Entire Vocabulary of a Language<TITLE>LEXSEMTM: A Sense Distribution Dataset for Unsupervised Learning<TITLE>LEXSEMTM: A Sense Distribution Dataset of Unsupervised Learning<TITLE>LEXSEMTM: A Sense Distribution Dataset for English
32,Nonlinear Learning using Local Coordinate Coding,"This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difficult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods.",<TITLE>Local Coordinate Coding for Semi-Supervised Nonlinear Manifolds<TITLE>Local Coordinate Coding on High Dimensional Nonlinear Manifolds.<TITLE>Local Coordinate Coding on High Dimensional Nonlinear Manifolds<TITLE>Local Coordinate Coding for High Dimensional Nonlinear Manifolds<TITLE>Global Linear Learning on High Dimensional Nonlinear Manifolds,<TITLE>Local Coordinate Coding for Semi-Supervised Manifold Learning on High Dimensional Non-Linear Manifolds<TITLE>Local Coordinate Coding for Semi-Supervised Manifold Learning on High Dimensional Nonlinear Manifolds with Global<TITLE>Local Coordinate Coding for Semi-Supervised Manifold Learning on High Dimensional Non-Linear Manifolds.<TITLE>Local Coordinate Coding for Semi-Supervised Manifold Learning on High Dimensional Nonlinear Manifolds via Global<TITLE>Local Coordinate Coding for Semi-Supervised Manifold Learning on High Dimensional Nonlinear Manifolds with Application,<TITLE>A Local Coordinate Coding Scheme for Semi-Supervised Learning on Nonlinear Manifolds<TITLE>Local Coordinate Coding for Semi-Supervised Learning on Nonlinear Manifolds.<TITLE>Local Coordinate Coding for Semi-Supervised Learning on Nonlinear Manifolds<TITLE>Global Linear Learning on High Dimensional Nonlinear Manifolds<TITLE>Global Linear Learning on Nonlinear Manifolds,<TITLE>Semi-Supervised Learning on High Dimensional Nonlinear Manifolds<TITLE>Semi-Supervised Learning on High dimensional Nonlinear Manifolds<TITLE>Semi-Supervised Learning of High Dimensional Nonlinear Manifolds<TITLE>Semi-Supervised Learning on High Dimensional Nonlinear Manifold<TITLE>Semi-supervised Learning on High Dimensional Nonlinear Manifolds,<TITLE> Semi-Supervised Learning on High Dimensional Nonlinear Manifolds<TITLE> Semi-Supervised Learning on High Dimensional Nonlinear manifolds<TITLE> Semi-supervised Learning on High Dimensional Nonlinear Manifolds<TITLE> Semi-Supervised Learning on High Dimensional Nonlinear Mixtures<TITLE> Semi-Supervised Learning on High Dimensional Nonlinear Models,<TITLE>Global Linear Programming for Semi-Supervised Learning on High Dimensional Manifolds<TITLE>Global Linear Coding for Semi-Supervised Learning on High Dimensional Manifolds<TITLE>Semi-Supervised Learning on High Dimensional Manifolds via Global Linear Coding<TITLE>Semi-Supervised Learning on High Dimensional Manifolds with Global Linear Functions<TITLE>Semi-Supervised Learning on High Dimensional Manifolds
33,Kernels for Multi--task Learning,"This paper provides a foundation for multi–task learning using reproducing kernel Hilbert spaces of vector–valued functions. In this setting, the kernel is a matrix–valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix– valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi–task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation.",<TITLE>Linear Multi-Task Learning with Matrix-Valued Kernels<TITLE>Linear Multi-task Learning with Matrix-Valued Kernels<TITLE>Linear Multi-Task Learning with Matrix-Valued Kernel Functions<TITLE>On the Representation of Matrix-Valued Kernels<TITLE>Kernel Representations of Vector-Valued Functions,<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernel Hilbert Spaces of Vector-valued Functions.<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions using Hilbert<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions using Reprodu<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions via Represent<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions via Hilbert,<TITLE>Multi-Task Learning with Reproducing Kernels<TITLE>Multi-Task Learning with Reproducing Kernel Hilbert Spaces<TITLE>Multi-task Learning with Reproducing Kernels<TITLE>Multi-Task Learning with Reproducing Kernels<TITLE>Multi-Task Learning with Reproducing Kernels,<TITLE>Multi-Task Learning using Replicating Kernel Hilbert Spaces of Vector–Valuated Functions<TITLE>Multi-Task Learning with Replicating Kernel Hilbert Spaces of Vector–Valuated Functions<TITLE>Multi-Task Learning using Replicating Kernel Hilbert Spaces of Vector–Valid Functions<TITLE>Multi-Task Learning with Replicating Kernels of Vector–Valued Functions<TITLE>Multi-Task Learning using Replicating Kernels of Vector–Valued Functions,<TITLE> Multi-Task Learning with Reproducing Kernel Hilbert Spaces<TITLE> Multitask Learning with Reproducing Kernel Hilbert Spaces<TITLE> Multi–Task Learning with Reproducing Kernel Hilbert Spaces<TITLE> Multi-Task Learning Using Reproducing Kernel Hilbert Spaces<TITLE> Multi–Task Learning Using Reproducing Kernel Hilbert Spaces,<TITLE>Multi-Task Learning with Matrix-Valued Kernels<TITLE>Multi-Task Learning with Matrix-valued Kernels<TITLE>On the Representation of Matrix-Valued Kernels<TITLE>On the Representation of Matrix-valued Kernels<TITLE>Learning Linear Multi-Task Kernels
34,Chinese-English Backward Transliteration Assisted with Mining Monolingual Web Pages,"In this paper, we present a novel backward transliteration approach which can further assist the existing statistical model by mining monolingual web resources. Firstly, we employ the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to rerank the revised candidates based on the information extracted from monolingual web pages. To get a better precision during the reranking process, a variety of web-based information is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be assigned with lower ranks. The experimental results show that the proposed framework can significantly outperform the baseline transliteration system in both precision and recall.",<TITLE>Recovering Transliteration Candidates from Monolingual Web Pages for Statistical Machine Translation<TITLE>Recovering Transliteration Candidates from Monolingual Web Pages for Statistical Modeling<TITLE>Recovering Transliteration Candidates from Monolingual Web Pages with Synthetic Search<TITLE>Recovering Transliteration Candidates from Monolingual Web Pages<TITLE>Recovering Transliteration Candidates Using Monolingual Web Resources,<TITLE>Backward Transliteration with Syllable-Based Search Based on Web-based Information Extraction from Monolingual Web<TITLE>Backward Transliteration with Syllable-Based Search Based on Monolingual Web Resources and Improved Reranking for<TITLE>Backward Transliteration with Syllable-Based Search Based on Monolingual Web Resources and Improved Reranking of<TITLE>Backward Transliteration with Syllable-Based Search Based on Web-based Information Extraction and Reranking of<TITLE>Backward Transliteration with Syllable-Based Search Based on Web-based Information Extraction and Reranking for,<TITLE>A Novel Backward Transliteration Approach Using Syllable-based Search and Monolingual Web Resources<TITLE>A Novel Backward Transliteration Approach Using Syllable-Based Search and Monolingual Web Resources<TITLE>A Novel Backward Transliteration Approach Using Syllable-based Search and Monolingual Web Information<TITLE>A Novel Backward Transliteration Approach Using Syllable-Based Search and Monolingual Web Information<TITLE>A Novel Backward Transliteration Approach Using Syllable-based Search and Web Resources,<TITLE>A Backward Transliteration Approach for Multilingual Web Resources<TITLE>A Backward Transliteration Approach to the Statistic Model<TITLE>Backward Transliteration by Mining Monolingual Web Resource Resources<TITLE>Backward Transliteration by Mining Monolingual Web Resources<TITLE>Backward Transliteration by Mining Monolingual Web Resources,<TITLE> Forward Transliteration for Statistical Machine Translation<TITLE> Backward Transliteration for Statistical Machine Translation<TITLE> Forward Transliteration for Statistical Translation<TITLE> Forward Transliteration for Statistical Machine Translation Using Web-based Information<TITLE> Forward Transliteration for Statistical Machine Translation Using Web-Based Information,<TITLE>Exploiting Monolingual Web Resources for Better Recall and Ranking<TITLE>Exploiting Monolingual Web Resources for Better Recall and Precision<TITLE>Exploiting Monolingual Web Resources for Reranking Candidates<TITLE>Mining Monolingual Web Resources for Better Recall and Ranking<TITLE>Mining Monolingual Web Resources for Better Recall and Precision
35,Cross-domain Semantic Parsing via Paraphrasing,"Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.",<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing with Syntactic Paraphrase<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation Problem<TITLE>Cross-domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing with Domain Adaptation,<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation with Paraphrasing and Pre-Trained Word Embeddings<TITLE>Cross-Domain Semantic Parsing as a Domain Adaptation Problem with Paraphrasing and Pre-Trained Word Emb<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation in Natural Language using Paraphrasing and Pre-Trained Word<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation with Paraphrasing and Pre-Trained Word Embedding<TITLE>Cross-Domain Semantic Parsing as a Domain Adaptation Problem with Paraphrasing and Pre-Trained Word Represent,<TITLE>Cross-Domain Semantic Parsing with Paraphrasing<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation,<TITLE>Cross-Domain Semantic Parsing as a Domain adaptation problem<TITLE>Cross-Domain Semantic Parsing as a Domain adaptation Problem<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation Problem<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing for Domain Adaptation,<TITLE> Cross-Domain Semantic Parsing as a Domain Adaptation Problem<TITLE> Domain Adaptation for Cross-Domain Semantic Parsing<TITLE> Cross-Domain Semantic Parsing with Neural Networks<TITLE> Cross-Domain Semantic Parsing as a Domain Adaptation Task<TITLE> Cross-Domain Semantic Parsing with Adaptive Neural Networks,<TITLE>Cross-Domain Semantic Parsing with Pre-trained Word Embedding<TITLE>Cross-Domain Semantic Parsing as a Domain Adaptation Problem<TITLE>Cross-Domain Semantic Parsing via Paraphrasing<TITLE>Cross-Domain Semantic Parsing with Paraphrasing<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation
36,Estimation of Simultaneously Sparse and Low Rank Matrices.,The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves `1-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.,<TITLE>A Convex Mixed Penalty for Sparse and Low-Rank Matrix Estimation<TITLE>Convex Mixed Penalty for Sparse and Low-Rank Matrix Estimation.<TITLE>A Convex Mixed Penalty for Sparse and Low-Rank Graphs.<TITLE>Convex Mixed Penalty for Sparse and Low-Rank Matrix Estimation<TITLE>A Convex Mixed Penalty for Sparse and Low-Rank Graphs,<TITLE>A Convex Mixed Penalties for Sparse and Low-Rank Matrix Estimation with Application to Protein Interaction Problems.<TITLE>A Convex Mixed Penalties for Sparse and Low-Rank Matrix Estimation with Applications to Protein Interaction Problems.<TITLE>A Convex Mixed Penalties for Sparse and Low-Rank Matrix Estimation with Application to Protein Interaction Resolution.<TITLE>A Convex Mixed Penalties for Sparse and Low-Rank Matrix Estimation with Applications to Protein Interaction Resolution.<TITLE>A Convex Mixed Penalties for Sparse and Low-Rank Matrix Estimation with Application to Protein Interaction Resolution on,<TITLE>Convex Mixed Penalties for Sparse and Low-Rank Matrix Estimation<TITLE>Sparse and Low-Rank Matrix Estimation via Convex Mixed Penalties<TITLE>Sparse and Low-Rank Matrix Estimation with Convex Mixed Penalties<TITLE>Sparse and Low-Rank Matrix Estimation with Convex Mixed Penalties<TITLE>Sparse and Low-Rank Matrix Estimation with Convex Mixed Penalty,<TITLE>A Penalized Matrix Estimation Procedure for Protein Interactions<TITLE>Sentimentalized Matrix Estimation for Protein Interactions<TITLE>A Penalized Matrix Estimation for Protein Interactions<TITLE>Sentenced Matrix Estimation for Protein Interactions<TITLE>A Penalized Matrix Estimation Procedure,<TITLE> A Convex Mixed Penalty for Link Prediction<TITLE> Sparse and Low-Rank Protein Interactions via Convex Mixed Penalty<TITLE> Sparse and Low-Rank Protein Interactions<TITLE> Sparse and Low-Rank Protein Interactions Revisited<TITLE> Sparse and Low-Rank Protein Interactions via Convex Mixed Penalties,<TITLE>Sparse and Low-Rank Matrix Estimation via Convex Mixed Penalties<TITLE>Sparse and Low-Rank Penalties for Adjacency Matrix Estimation<TITLE>Sparse and Low-Rank Matrix Estimation via Convex Mixed Penalty<TITLE>Sparse and Low-Rank Penalties for Protein Interactions<TITLE>Sparse and Low-Rank Optimal Matrix Estimation
37,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,"In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the automated analysis of negation. Unlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of propositional semantics, i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations. We relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date.",<TITLE>A System Combination Approach to Negation Scope Resolution in Implicit Semantics<TITLE>Automated Negation Scope Resolution from the 2012 *SEM Conference<TITLE>Automated Negation Scope Resolution in Underspecified Semantics<TITLE>Automated Negation Scope Resolution for the 2012 *SEM Conference<TITLE>A System Combination Approach to Negation Scope Resolution,<TITLE>Revisiting Shared Task 1: Negation Scope Resolution in Underspecified Semantics with Logical-Form Meaning Represent<TITLE>Revisiting Shared Task 1: Negation Scope Resolution in Underspecified Semantics with Logical-Form Representations<TITLE>A System Combination Approach to Negation Scope Resolution in Underspecified Semantics: An Empirical Study of Shared<TITLE>A System Combination Approach to Negation Scope Resolution in Underspecified Semantics: An Empirical Study on Shared<TITLE>A System Combination Approach to Negation Scope Resolution in Underspecified Semantics: An Empirical Study of the,<TITLE>Automated Negation Scope Resolution over Explicit and Formal Semantics<TITLE>Automated Negation Scope Resolution over Explicit and Formal Representations<TITLE>Automated Negation Analysis over Explicit and Formal Semantics<TITLE>Automated Negation Scope Resolution over Explicit Semantic Representations<TITLE>Automated Negation Scope Resolution over Explicit Semantics,<TITLE>Shared Task 1: the Automated Analyse of Negation<TITLE>Shared Task 1: the Automated Analyzing of Negation<TITLE>Shared Task 1: the Automated Analysis of Negation Scope<TITLE>Shared Task 1: The Automated Analysis of Negation<TITLE>Shared Task 1: the Automated Analysis of Negation,<TITLE> Automatic Interpretation of Negation Scope<TITLE> Automatic Analysis of Negation Scope<TITLE> Automatic Interpretation of Negation Scope in Semantics<TITLE> Automatic Analysis of Negation Scope in Semantics<TITLE> Modeling Negation Scope with Deep Parsing,<TITLE>Revisiting Shared Task 1 from the 2012 *SEM Conference: The Automated Analysis of Negation Scope<TITLE>Revisiting Shared Task 1 from the 2012 *SEM Conference: An Automatic Analysis of Negation Scope<TITLE>Revisiting Shared Task 1 from the 2012 *SEM Conference: The Automatic Analysis of Negation Scope<TITLE>Revisiting Shared Task 1 from the 2012 *SEM Conference: An Automated Analysis of Negation Scope<TITLE>Revisiting Shared Task 1 from the 2012 *SEM Conference: Automatic Analysis of Negation Scope
38,Tree-Structured Infinite Sparse Factor Model.,"A tree-structured multiplicative gamma process (TMGP) is developed, for inferring the depth of a tree-based factor-analysis model. This new model is coupled with the nested Chinese restaurant process, to nonparametrically infer the depth and width (structure) of the tree. In addition to developing the model, theoretical properties of the TMGP are addressed, and a novel MCMC sampler is developed. The structure of the inferred tree is used to learn relationships between high-dimensional data, and the model is also applied to compressive sensing and interpolation of incomplete images.",<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree-based Factor-Analysis<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree-Based Factor-Analysis<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree-based Factor-analysis<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree-Based Factor Analysis<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree-based Factor Analysis,<TITLE>Tree-Structured Multiplicative Gamma Processes for Inferring Depth and Width in High-Dimensional Factor Analysis Models<TITLE>Tree-Structured Multiplicative Gamma Processes for Inferring Depth and Width of Tree-Based Factor Analysis Models.<TITLE>Tree-Structured Multiplicative Gamma Processes for Inferring Depth and Width in High-Dimensional Factor Analysis.<TITLE>Tree-Structured Multiplicative Gamma Processes for Inferring Depth and Width in High-Dimensional Factor Analysis of<TITLE>Tree-Structured Multiplicative Gamma Processes for Inferring Depth and Width of Tree-Based Factor Analysis Modeling,<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree-based Factor Analysis<TITLE>Tree-structured Multiplicative Gamma Processes for Tree-based Factor Analysis<TITLE>Tree-Structured Multiplicative Gamma Processes for Tree Factor Analysis<TITLE>Tree-Structured Multiplicative Gamma Processes<TITLE>Tree-Structured Multiplicative Gamma Processes,<TITLE>Inferring Tree-Based Factor-Analyse Models<TITLE>Inferring Tree-Based Factor-Analyse Model<TITLE>Inferring Tree-Based Factor-Analyse<TITLE>A Tree-Based Multimedia Gamma Process<TITLE>Inferring Tree-Based Factor Analysis,<TITLE> Nested Chinese Restaurant Factor Analysis<TITLE> A Tree-Structured Factor Analysis Model for Chinese Restaurant Processes<TITLE> Nested Chinese Restaurant Factor Analysis Model<TITLE> A Tree-Structured Factor-Analysis Model for Chinese Restaurant Process<TITLE> Nested Chinese Restaurant Factor-Analysis Model,<TITLE>A Tree-structured Multiplicative Gamma Process for Factor Analysis<TITLE>A Tree-Structured Multiplicative Gamma Process<TITLE>Tree-Structured Multiplicative Gamma Processes<TITLE>A Tree-structured Multiplicative Gamma Process<TITLE>Tree-structured Multiplicative Gamma Processes
39,Latent-Variable Synchronous CFGs for Hierarchical Translation,"Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs. In this paper, we extend these techniques to learn latent refinements of single-category synchronous grammars, so as to improve translation performance. We compare two estimators for this latent-variable model: one based on EM and the other is a spectral algorithm based on the method of moments. We evaluate their performance on a Chinese–English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches, but in particular the momentsbased estimator is both faster and performs better than EM.",<TITLE>Latent Refinement of Single-Category Synchronous Grammars for Machine Translation<TITLE>Latent refinements of single-category synchronous grammars for improving translation accuracy<TITLE>Latent refinements of single-category synchronous grammars for improving translation performance<TITLE>Latent Refinement of Synchronous Grammars for Statistical Machine Translation<TITLE>Latent Refinement of Synchronous Grammars for Machine Translation,<TITLE>Learning Latent Refinements of Single Category Synchronous Grammars for Statistical Machine Translation with Moments-Based PCFG<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars for Statistical Machine Translation with Moments-Based PC<TITLE>Learning Latent Refinements of Single Category Synchronous Grammars for Statistical Machine Translation with Moments-based PCFG<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars to Improve Statistical Machine Translation Performance with Moments-<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars for Statistical Machine Translation with Moments-Based Rates,<TITLE>Learning Latent Refinements of Synchronous Grammars for Statistical Machine Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Improved Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Machine Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Improved Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Statistical Machine Translation,<TITLE>Learning Latent Raffination of Single-Category Synchronous Grammars<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars<TITLE>Learning Latent Raffinations of Single-Category Synchronous Grammars<TITLE>Latent Raffination of Single-Category Synchronous Grammars<TITLE>Latent Refinement of Single-Category Synchronous Grammars,<TITLE> Latent Refinement of Synchronous Grammars for Machine Translation<TITLE> Learning Latent Refinement of Synchronous Grammars for Machine Translation<TITLE> Latent Refinement of Synchronous Grammars<TITLE> Learning Latent Refinement of Synchronous Grammars<TITLE> Learning Latent Refinements of Synchronous Grammars,<TITLE>Learning Latent Refinements of Synchronous Grammars for Statistical Machine Translation<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars<TITLE>Learning Latent Refinements for Single-Category Synchronous Grammars<TITLE>Learning Latent Refinements of Synchronous Grammars for Machine Translation<TITLE>Learning Latent Refinements for Synchronous Grammars
40,Synchronous Tree Adjoining Machine Translation,"Tree Adjoining Grammars have well-known advantages, but are typically considered too difficult for practical systems. We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable. Using adjoining to model optionality allows general translation patterns to be learned without the clutter of endless variations of optional material. The appropriate modifiers can later be spliced in as needed. In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data. We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding. Finally, we show that adjoining results in an end-to-end improvement of +0.8 BLEU over a baseline statistical syntax-based MTmodel on a large-scale Arabic/EnglishMT task.",<TITLE>Acquiring Synchronous Tree Adjoining Grammars with Application to Machine Translation<TITLE>Acquiring Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Acquiring Synchronous Tree Adjoining Grammars for Machine Translation<TITLE>Acquisition of Synchronous Tree Adjoining Grammars<TITLE>Acquiring Synchronous Tree Adjoining Grammars,<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-to-English<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-English Machine Translation<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-to-Tree<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding and Decoding in Machine<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-to-Translation,<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical MT<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation,<TITLE>A Synchronous Tree Adjoining Grammar with Adjoining<TITLE>Learning Synchronous Tree Adjoining Grammars and associated Probabilities<TITLE>Learning Synchronous Tree Adjoining Grammars<TITLE>A Synchronous Tree Adjoining Grammar<TITLE>Tree Adjoining Grammars,<TITLE> Improving Syntax-Based Machine Translation with Aligned Tree Adjoining Grammars<TITLE> Improving Syntax-Based Statistical Machine Translation with Aligned Tree Adjoining Grammar<TITLE> Improving Syntax-Based Machine Translation with Aligned Tree Adjoining Grammar<TITLE> Improving Syntax-Based Statistical Machine Translation with Alignments<TITLE> Improving Syntax-Based Statistical Machine Translation by Splitting Alignments,<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars with Model optionality<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation Models<TITLE>Synchronous Tree Adjoining Grammars for Statistical Machine Translation
41,Combining Word Embeddings and Feature Embeddings for Fine-grained Relation Extraction,"Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction.",<TITLE>Combining Features and Word Embeddings for Compositional Representation Learning<TITLE>Conjoins Features and Word Embeddings for Compositional Representations<TITLE>Conjoins Features and Word Embeddings for Compositional Representation<TITLE>Conjoins Features and Word Embeddings for Compositional Models<TITLE>Combining Features and Word Embeddings for Compositional Representations,<TITLE>Joint Learning of Word Embeddings and Features for Compositional Embedding with Application to Linguistic Information Extraction<TITLE>Joint Learning of Word Embeddings and Features for Compositional Embedding with Application to Linguistic Structure Extraction<TITLE>Joint Learning of Word Embeddings and Features for Compositional Embedding with Application to Linguistic Structural Represent<TITLE>Joint Learning of Word Embeddings and Features for Compositional Embedding with Application to Linguistic Structural Rel<TITLE>Joint Learning of Word Embeddings and Features for Compositional Embedding with Application to Linguistic Structural Information,<TITLE>Scalable Feature Embedding Models for Fine-Grained Relation Extraction<TITLE>Compositional Embedding Models for Fine-Grained Relation Extraction<TITLE>Learning Feature Embeddings for Compositional Embedding Modeling<TITLE>Learning Feature Embeddings for Compositional Embedding Models<TITLE>Compositional Embedding Models with Feature Embeddings,<TITLE>Learning Features and Word Embeddings with Compositional Models<TITLE>Compositional Word Embeddings for Linguistic Structures<TITLE>Conjoining Features and Word Embeddings for Language Structure<TITLE>Compositional Word Embeddings for Linguistic Structure<TITLE>Conjoining Features and Word Embeddings,<TITLE> Conjunctive Embedding Models for Relation Extraction<TITLE> A Joint Model for Compositional Embedding<TITLE> A Joint Model for Compositional Embedding Models<TITLE> Conjunctive Embedding Models<TITLE> A Joint Model for Compositional Embedding Representation,<TITLE>Joint Learning of Feature Embeddings and Word Embeddings for Fine-grained Relation Extraction<TITLE>Joint Learning of Feature Embeddings and Word Embeddings for Compositional Embeddings<TITLE>Joint Learning of Feature Embeddings and Word Embeddings for Improved Compositional Representations<TITLE>Joint Learning of Feature Embeddings and Word Embeddings for Compositional Embedding<TITLE>A Joint Model for Conjoint Feature and Word Embeddings
42,Semantic Role Recognition Using Kernels on Weighted Marked Ordered Labeled Trees,"We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classification during runtime remained to be solved. In this paper, we give a solution that uses an efficient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification.",<TITLE>Fast and Accurate Semantic Role Argument Recognition with Weighted marked Ordered Labeled Trees<TITLE>Fast and Accurate Semantic Role Argument Recognition using Weighted marked Ordered Labeled Trees<TITLE>Fast and Accurate Recognition of Semantic Role Arguments with Weighted marked Labeled Trees<TITLE>Fast Semantic Role Argument Recognition with Weighted marked Ordered Labeled Trees<TITLE>Fast and Efficient Kernel for Semantic Role Argument Recognition,<TITLE>Fast Kernel for Recognizing Semantic Role Arguments using Weighted Marked Ordered Labeled Tree Kernels and Fast Training<TITLE>Fast Kernel for Recognizing Semantic Role Arguments using Weighted Marked Ordered Labeled Tree Kernels and Efficient<TITLE>Fast Kernel for Recognizing Semantic Role Arguments using Weighted Marked Ordered Labeled Tree Kernels and its Application<TITLE>Fast Kernel for Recognizing Semantic Role Arguments using Weighted Marked Ordered Labeled Tree Kernels and the WM<TITLE>Fast Kernel for Recognizing Semantic Role Arguments using Weighted Marked Ordered Labeled Tree Kernels and Fast Pred,<TITLE>A Kernel on Weighted Marked Ordered Trees for Semantic Role Argument Recognition<TITLE>A Kernel on Weighted Marked Ordered Trees for Semantic Role Arguments Recognition<TITLE>A Kernel on Weighted Marked Ordered Trees for Semantic Role Argument Recognition<TITLE>A Kernel on Weighted Marked Ordered Trees for Semantic Role Arguments<TITLE>A Kernel on Weighted Marked Ordered Trees for Semantic Role Arguments,<TITLE>Identifying Semantic Role Arguments using WMOLT Kernels<TITLE>Detecting Semantic Role Arguments using WMOLT Kernels<TITLE>Identifying Semantic Role Arguments with WMOLT Kernels<TITLE>Identifying Semantic Role Arguments using the WMOLT kernel<TITLE>The WMOLT Kernel for Semantic Role Argument Recognition,<TITLE> Semantic Role Arguments Recognition Using Weighted Marked Trees<TITLE> Semantic Role Arguments Recognition Using Weighted Mined Trees<TITLE> Semantic Role Arguments Recognition using Weighted Marked Trees<TITLE> Semantic Role Argument Recognition Using Weighted Marked Trees<TITLE> Semantic Role Arguments Recognition Using Weighted Kernels,<TITLE>Efficient Classification of Semantic Role Arguments Using the WMOLT Kernel on marked-ordered trees<TITLE>Efficient Classification of Semantic Role Arguments Using the WMOLT Kernel on marked Ordered Parser<TITLE>Efficient Classification of Semantic Role Arguments Using the WMOLT Kernel on marked ordered trees<TITLE>Efficient Classification of Semantic Role Arguments Using the WMOLT Kernel on marked Ordered Trees<TITLE>Efficient Classification of Semantic Role Arguments Using a Kernel on marked ordered trees
43,Robust Incremental Neural Semantic Graph Parsing,"Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.",<TITLE>Neural Transition-based Semantic Graph Parsing for Minimal Recursion Semantics<TITLE>Neural Transition-based Semantic Graph Parsing for Minimal Recursion Semantics.<TITLE>Neural Encoder-Decoder Transition-based Semantic Graph Parsing<TITLE>Semantic Graph Parsing with Stack-based Embedding Features<TITLE>Semantic Graph Parsing for Minimal Recursion Semantics,<TITLE>Neural Encoder-Decoder Transition-based Parsing of Minimal Recursion Semantic Parsers for Natural Language Processing<TITLE>Neural Encoder-Decoder Transition-based Parsing of Minimal Recursion Semantic Representations for Natural Language Processing<TITLE>Neural Encoder-Decoder Transition-based Parsing for Minimal Recursion Semantic Parsing with Stack Embedd<TITLE>Neural Encoder-Decoder Transition-based Parsing for Minimal Recursion Semantic Parsing of Sentences to<TITLE>Neural Encoder-Decoder Transition-based Parsing for Minimal Recursion Semantic Parsing with Stack-based,<TITLE>A Neural Transition-based Parser for Minimal Recursion Semantics<TITLE>A Neural Transition-Based Parser for Minimal Recursion Semantics<TITLE>A Neural Transition-based Parser for Minimal-Recursion Semantics<TITLE>A Neural Transition-based Parser for Minimal Recursion Semantics<TITLE>A Neural Transition-Based Parser for Minimal Recursion Semantics,<TITLE>Full-coverage Semantic Graph Parsing for Minimal Recursion Semantics<TITLE>Minimal Recursion Semantics with Neural Encoder-Decoder Transitions<TITLE>Minimal Recursion Semantics with Neural Encoder-Decoder Transition<TITLE>Minimal Recursion Semantics with Neural Graph Parsing<TITLE>Graph Parsing for Minimal Recursion Semantics,<TITLE> Neural Decoding for Minimal Recursion Semantics<TITLE> Minimal Recursion Semantic Graph Parsing<TITLE> Minimal Recursion Semantic Parsing<TITLE> Minimal Recursion Semantic Parsing with Neural Encoders<TITLE> Minimal Recursion Semantic Graph Parsing with Neural Encoders,<TITLE>Neural-Decoder Parsing for Minimal Recursion Representations<TITLE>Neural-Decoder Parsing for Minimal Recursions<TITLE>Neural-Decoder Parsing of Minimal Recursions<TITLE>Neural-Decoder Parsing for Minimal Recursion Representation<TITLE>Minimal Recursion Parsing with Neural Networks
44,IBC-C: A Dataset for Armed Conflict Analysis,"We describe the Iraq Body Count Corpus (IBC-C) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. IBCC provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. IBCC is constructed using data collected by the Iraq Body Count project which has been recording incidents from the ongoing war in Iraq since 2003. We describe the dataset’s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks.",<TITLE>Iraq Body Count Corpus: A Ground-Truth Dataset for Conflict Resolution and Event Deplication<TITLE>Iraq Body Count Corpus: A Ground-Truth Dataset for Conflict Analysis and Event Deplication<TITLE>The Iraq Body Count Corpus: A Ground-Truth Dataset for Conflict Analysis and Resolution<TITLE>Iraq Body Count Corpus: A Ground-Truth Dataset for Conflict Analysis and Resolution<TITLE>The Iraq Body Count Corpus: A Ground-Truth Dataset for Conflict Analysis,"<TITLE>The Iraq Body Count Corpus: A Dataset for Conflict-related Named Entity Recognition, Slot Filling, and De<TITLE>The Iraq Body Count Corpus: A Dataset for Conflict-related Named Entity Recognition, Slot Filling, and Event<TITLE>The Iraq Body Count Corpus: A Dataset for Conflict-related Named Entity Recognition, Slot Filling and De-<TITLE>The Iraq Body Count Corpus: A Dataset for Conflict-related Named Entity Recognition, Slot Filling, and Al<TITLE>The Iraq Body Count Corpus: A Dataset for Conflict-related Named Entity Recognition, Slot Filling and Event De",<TITLE>Iraq Body Count Corpus: A Ground-truth Dataset for Conflict Analysis and Named Entity Recognition<TITLE>Iraq Body Count Corpus: The Iraq Body Count Dataset for Conflict Analysis and Named Entity Recognition<TITLE>Iraq Body Count Corpus: A Ground-truth Dataset for Conflict Analysis and Named Entity Recognition<TITLE>Iraq Body Count Corpus: A Ground-truth Dataset for Conflict Analysis<TITLE>Iraq Body Count Corpus: The Iraq Body Count Dataset for Conflict Analysis,<TITLE>IBC-C: The Iraq Body Count Corpus<TITLE>The Iraq Body Count Corpus Dataset for Conflict Analysis<TITLE>The Iraq Body Count Corpus Dataset<TITLE>The Iraq Body Count Corpus<TITLE>Iraq Body Count Corpus,<TITLE> Iraq Body Count: A New Dataset for Conflict Analysis<TITLE> Iraq Body Count: A Case Study on Name Entity Recognition<TITLE> Iraq Body Count: A Dataset and Benchmarks<TITLE> Iraq Body Count: A New Dataset and Benchmark<TITLE> Iraq Body Count: A Dataset and Benchmark,"<TITLE>The Iraq Body Count Corpus for Named Entity Recognition, Slot Filling, and Event Deduplication<TITLE>Named Entity Recognition, Slot Filling, and Event Deduplication from the Iraq Body Count Corpus<TITLE>Named Entity Recognition, Slot Filling, and Event Deduplication from the Iraq Body Count Dataset<TITLE>The Iraq Body Count Corpus for Named Entity Recognition, Slot Filling and Event Deduplication<TITLE>The Iraq Body Count Corpus for Named Entity Recognition, Slot Filling, and Deduplication"
45,Bregman Alternating Direction Method of Multipliers,"We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.",<TITLE>Generalization Error Bounds on the Input-Output Mutual Information<TITLE>Lower Bounds on Generalization Error of Learning Algorithms<TITLE>Generalization Error Bounds on Input-Output Mutual Information<TITLE>Generalization Error Bounds for Learning with Mutual Information<TITLE>Lower Bounds on Generalization Error in Learning Problems,<TITLE>Generalization Error Bounds on the Generalization Error of Learning Algorithms via Mutual Information Control and Random Noise Regularization.<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Control and Random Noise Regularization.<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Regularization and Random Noise Control.<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Regularization and Random Noise in Rein<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Regularization and Random Noise in E,<TITLE>Generalization Error Bounds for Learning Algorithms with Relative Entropy and Random Noise<TITLE>On the Generalization Error of Learning Algorithms with Relative Entropy and Random Noise<TITLE>Generalization Error Bounds for Learning Algorithms with Relative Entropy and Random Noise<TITLE>On the Generalization Error of Learning Algorithms<TITLE>Generalization Error Bounds for Learning Algorithms,<TITLE>Generalization of a Learning Algorithm<TITLE>Generalization of Learning Algorithms<TITLE>Generalization in Learning with Mutual Information<TITLE>Generalization in Learning Problems<TITLE>Generalization in Learning,<TITLE> Bounds on Generalization Error of Learning Algorithms<TITLE> Information-Theoretic Bounds on Generalization Error<TITLE> Bounds on Generalization Error in Learning Problems<TITLE> Information-Theoretic Generalization Error Bounds<TITLE> Bounds on Generalization Error in Learning,<TITLE>Upper Bounds on the Generalization Error<TITLE>On the Generalization Error of Learning Algorithms<TITLE>On the Generalization Error of ERM<TITLE>Upper Bounds on the ERM Error<TITLE>Upper bounds on the ERM error
46,Deep Exploration via Bootstrapped DQN,"E cient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as ‘-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically e cient RL are not computationally tractable in complex environments. Randomized value functions o er a promising approach to e cient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.",<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning in Arcade Learning Environment<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning in Complex Environments<TITLE>Bootstrapped Reinforcement Learning with Randomized Value Functions<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning<TITLE>Bootstrapped Deep Exploration with Generalization,<TITLE>Bootstrapped DQN: Bootstrapped Exploration with Deep Neural Networks for Efficient Reinforcement Learning in Complex Environments<TITLE>Bootstrapped DQN: Bootstrapped Exploration with Generalized Value Functions for Efficient Reinforcement Learning in Complex En<TITLE>Bootstrapped DQN: Bootstrapped Exploration with Generalization and Deep Neural Networks for Efficient Reinforcement Learning in<TITLE>Bootstrapped DQN: Bootstrapped Exploration with Generalization and Deep Neural Networks for Efficient Reinforcement Learning.<TITLE>Bootstrapped DQN: Bootstrapped Exploration with Deep Neural Networks for Efficient Reinforcement Learning in Complex Reinforcement,<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning in Arcade Games<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning<TITLE>Bootstrapped Deep Exploration with Generalization<TITLE>Bootstrapped Deep Exploration for Reinforcement Learning,<TITLE>Bootstrapped DQN for Exponentally Faster Reinforcement Learning<TITLE>Bootstrapped DQN for Exponently Faster Reinforcement Learning<TITLE>Bootstrapped DQN for Expansive Reinforcement Learning<TITLE>Bootstrapped DQN for E cient Exploration<TITLE>Bootstrapped DQN for E Cient Exploration,<TITLE> Randomized Value Functions for Efficient Exploration<TITLE> Bootstrapped DQN for Efficient Exploration<TITLE> DQN: A Bootstrapped DQN Approach to Efficient Exploration<TITLE> Bootstrapped DQN: Efficient Exploration with Deep Neural Networks<TITLE> Bootstrapped DQN: A Bootstrapping Approach to Efficient Exploration,<TITLE>bootstrappedN: Accelerating Deep Exploration for Reinforcement Learning<TITLE>bootstrappedN: Accelerating Reinforcement Learning with Deep Exploration<TITLE>bootstrappedN: Faster Deep Exploration for Reinforcement Learning<TITLE>Deep Reinforcement Learning with bootstrappedN<TITLE>Deep Reinforcement Learning with BootstrappedN
47,An Empirical Study of Chinese Name Matching and Applications,"Methods for name matching, an important component to support downstream tasks such as entity linking and entity clustering, have focused on alphabetic languages, primarily English. In contrast, logogram languages such as Chinese remain untested. We evaluate methods for name matching in Chinese, including both string matching and learning approaches. Our approach, based on new representations for Chinese, improves both name matching and a downstream entity clustering task.",<TITLE>Chinese Name Matching in Logogram Languages Using New Representations<TITLE>Name Matching in Chinese: New Representations and Learning Methods<TITLE>Name Matching in Chinese: New Representations and Methods<TITLE>Name Matching in Chinese Using New Representations<TITLE>Chinese Name Matching in Logogram Languages,<TITLE>Named Entity Clustering and String Matching in Chinese: A Case Study on Alphabetic Languages using Word Vectors<TITLE>Named Entity Clustering and String Matching in Chinese: A Case Study on Alphabetic Languages using Word Embeddings<TITLE>Named Entity Clustering and String Matching in Chinese: A Case Study on Alphabetic Languages using Word Embedding<TITLE>Named Entity Clustering and String Matching in Chinese: A Case Study on Alphabetic Languages Using Word Representations for<TITLE>Named Entity Clustering and String Matching in Chinese: A Case Study on Alphabetic Languages using Word Representations for,<TITLE>Name Matching in Logogram Languages: A Case Study on Chinese<TITLE>Name Matching in Logogram Languages: A Case for Chinese<TITLE>Name Matching in Logogram Languages with New Representations<TITLE>Name Matching in Logogram Languages with New Representations<TITLE>Name Matching in Logogram Languages,<TITLE>Name Matching and Entity Clustering in Chinese<TITLE>Chinese Name Matching with Chinese Entity Clustering<TITLE>Name Matching and Entity Clustering for Chinese<TITLE>Chinese Name Matching and Entity Clustering<TITLE>Name Matching and Entity Clustering,<TITLE> Name Matching in Logogram Languages<TITLE> Chinese Named Entity Matching Using String Matching and Learning<TITLE> Name Matching in Logogram Languages via String Matching<TITLE> Name Matching in Logogram Languages with New Representations<TITLE> Name Matching in Logogram Language,<TITLE>Name Matching in Chinese: New Representations and Learning<TITLE>Name Matching in Chinese: A Case Study<TITLE>Name Matching in Chinese: An Evaluation<TITLE>Name Matching in Chinese<TITLE>Name Matching for Chinese
48,The Importance of Generation Order in Language Modeling,"Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence ""templates"" and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of the generation order for neural language models.",<TITLE>The Effect of Token Generation Order on Neural Language Models<TITLE>Evaluating Token Generation Order for Neural Language Models<TITLE>Generating Templates for Neural Language Models<TITLE>Token Generation Order for Neural Language Models<TITLE>Token Generation Order in Neural Language Models,<TITLE>Two-Pass Language Models for Neural Machine Translation and Summarization: The Impact of Token Generation Order on Model Quality in Neural<TITLE>Two-Pass Language Models for Neural Machine Translation and Summarization: The Impact of Token Generation Order in the Two-Pass<TITLE>Two-Pass Language Models for Neural Machine Translation and Summarization: The Impact of Token Generation Order on Templates and Missing<TITLE>Two-Pass Language Models for Neural Machine Translation and Summarization: The Impact of Token Generation Order on Templates and Hidden<TITLE>Two-Pass Language Models for Neural Machine Translation and Summarization: The Impact of Token Generation Order on Model Quality in the,<TITLE>Generation Order in Neural Language Models: A Two-Pass Approach<TITLE>Generation Order for Neural Language Models: A Two-Pass Approach<TITLE>Generation Order in Neural Language Models: A Two-Pass Analysis<TITLE>Generation Order for Neural Language Models: A Two-Pass Analysis<TITLE>Generation Order for Neural Language Models: A Two-Pass Approach,<TITLE>A Two-Pass Language Model for Neural Language Models<TITLE>Two-Pass Language Models for Neural Language Models<TITLE>A Two-Pass Language Model for Neural Language Model Generation<TITLE>Two-Pass Generation Order for Neural Language Models<TITLE>The Impact of Token Generation Order on Model Quality,<TITLE> Generating Templates for Neural Language Models<TITLE> A Two-Pass Language Model for Neural Machine Translation<TITLE> Generating Templates for Neural Machine Translation<TITLE> Generating Templates for Neural Language Models: A Two-Pass Approach<TITLE> Generating Templates for Neural Language Models: A Two-Pass Model,<TITLE>An Empirical Study of the Effect of Generation Order on Neural Language Models<TITLE>An Empirical Study of the Effect of Generation Order for Neural Language Models<TITLE>An Empirical Study of the Effect of Generation Order in Neural Language Models<TITLE>Investigating the Influence of Token Generation Order on Neural Language Models<TITLE>Investigating the Influence of Generation Order on Neural Language Models
49,"Si O No, Que Penses? Catalonian Independence and Linguistic Identity on Social Media","Political identity is often manifested in language variation, but the relationship between the two is still relatively unexplored from a quantitative perspective. This study examines the use of Catalan, a language local to the semi-autonomous region of Catalonia in Spain, on Twitter in discourse related to the 2017 independence referendum. We corroborate prior findings that pro-independence tweets are more likely to include the local language than anti-independence tweets. We also find that Catalan is used more often in referendum-related discourse than in other contexts, contrary to prior findings on language variation. This suggests a strong role for the Catalan language in the expression of Catalonian political identity.",<TITLE>The role of Catalan in the expression of Catalonian political identity on Twitter<TITLE>The Role of the Catalan Language in the Emergence of Catalan Political Identity<TITLE>The Role of Catalan in the Emergence of Catalonian Political Identity<TITLE>The role of Catalan in the expression of Catalonian political identity<TITLE>The Role of Catalan in the Expression of Catalonian Political Identity,"<TITLE>The use of Catalan, a Language Local to the Semi-Autonomous Region of Catalonia for the expression of Catalonian Political Identity<TITLE>The use of Catalan, a Language Local to the Semi-Autonomous Region of Catalonia in Twitter Discourse related to the 2017<TITLE>The use of Catalan, a Language Local to the Semi-Autonomous Region of Catalonia for the expression of Catalonian political identity<TITLE>The use of Catalan, a Language Local to the Semi-Autonomous Region of Catalonia in Twitter Discourse related to the Independence<TITLE>The use of Catalan, a Language Local to the Semi-Autonomous Region of Catalonia for the expression of Catalonian Political Ident",<TITLE>The role of the Catalan language in political identity and language variation on Twitter<TITLE>The role of the Catalan Language in the Expression of Catalonian Political Identity<TITLE>The role of the Catalan language in political identity on Twitter<TITLE>Catalan Language and Political Identity in Twitter Discourse<TITLE>Catalan Language and Political Identity on Twitter,<TITLE>The Use of Catalan in Discourse related to the 2017 independence referendum<TITLE>The Use of Catalan in Discourse Related to the 2017 Independence referendum<TITLE>The Use of Catalan in Discourse related to the 2017 Independence referendum<TITLE>The Use of Catalan in Discourse Related to the 2017 independence referendum<TITLE>Catalonian Political Identity Using Twitter in Discourse,<TITLE> Language and Political Identity in Catalan<TITLE> Language and Political Identity in Spanish<TITLE> Carca: A Corpus of Spanish Political Identity Discourse<TITLE> Language and Political Identity in Catalan: A Case Study on Twitter<TITLE> Carca: A Corpus of Spanish Political Identity,<TITLE>The role of the Catalan language in the expression of Catalonian political identity<TITLE>The role of the Catalan language in the political identity of Catalonia on Twitter<TITLE>The Role of the Catalan Language in the Expression of Political Identity<TITLE>The role of the Catalan language in the political identity of Catalonia<TITLE>The Role of the Catalan Language in Political Identity
50,NeuralREG: An end-to-end approach to referring expression generation,"Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.",<TITLE>Neural Referring Expression Generation with Deep Neural Networks<TITLE>Neural Referring Expression Generation with Deep Neural Networks.<TITLE>Neural Referring Expression Generation via Deep Neural Networks.<TITLE>Neural Referring Expression Generation via Deep Neural Networks<TITLE>Neural Referring Expression Generation Using Deep Neural Networks,<TITLE>Neural Referring Expression Generation with Delexicalized Web Corpora and Deep Neural Networks (without Explicit Feature Extraction)<TITLE>Neural Referring Expression Generation with Delexicalized Web Corpora and Deep Neural Networks: A Case Study on Discourse<TITLE>Neural Referring Expression Generation with Delexicalized Web Corpora and Deep Neural Networks: A Case Study on the Web<TITLE>Neural Referring Expression Generation with Delexicalized Web Corpora and Deep Neural Networks: An Application to Discourse Ent<TITLE>Neural Referring Expression Generation with Delexicalized Web Corpora and Deep Neural Networks: A Case Study in Textual,<TITLE>Neural Referring Expression Generation with Deep Neural Networks<TITLE>Neural Referring Expression Generation for WebNLG<TITLE>Neural Referring Expression Generation: A Neural Approach<TITLE>Neural Referring Expression Generation with Deep Neural Networks<TITLE>Neural Referring Expression Generation for WebNLG,<TITLE>Neural Referring Expression Generation with Deep Neural Networks<TITLE>Neural Referring Expression Generation: A Neural Approach<TITLE>Deep Neural Networks for Referring Expression Generation<TITLE>Neural Networks for Referring Expression Generation<TITLE>Neural Referring Expression Generation,<TITLE> Neural REG for Referring Expression Generation<TITLE> Neural REG Models for Referring Expression Generation<TITLE> Neural REG: A Deep Neural Model for Referring Expression Generation<TITLE> Neural REG Modeling for Referring Expression Generation<TITLE> Neural REG for Referring Expression Generation: A Neural Approach,<TITLE>Neural REG: A Neural Approach to Referring Expression Generation<TITLE>Neural REG: A Neural Approach for Referring Expression Generation<TITLE>Neural REG: Referring Expression Generation with Deep Neural Networks<TITLE>Neural REG: Referring Expression Generation Using Deep Neural Networks<TITLE>Neural REG: Referring Expression Generation with Neural Networks
51,Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages,"Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and evaluate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.",<TITLE>Semi-Supervised Learning of Part-of-Speech Tagging Techniques for Low-Resource Languages<TITLE>Semi-Supervised Learning of Part-of-Speech Tagging for Low-Resource Languages<TITLE>Semi-Supervised Learning of Part-of-Speech Tagging for Low-resource Languages<TITLE>Semi-supervised Learning of Part-of-Speech Tagging for Low-Resource Languages<TITLE>Semi-supervised Learning of Part-of-Speech Tagging for Low-resource Languages,<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Words into Corpora<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Extracted Annotation of Word Types in Low-<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Word Types in Low<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Word Types in the<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Word Type Information.,<TITLE>How Much Data Do You Need to Train a Semi-Supervised Part-of-Speech Tagger?<TITLE>How Much Data Do You Need for Semi-Supervised Part-of-Speech Tagging?<TITLE>How much data is necessary to train a semi-supervised part-of-speech tagger?<TITLE>How Much Data Do You Need to Train a Semi-Supervised POS-Tagger?<TITLE>How much data is necessary to train a semi-supervised POS-tagger?,<TITLE>Part-of-Speech Tagging with Timed Annotations<TITLE>Part-of-Speech Tagging with Timed Annotation<TITLE>Semi-Supervised Learning for Low-Resource Language Processing<TITLE>Semi-Supervised Learning for Low-Resource Languages<TITLE>Part-of-Speech Tagging for Language Processing,<TITLE> Semi-Supervised Learning of Part-of-Speech Tagging<TITLE> Semi-Supervised Learning for Low-Resource Languages<TITLE> Semi-Supervised Learning of Low-Resource Languages<TITLE> Semi-Supervised Learning of Part-of-Speech Tagging Using Data from Low-Resource Languages<TITLE> Semi-Supervised Learning of Part-of-Speech Tagging Using Finite-State Morphology,<TITLE>How Much Data Does a Semi-Supervised Part-of-Speech Tagging System Need?<TITLE>How Much Data Does a Semi-Supervised Part-of-Speech Tagging System Use?<TITLE>How Much Data Do We Need to Train a Part-of-Speech Tagging System?<TITLE>How Much Data Should We Use for Part-of-Speech Tagging?<TITLE>How Much Data Does a Semi-Supervised POS-Tagger Need?
52,Context-Sensitive Lexicon Features for Neural Sentiment Analysis,"Sentiment lexicons have been leveraged as a useful source of features for sentiment analysis models, leading to the state-of-the-art accuracies. On the other hand, most existing methods use sentiment lexicons without considering context, typically taking the count, sum of strength, or maximum sentiment scores over the whole input. We propose a context-sensitive lexicon-based method based on a simple weighted-sum model, using a recurrent neural network to learn the sentiments strength, intensification and negation of lexicon sentiments in composing the sentiment value of sentences. Results show that our model can not only learn such operation details, but also give significant improvements over state-of-the-art recurrent neural network baselines without lexical features, achieving the best results on a Twitter benchmark.",<TITLE>Context-sensitive Sentiment Lexicon Learning with Recurrent Neural Networks<TITLE>Context-sensitive Sentiment Lexicon Learning with Recurrent Neural Network<TITLE>Learning Sentiment Lexicons from Recurrent Neural Networks<TITLE>Learning Sentiment Lexicons with Recurrent Neural Networks<TITLE>Learning Sentiment Lexicons from Recurrent Neural Network,<TITLE>Learning Sentiment Lexicons for Sentiment Analysis using Weighted-Sum Model and Recurrent Neural Networks Based on a Recurrent<TITLE>Learning Sentiment Lexicons for Sentiment Analysis using Weighted-Sum Model and Recurrent Neural Networks with Context-sensitive Neural<TITLE>Learning Sentiment Lexicons for Sentiment Analysis using Weighted-Sum Model and Recurrent Neural Networks with Context Sensitive Neural<TITLE>Learning Sentiment Lexicons for Sentiment Analysis using Weighted-Sum Model and Recurrent Neural Networks with Context Sensitive Sent<TITLE>Learning Sentiment Lexicons for Sentiment Analysis using Weighted-Sum Model and Recurrent Neural Networks with Context Sensitive L,<TITLE>Learning Lexical Features for Context-Sensitive Sentiment Analysis with Recurrent Neural Networks<TITLE>Learning Lexical Features for Context-sensitive Sentiment Analysis with a Recurrent Neural Network<TITLE>Learning Lexical Features for Context-Sensitive Sentiment Analysis using Recurrent Neural Networks<TITLE>Learning Sentiment Lexicons for Context-Sensitive Neural Sentiment Analysis<TITLE>Learning Sentiment Lexicons for Context-sensitive Neural Sentiment Analysis,<TITLE>Context-Sensitive Sentiment Lexicons with Recurrent Neural Network<TITLE>Context-Sensitive Sentiment Lexicons for Sentiment Analysis<TITLE>Context-sensitive Sentiment Lexicons for Sentiment Analysis<TITLE>Context-sensitive Sentiment Lexicons with Recurrent Neural Networks<TITLE>Context-Sensitive Sentiment Lexicons,<TITLE> Context Sensitive Lexicon Based Sentiment Analysis<TITLE> Context Sensitive Lexicon Learning for Sentiment Analysis<TITLE> Context Sensitive Lexicon Based Sentiment Classification<TITLE> Context Sensitive Lexicon Based Sentiment Modeling<TITLE> Context Sensitive Lexicon-Based Sentiment Analysis,<TITLE>A Context-sensitive Lexicon-based Sentiment Analysis Method Using Recurrent Neural Networks<TITLE>A Context-sensitive Lexicon-based Sentiment Analysis Model Using Recurrent Neural Networks<TITLE>A Context-sensitive Lexicon-based Sentiment Analysis Method with Recurrent Neural Networks<TITLE>A Context-sensitive Lexicon-based Sentiment Analysis Model using Recurrent Neural Networks<TITLE>A Context-sensitive Lexicon-based Sentiment Analysis Method Using Recurrent Neural Network
53,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.",<TITLE>A Joint Many-Task Model for Transfer and Multi-Task Learning<TITLE>A Joint Many-Task Model for Transfer and Multi-task Learning<TITLE>Joint Multi-Task Learning with Linguistic Hierarchies<TITLE>Joint Multi-Task Learning with Linguistic Hierarchy<TITLE>A Joint Many-Task Model for Multi-Task Learning,<TITLE>Joint Many-Task Transfer and Multi-task Learning with Regularization of Linguistic Hierarchies: A Case Study<TITLE>Joint Many-Task Transfer and Multi-Task Learning with Regularization of Linguistic Hierarchies: A Case Study<TITLE>Joint Many-Task Transfer and Multi-task Learning with Regularization of Linguistic Hierarchies: a Case Study<TITLE>Joint Many-Task Transfer and Multi-task Learning with Regularization of Linguistic Hierarchies: An Application to<TITLE>Joint Many-Task Transfer and Multi-task Learning with Regularization of Linguistic Hierarchies for Improved Task Prediction,<TITLE>Joint Many-Task Learning for Transfer and Multi-Task Tasks<TITLE>Joint Many-Task Learning with Hierarchical Linguistic Structure<TITLE>Joint Many-Task Learning with Hierarchical Structural Representations<TITLE>Joint Many-Task Learning with Hierarchical Structures<TITLE>Joint Many-Task Learning with Hierarchical Structures,<TITLE>Multi-Task Transfer and Multi-Task Learning<TITLE>Joint Multi-Task Learning with a Single Model<TITLE>Joint Multi-Task Learning for a Single Model<TITLE>Joint Multi-Task Learning for Complex Tasks<TITLE>Joint Multi-Task Learning for Multiple Tasks,"<TITLE> Joint Many-Task Learning for Morphology, Syntax and Semantics<TITLE> Joint Many-Task Morphology Learning<TITLE> Jointly Learning Morphology, Syntax and Semantics<TITLE> Joint Many-Task Morphology Modeling<TITLE> Joint Many-Task Learning for Morphological Analysis","<TITLE>A Joint Many-Task Model for Morphology, Syntax and Semantics<TITLE>Joint Many-Task Learning for Morphology, Syntax, and Semantics<TITLE>A Joint Many-Task Model for Morphological, Syntax and Semantic Learning<TITLE>A Joint Many-Task Model for Morphological, Syntax and Semantics<TITLE>Joint Many-Task Learning for Morphology, Syntax and Semantics"
54,An Empirical Study of Automatic Chinese Word Segmentation for Spoken Language Understanding and Named Entity Recognition,"Word segmentation is usually recognized as the first step for many Chinese natural language processing tasks, yet its impact on these subsequent tasks is relatively under-studied. For example, how to solve the mismatch problem when applying an existing word segmenter to new data? Does a better word segmenter yield a better subsequent NLP task performance? In this work, we conduct an initial attempt to answer these questions on two related subsequent tasks: semantic slot filling in spoken language understanding and named entity recognition. We propose three techniques to solve the mismatch problem: using word segmentation outputs as additional features, adaptation with partial-learning and taking advantage of n-best word segmentation list. Experimental results demonstrate the effectiveness of these techniques for both tasks and we achieve an error reduction of about 11% for spoken language understanding and 24% for named entity recognition over the baseline systems.",<TITLE>How to Solve the Mismatch Problem in Chinese Word Segmentation<TITLE>Using Word Segmentation Output to Solve the Mismatch Problem<TITLE>How to Solve the Mismatch Problem in Chinese NLP<TITLE>Using Word Segmentation Output for Chinese NLP Tasks<TITLE>Improving Word Segmentation for Chinese NLP Tasks,<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Spoken Language Understanding and NLP Tasks<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese NLP Tasks: An Initial Investigation<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese NLP Tasks: An Application to<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese Natural Language Processing Tasks: An Initial<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese NLP Tasks: An Application of,<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition Tasks<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem of Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem in Word Segmentation Tasks for Chinese NLP,<TITLE>Resolving the Mismatch Problem with Semantic Slot Filling in Word Segmentation<TITLE>Resolving the Mismatch Problem with Neural Word Segmentation Outputs<TITLE>Resolving the Mismatch Problem with Word Segmentation Outputs<TITLE>Resolving the Mismatch Problem with Semantic Slot Filling<TITLE>Resolving the Mismatch Problem with Segmentation Outputs,<TITLE> Semantic Slot Filling in Spoken Language Understanding and Named Entity Recognition<TITLE> Improving Semantic Slot Filling by Adapting Word Segmentation Outputs<TITLE> Semantic Slot Filling in Spoken Language Understanding Using Word Segmentation<TITLE> Semantic Slot Filling in Spoken Language Understanding<TITLE> Semantic Slot Filling in Spoken Language Understanding and NER,<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and named entity recognition<TITLE>Solving the Mismatch in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Improving Word Segmentation for Spoken Language Understanding and Named Entity Recognition in Chinese<TITLE>Improving Word Segmentation for Spoken Language Understanding and Named Entity Recognition
55,Cross-lingual Opinion Analysis via Negative Transfer Detection,"Transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages. However, the cumulative class noise in transfer learning adversely affects performance when more training data is used. In this paper, we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers. Evaluation on NLP&CC 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used.",<TITLE>Detecting Negative Transfer Effects in Transductive Transfer Learning.<TITLE>Detecting Negative Transfer Effects in Transductive Transfer Learning<TITLE>Detecting Negative Transfer Data in Transductive Transfer Learning<TITLE>Detecting Negative Transfer Messages in Transductive Transfer Learning<TITLE>Detecting Negative Transfer in Transductive Transfer Learning,<TITLE>Detecting Noise in Transfer Learning for Cross-Lingual Opinion Analysis of Negative Transfers: A Case Study on N<TITLE>Detecting Noise in Transfer Learning for Cross-Lingual Opinion Analysis of Negative Transfers with Monotonic Increase in<TITLE>Detecting Noise in Transfer Learning for Cross-Lingual Opinion Analysis of Negative Transfers: A Monotonic Approach<TITLE>Detecting Noise in Transfer Learning for Cross-Lingual Opinion Analysis of Negative Transfers with Monotonic Class Noise<TITLE>Detecting Noise in Transfer Learning for Cross-Lingual Opinion Analysis of Negative Transfers: A Monotonic Increase,<TITLE>Identifying Class Noise in Transductive Transfer Learning for Cross-lingual Opinion Analysis<TITLE>Identifying Noise in Transfer Learning for Cross-lingual Opinion Analysis Using Negative Transfer Detection<TITLE>Identifying Class Noise in Transductive Transfer Learning for Cross-lingual Opinion Analysis<TITLE>Identifying Class Noise in Transfer Learning for Cross-lingual Opinion Analysis<TITLE>Identifying Class Noise in Transfer Learning for Cross-lingual Opinion Analysis,<TITLE>Negative Class Noise Detection in Transductive Transfer Learning<TITLE>Negative Class Noise Detection for Transductive Transfer Learning<TITLE>Detecting Negative Class Noise in Transductive Transfer Learning<TITLE>Detecting Negative Transfers in Transductive Transfer Learning<TITLE>Negative Class Noise in Transductive Transfer Learning,<TITLE> Transductive Transfer Learning for Cross-Lingual Opinion Analysis<TITLE> Transductive Transfer Learning for Cross-lingual Opinion Analysis<TITLE> Transductive Transfer Learning for Cross-Linguistic Opinion Analysis<TITLE> Transductive Transfer Learning for Opinion Analysis<TITLE> Neural Transductive Transfer Learning for Opinion Analysis,<TITLE>Class Noise Detection in Transductive Transfer Learning for Cross-lingual Opinion Analysis<TITLE>Class Noise Detection for Transductive Transfer Learning in Cross-lingual Opinion Analysis<TITLE>Transductive Transfer Learning for Cross-Lingual Opinion Analysis<TITLE>Transductive Transfer Learning for Cross-lingual Opinion Analysis<TITLE>Class Noise Detection for Transductive Transfer Learning
56,A Framework for Entailed Relation Recognition,"We define the problem of recognizing entailed relations – given an open set of relations, find all occurrences of the relations of interest in a given document set – and pose it as a challenge to scalable information extraction and retrieval. Existing approaches to relation recognition do not address well problems with an open set of relations and a need for high recall: supervised methods are not easily scaled, while unsupervised and semi-supervised methods address a limited aspect of the problem, as they are restricted to frequent, explicit, highly localized patterns. We argue that textual entailment (TE) is necessary to solve such problems, propose a scalable TE architecture, and provide preliminary results on an Entailed Relation Recognition task.",<TITLE>Relation Recognition with Textual Entailment: A Scalable Approach<TITLE>Recognizing Entailed Relations with Textual Entailment<TITLE>Recognizing Entailed Relations using Textual Entailment<TITLE>Textual Entailment for Relation Recognition<TITLE>Relation Recognition with Textual Entailment,<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Methods. A Scalable Architecture<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Learning. A Scalable Architecture<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Methods. A Scalable Information<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Methods. A Scalable Approach<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Learning. A Scalable Approach,<TITLE>Textual Entailment for Relation Recognition: A Scalable Approach<TITLE>Textual Entailment for Relation Recognition<TITLE>Textual Entailment for Relation Recognition<TITLE>Textual Entailment for Relation Recognition<TITLE>Textual Entailment for Relation Recognition,<TITLE>Adaptive Textual Entailment for Entailed Relation Recognition<TITLE>Resolving Entailed Relation Recognition with Textual Entailment<TITLE>Adapting Textual Entailment for Entailed Relation Recognition<TITLE>Resolving Entailed Relation Recognition with Textual Entailments<TITLE>Adaptive Textual Entailment for Entailed Relations,<TITLE> Towards Scalable Information Extraction and Retrieval<TITLE> An Entailed Relation Recognition Architecture for Textual Entailment<TITLE> Towards Scalable Information Extraction and Retrieval from Texts<TITLE> An Entailed Relation Recognition Architecture<TITLE> Towards Scalable Information Extraction and Retrieval from Textual Relations,<TITLE>Textual Entailment for Open-Set Relation Recognition<TITLE>Entailed Relation Recognition with Textual Entailment<TITLE>Entailed Relation Recognition via Textual Entailment<TITLE>Textual Entailment for Open Relation Recognition<TITLE>Textual Entailment for Relation Recognition
57,Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation,"Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. One of the big challenges with this task is the lack of sufficient annotated data. While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction. Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences. Experimental results confirm that our method alleviates the data scarcity problem significantly. It also effectively boosts the performances of several current models for aspect term extraction.",<TITLE>Conditional Augmentation of Aspect Term Extraction as a Conditional Generation Task<TITLE>Conditional Augmentation of Aspect Term Extraction via Sequence toSequence Method<TITLE>Conditional Augmentation of Aspect Term Extraction as Conditional Generation.<TITLE>Conditional Augmentation of Aspect Term Extraction as Conditional Generation<TITLE>Conditional Augmentation of Aspect Term Extraction from Texts,<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentiment Analysis<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentence Generation<TITLE>Conditional Augmentation of Aspect Term Extraction with Masked Sequence-to-Sequence Method for Sentiment Analysis and<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentiment Generation<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentiment Target,<TITLE>Conditional Augmentation of Aspect Term Extraction via Conditional Generation<TITLE>Conditional Augmentation of Aspect Term Extraction with Conditional Generation<TITLE>Conditional Augmentation for Aspect Term Extraction with Conditional Generation<TITLE>Conditional Augmentation of Aspect Term Extraction via Conditional Generation<TITLE>Conditional Augmentation for Aspect Term Extraction,<TITLE>Aspect term Extraction with Conditional Error<TITLE>Aspect term Extraction with Conditional Boosting<TITLE>Conditional Extraction of Aspect term Extraction<TITLE>Aspect term Extraction with Conditional augmentation<TITLE>Conditional Extraction of Aspect Terminology,<TITLE> Conditional Augmentation for Aspect Term Extraction<TITLE> Conditional Augmentation of Aspect Term Extraction<TITLE> Data Augmentation for Aspect Term Extraction<TITLE> Augmenting Aspect Term Extraction with Conditional Generation<TITLE> Modeling Aspect Term Extraction as Conditional Generation,<TITLE>Conditional Aspect Term Extraction via Masked Sequence-to-Sequence Generation<TITLE>Masked Sequence-to-Sequence Aspect Term Extraction<TITLE>Conditional Data Augmentation for Aspect Term Extraction<TITLE>Conditional Augmentation of Aspect Term Extraction<TITLE>Aspect Term Extraction as Conditional Generation
58,Dependency Parsing of Japanese Spoken Monologue Based on Clause Boundaries,"Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences.",<TITLE>Dependency Parsing of Japanese Monologues Based on Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologue Sentences using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues Using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues with Sentence Segmentation,<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependency Parser<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependencies over Language<TITLE>Dependency Parsing of Japanese Monologues Using Sentence Segmentation and Stochastic Dependency Parser for<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependency Parsings<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependency Parsers,<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologues<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologue Sentences<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologues<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologue Sentences<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologues,<TITLE>Dependency Parsing of Japanese Monologues with Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues via Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues<TITLE>Dependency Parsing for Japanese Monologues,<TITLE> Dependency Parsing of Japanese Monologues Using Sentence Segmentation<TITLE> Dependency Parsing of Japanese Monologues with Sentence Segmentation<TITLE> Japanese Dependency Parsing with Sentence Segmentation<TITLE> Dependency Parsing of Japanese Monologues<TITLE> Dependency Parsing of Japanese Monologues Using Parse Segmentation,<TITLE>Dependency Parsing of Japanese Spoken Monologues Based on Sentence Segmentation<TITLE>Dependency Parsing of Japanese Spoken Monologues based on Sentence Segmentation<TITLE>Dependency Parsing of Japanese Spoken Monologues Using Sentence Segmentation<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Spoken Monologues<TITLE>Dependency Parsing of Spoken Monologues Based on Sentence Segmentation
59,Named Entity Disambiguation for Noisy Text,"We address the task of Named Entity Disambiguation (NED) for noisy text. We present WikilinksNED, a large-scale NED dataset of text fragments from the web, which is significantly noisier and more challenging than existing news-based datasets. To capture the limited and noisy local context surrounding each mention, we design a neural model and train it with a novel method for sampling informative negative examples. We also describe a new way of initializing word and entity embeddings that significantly improves performance. Our model significantly outperforms existing state-of-the-art methods on WikilinksNED while achieving comparable performance on a smaller newswire dataset.",<TITLE>A Large-Scale Named Entity Disambiguation Dataset for Noisy Text<TITLE>A Large-Scale Named Entity Disambiguation Dataset of Noisy Text<TITLE>A Large-Scale Named Entity Disambiguation Dataset from the Web<TITLE>Unsupervised Named Entity Disambiguation for Noisy Text<TITLE>Neural Named Entity Disambiguation for Noisy Text,<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-Based Neural Model for Extracting Informative Examples<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-based Neural Model for Extracting Informative Examples<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-based Approach to Extracting Information from the Web<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-Based Approach to Extracting Informative Examples from<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-based Approach to Extracting Informative Examples from,<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Noisy Text<TITLE>WikilinksNED: Noisy Named Entity Disambiguation with Local Context<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Local Context<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Local Context<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Local Context,<TITLE>Named Entity Disambiguation for Noise-Free Text<TITLE>Named Entity Disambiguation for Noise-based Text<TITLE>Named Entity Disambiguation for Neural Texts<TITLE>Named Entity Disambiguation for Neural Text<TITLE>Named Entity Disambiguation for Noiseful Text,<TITLE> Unsupervised Named Entity Disambiguation for Noisy Text<TITLE> Unsupervised Named Entity Disambiguation Using Word and Entity Embeddings<TITLE> Unsupervised Named Entity Disambiguation for Unseen Text<TITLE> Unsupervised Named Entity Disambiguation Using Word Embeddings<TITLE> Unsupervised Named Entity Disambiguation for Text,<TITLE>WikilinksNED: A Large-Scale Named Entity Disambiguation Dataset<TITLE>Named Entity Disambiguation for Noisy Text: WikilinksNED<TITLE>WikilinksNED: Named Entity Disambiguation for Noisy Text<TITLE>Named Entity Disambiguation for Noisy Text: WikiLinksNED<TITLE>Named Entity Disambiguation for Noisy Text
60,Global Encoding for Abstractive Summarization,"In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition.",<TITLE>A Global Encoding Framework for Neural Abstractive Summarization<TITLE>A Global Encoding Framework for Abstractive Summarization<TITLE>Global Encoding for Neural Abstractive Summarization<TITLE>Global Encoding of Neural Abstractive Summarization<TITLE>Global Encoding for Abstractive Summarization,<TITLE>A Global Encoding Framework for Neural Abstractive Summarization Based on the Source-side Information from Seq2Seq<TITLE>A Global Encoding Framework for Neural Abstractive Summarization Based on the Source-side Information Flow of Seq2Se<TITLE>A Global Encoding Framework for Neural Abstractive Summarization Based on the Source-side Information Flow of Sequence-to-<TITLE>A Global Encoding Framework for Neural Abstractive Summarization Based on the Source-side Information Flow in Seq2Se<TITLE>A Global Encoding Framework for Neural Abstractive Summarization Based on the Source-side Information Flow of Seq2seq,<TITLE>A Global Encoding Framework for Neural Abstractive Summarization<TITLE>Global Encoding for Neural Abstractive Summarization<TITLE>Global Encoding for Neural Abstractive Summarization<TITLE>Global Encoding for Neural Abstractive Summarization<TITLE>Global Encoding for Neural Abstractive Summarization,<TITLE>Global Encoding for Sequence-to-Sequence Summarization<TITLE>Global Encoding for Sequence-to-Sequence summarization<TITLE>Global Encoding for Sequence-to-Sequence Synthesis<TITLE>A Global Encoding Framework for Neural Abstractive Summarization<TITLE>Global Encoding for Neural Abstractive Summarization,<TITLE> Global Encoding for Neural Abstractive Summarization<TITLE> Global Encoding for Abstractive Summarization<TITLE> A Global Encoding Framework for Neural Abstractive Summarization<TITLE> A Global Encoding Framework for Abstractive Summarization<TITLE> Global Representation for Abstractive Summarization,<TITLE>Global Parser for Neural Abstractive Summarization<TITLE>Global Annotation for Neural Abstractive Summarization<TITLE>Neural Abstractive Summarization with Global Annotation<TITLE>Neural Abstractive Summarization with Global Parser<TITLE>Neural Abstractive Summarization with Global Encoding
61,Multitask Parsing Across Semantic Representations,"The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.",<TITLE>Multitask Learning Improves UCCA Parsing<TITLE>Multitask Learning for Improving Semantic Parsing<TITLE>Multitask Learning Improves Semantic Parsing<TITLE>Multitask Learning for UCCA Parsing<TITLE>Multitask Learning improves UCCA Parsing,"<TITLE>Multitask Learning for UCCA Parsing with Transition-based System and Learning for Improved Semantic Parsing Tasks<TITLE>Multitask Learning for UCCA Parsing: A Transition-based System and AMR, SDP and ADU<TITLE>Multitask Learning for UCCA Parsing: A Transition-based System and AMR, SDP and ADUC<TITLE>Multitask Learning for UCCA Parsing: A Transition-based System and AMR, SDP and ADM<TITLE>Multitask Learning for UCCA Parsing: A Transition-based System and AMR, SDP and ADN",<TITLE>Multitask Multitask Learning for Semantic Parsing<TITLE>Multitask Multitask Semantic Parsing<TITLE>Multitask Learning for Semantic Parsing<TITLE>Multitask Learning for Semantic Parsing<TITLE>Multitask Learning for Semantic Parsing,<TITLE>Multitask Learning for Semantic Parsing and Universal Dependency Parsing<TITLE>Multitask Learning for Semantic Parsing with Universal Dependencies and AMR<TITLE>Multitask Learning for Semantic Parsing and Universal Dependencies<TITLE>Multitask Learning for Semantic Parsing with Universal Dependencies<TITLE>Multitask Learning for Semantic Parsing,<TITLE> Improving Semantic Parsing Performance by Multitask Learning<TITLE> Multitask Learning Improves Semantic Parsing<TITLE> Multitask Learning Improves Semantic Parsing Performance<TITLE> Improving Semantic Parsing Performance with Multitask Learning<TITLE> Multitask Learning for Semantic Parsing,<TITLE>Improving UCCA Parsing with Multitask Learning<TITLE>Multitask Learning for UCCA Parsing<TITLE>Multitask Learning for UCCA Parsers<TITLE>Multitask Learning for Semantic Parsing<TITLE>Multitask Learning for UCCA Parser
62,Tense Sense Disambiguation: A New Syntactic Polysemy Task,"Polysemy is a major characteristic of natural languages. Like words, syntactic forms can have several meanings. Understanding the correct meaning of a syntactic form is of great importance to many NLP applications. In this paper we address an important type of syntactic polysemy – the multiple possible senses of tense syntactic forms. We make our discussion concrete by introducing the task of Tense Sense Disambiguation (TSD): given a concrete tense syntactic form present in a sentence, select its appropriate sense among a set of possible senses. Using English grammar textbooks, we compiled a syntactic sense dictionary comprising common tense syntactic forms and semantic senses for each. We annotated thousands of BNC sentences using the defined senses. We describe a supervised TSD algorithm trained on these annotations, which outperforms a strong baseline for the task.",<TITLE>Tense Syntactic Sense Disambiguation: A Case Study on BNC<TITLE>Tense Syntactic Semantics and Tense Sense Disambiguation<TITLE>Tense Syntactic Sense Disambiguation for NLP<TITLE>Tense Syntactic Forms and Semantic Senses<TITLE>Tense Syntactic Sense Disambiguation,<TITLE>Tense Sense Disambiguation: An Important Syntactic Polysemy Dictionary Based on Grammar and Semantic Senses<TITLE>Tense Sense Disambiguation: An Important Syntactic Polysemy Dictionary Based on Grammar Texts and Semantic<TITLE>Tense Sense Disambiguation: An Important Syntactic Polysemy Dictionary Based on Grammar and Semantic Induction<TITLE>Tense Sense Disambiguation: An Important Syntactic Polysemy Dictionary Based on Grammar Texts and Semantics<TITLE>Tense Sense Disambiguation: An Important Syntactic Polysemy Dictionary Based on Grammar and Semantic Information from,<TITLE>Tense Sense Disambiguation: Multiple Senses of Tense Syntactic Forms<TITLE>Tense Sense Disambiguation: Multiple Senses for Tense Syntactic Forms<TITLE>Tense Sense Disambiguation: Multiple Senses of Syntactic Forms<TITLE>Tense Sense Disambiguation: Tense Polysemy in English<TITLE>Tense Sense Disambiguation: A Case Study in English,<TITLE>Tense Sense Disambiguation for Syntactic Forms<TITLE>Tense Sense Disambiguation for Syntactic Forms.<TITLE>Tense Sense Disambiguation: Understanding Syntactic Forms<TITLE>Tense Syntactic Form Disambiguation<TITLE>Tense Sense Disambiguation,<TITLE> Tense Sense Disambiguation for Syntactic Polysemy<TITLE> Tense Sense Disambiguation with Multiple Senses<TITLE> Tense Sense Disambiguation<TITLE> Tense Sense Disambiguation for Syntaxical Polysemy<TITLE> Tense Sense Disambiguation for Syntax-Based Semantic Senses,<TITLE>Tense Sense Disambiguation: A Case Study in English Grammar<TITLE>Tense Sense Disambiguation: A Case Study on English Grammar<TITLE>Tense Sense Disambiguation: A Corpus of Multiple Senses<TITLE>Tense Sense Disambiguation: An Introduction to the Task<TITLE>Tense Sense Disambiguation: An English Grammar Textbook Approach
63,Sentence Compression with Joint Structural Inference,"Sentence compression techniques often assemble output sentences using fragments of lexical sequences such as ngrams or units of syntactic structure such as edges from a dependency tree representation. We present a novel approach for discriminative sentence compression that unifies these notions and jointly produces sequential and syntactic representations for output text, leveraging a compact integer linear programming formulation to maintain structural integrity. Our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-theart approaches. Experiments on corpora featuring human-generated compressions demonstrate a 13-15% relative gain in 4gram accuracy over a well-studied language model-based compression system.",<TITLE>Discriminative Sentence Compression with Compact Integer Programming Formulations<TITLE>Discriminative Sentence Compression with Dependency Tree Representations<TITLE>Discriminative Sentence Compression with Compact Integer Programming Formulas<TITLE>Discriminative Sentence Compression with Compact Integer Programming Formulation<TITLE>Discriminative Sentence Compression with Compact Integer Linear Programming,<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression with Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression using Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression Using Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression via Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression with a Compact Integer,<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representations<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Structures<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representation<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representations<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representation,<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representations<TITLE>Discriminative Sentence Compression with Integer Linear Programming for Text<TITLE>Discriminative Sentence Compression with a Compact Integer Linear Program<TITLE>Discriminative Sentence Compression with Integer Linear Programming<TITLE>Discriminative Sentence Compression with Integer Linear programming,<TITLE> Discriminative Sentence Compression with Compact Integer Linear Programming<TITLE> Discriminative Sentence Compression<TITLE> Discriminative Sentence Compression with Structural Integrity<TITLE> Discriminative Sentence Compression with Heterogeneous Structures<TITLE> Discriminative Sentence Compression with Structured Input,<TITLE>Joint Sequential and Syntactic Representations for Discriminative Sentence Compression<TITLE>Jointly Assembling Lexical and Syntactic Structures for Sentence Compression<TITLE>Discriminative Sentence Compression with Joint Syntactic and Sequential Representations<TITLE>Joint Sequential and Syntactic Structures for Discriminative Sentence Compression<TITLE>Joint Sequential and Syntactic Representations for Sentence Compression
64,A Lightweight and High Performance Monolingual Word Aligner,"Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.",<TITLE>A Discriminatively trained Monolingual Word Aligner<TITLE>Discriminatively trained Monolingual Word Aligner<TITLE>Discriminatively Trained Monolingual Word Alignment<TITLE>Fast Monolingual Word Alignment with Conditional Random Fields<TITLE>Discriminatively trained Monolingual Word Alignment,<TITLE>Fast Alignment of Monolingual Word Aligners with Conditional Random Fields and External Resources using Part-of-Spe<TITLE>Fast Alignment of Monolingual Word Aligners with Conditional Random Fields and External Resources for Natural Language Processing Tasks<TITLE>Fast Alignment of Monolingual Word Alignments with Conditional Random Fields and External Resources for Natural Language Processing Tasks<TITLE>Fast Alignment of Monolingual Word Aligners with Conditional Random Fields and External Resources using Part-of-speech<TITLE>Fast Alignment of Monolingual Word Aligners with Conditional Random Fields and External Resources for Natural Language Tagging.,<TITLE>Fast Monolingual Word Alignment with Conditional Random Fields<TITLE>Fast Monolingual Word Alignment with Conditional Random Field<TITLE>Fast Monolingual Word Alignment with Conditional Random Fields<TITLE>Fast Word Alignment with Conditional Random Fields<TITLE>Fast Word Alignment with Conditional Random Fields,<TITLE>Discriminatively trained Monolingual Word Alignment with Conditional Random Field<TITLE>Discriminatively trained monolingual word aligner with Conditional Random Fields<TITLE>Discriminatively trained monolingual word aligner with Conditional Random Field<TITLE>A Discriminatively trained Monolingual Word Aligner<TITLE>Discriminatively trained Monolingual Word Alignment,<TITLE> A Discriminatively-trained Monolingual Word Aligner<TITLE> A Discriminative Aligner for Statistical Machine Translation<TITLE> A Discriminatively-trained Monolingual Aligner<TITLE> A Discriminative Aligner for Multilingual Word Alignment<TITLE> A Discriminative Translation Aligner,<TITLE>Fast Monolingual Word Alignment using Conditional Random Fields.<TITLE>Fast Monolingual Word Alignment using Conditional Random Fields<TITLE>Fast Monolingual Word Alignment with Conditional Random Fields<TITLE>Fast Monolingual Word Alignment Using Conditional Random Fields<TITLE>Fast and Accurate Monolingual Word Alignment
65,XMEANT: Better semantic MT evaluation without reference translations,"We introduce XMEANT—a new cross-lingual version of the semantic frame based MT evaluation metric MEANT—which can correlate even more closely with human adequacy judgments than monolingual MEANT and eliminates the need for expensive human references. Previous work established that MEANT reflects translation adequacy with state-of-the-art accuracy, and optimizing MT systems against MEANT robustly improves translation quality. However, to go beyond tuning weights in the loglinear SMT model, a cross-lingual objective function that can deeply integrate semantic frame criteria into the MT training pipeline is needed. We show that cross-lingual XMEANT outperforms monolingual MEANT by (1) replacing the monolingual context vector model in MEANT with simple translation probabilities, and (2) incorporating bracketing ITG con-",<TITLE>XMEANT: Cross-Lingual Semantic Frame Based MT Evaluation Metric<TITLE>XMEANT: Cross-lingual Semantic Frame Evaluation Metric for Machine Translation<TITLE>XMEANT: Cross-Lingual Semantic Frame Based MT Evaluation Metrics<TITLE>XMEANT: Cross-lingual Semantic Frame Based MT Evaluation Metric<TITLE>XMEANT: Cross-Lingual Semantic Frame Evaluation Metric,<TITLE>XMEANT: A Cross-Lingual MT Evaluation Metric for Semantic Frame-based MT Evaluation with Monoling<TITLE>XMEANT: A Cross-Lingual MT Evaluation Metric for Semantic Frame-Based MT Evaluation with Monoling<TITLE>XMEANT: A Cross-Lingual MT Evaluation Metric for Semantic Frame-based MT Evaluation with Human Reference<TITLE>XMEANT: A Cross-Lingual MT Evaluation Metric for Semantic Frame-Based MT Evaluation with Human Reference<TITLE>XMEANT: A Cross-Lingual MT Evaluation Metric for SMT with Monolingual and Monolingually,<TITLE>XMEANT: Cross-lingual MT Evaluation Metric Based on Semantic Frame Criteria<TITLE>XMEANT: Cross-lingual MT Evaluation Metric for Semantic Frame Based MT<TITLE>XMEANT: Cross-lingual MT Evaluation Metric Based on Semantic Frame Information<TITLE>XMEANT: Cross-lingual MT Evaluation Metric for Semantic Frame Based MT<TITLE>XMEANT: Cross-lingual MT Evaluation Metric Based on Semantic Frames,<TITLE>Cross-lingual XMEANT: A Cross-Lingual Version of the Semantic Frame based Evaluation Metric<TITLE>Cross-lingual XMEANT: A Cross-Lingual Version of Semantic Frame based MT Evaluation<TITLE>XMEANT: Cross-Lingual Semantic Frame Evaluation Metrics<TITLE>XMEANT: Cross-Lingual Semantic Frame Evaluation Metric<TITLE>XMEANT: Cross-lingual Semantic Frame Evaluation Metric,<TITLE> Cross-Lingual Evaluation Metric Meant<TITLE> Cross-lingual Evaluation Metric Meant<TITLE> XMEANT: A Cross-lingual Evaluation Metric for Statistical Machine Translation<TITLE> Cross-lingual Evaluation Metric Meant: A Cross-lingual Evaluation Metric<TITLE> Cross-Lingual Evaluation Metric Meant: A Cross-Lingual Approach,<TITLE>Cross-lingual Meant for Machine Translation Evaluation<TITLE>Cross-lingual Meant for Statistical Machine Translation<TITLE>Cross-lingual Meant Evaluation for Machine Translation<TITLE>Cross-lingual Meant for Machine Translation<TITLE>Cross-lingual Evaluation of Machine Translation
66,BERT Rediscovers the Classical NLP Pipeline,"Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",<TITLE>BERT: An Interpretable and Localized Model for Pre-trained Neural Language Processing<TITLE>BERT: An Interpretable and Localized Model for Pre-trained Neural Language Models<TITLE>BERT: An Interpretable and Localized Model for Pre-trained Neural Language Understanding<TITLE>BERT: An Interpretable and Localized Model for Pre-trained Neural Network<TITLE>BERT: An Interpretable and Localized Model for Neural Machine Translation,<TITLE>BERT: Quantifying the Role of Linguistic Information in Pre-trained Text Encoders for Coreference and POS<TITLE>BERT: Quantifying the Role of Linguistic Information in Pre-trained Text Encoders for Coreference Tagging<TITLE>BERT: Quantifying the Role of Linguistic Information in Pre-Trained Text Encoders for Coreference and<TITLE>BERT: Quantifying the Role of Linguistic Information in Pre-trained Text Encoders for Coreference and T<TITLE>BERT: Quantifying the Role of Linguistic Information in Pre-Trained Text Encoders for Coreference Resolution,<TITLE>BERT: An Interpretable and Localizable Model of Linguistic Information in Pre-trained Text Representations<TITLE>Where is the Language in BERT? An Interpretable and Localizable Analysis of a Neural Network for NLP<TITLE>BERT: An Interpretable and Localizable Model of Linguistic Information in Text Encoders<TITLE>Where is the Language in BERT? An Interpretable and Localizable Model of NLP Tasks<TITLE>BERT: An Interpretable and Localizable Model of the NLP Pipeline,<TITLE>BERT: An Interpretable and Localizable Model for Pre-Trained Text Encoding<TITLE>BERT: An Interpretable and Localizable Model for Pre-trained Text Encoding<TITLE>BERT: An Interpretable Model for Pre-Trained Text Encoders<TITLE>A BERT Model for Pre-Trained Text Encoders<TITLE>A BERT Model for Pre-trained Text Encoders,<TITLE> Bert: A Localizable Model for Text Representation<TITLE> Bert: A Localizable Model for Text Encoding<TITLE> Bridging the Gap Between Language and Representation: An Interpretable Approach<TITLE> Bridging the Gap Between Language and Representation<TITLE> Bridging the Gap Between Language and Representation: Towards a Localizable Framework,<TITLE>BERT: Where and When Are We Near the End of NLP?<TITLE>BERT: Where is the Transliteration in the Network?<TITLE>BERT: Where and When Are We Near the End?<TITLE>BERT: Where is the Transliteration Pipeline?<TITLE>BERT Pipelines: Where and When Are They?
67,Sequence Generation with Optimal-Transport-Enhanced Reinforcement Learning,"Reinforcement learning (RL) has been widely used to aid training in language generation. This is achieved by enhancing standard maximum likelihood objectives with userspecified reward functions that encourage global semantic consistency. We propose a principled approach to address the difficulties associated with RL-based solutions, namely, highvariance gradients, uninformative rewards and brittle training. By leveraging the optimal transport distance, we introduce a regularizer that significantly alleviates the above issues. Our formulation emphasizes the preservation of semantic features, enabling end-to-end training instead of ad-hoc fine-tuning, and when combined with RL, it controls the exploration space for more efficient model updates. To validate the effectiveness of the proposed solution, we perform a comprehensive evaluation covering a wide variety of NLP tasks: machine translation, abstractive text summarization and image caption, with consistent improvements over competing",<TITLE>End-to-End Reinforcement Learning with Global Semantic Consistency<TITLE>Continuous Reinforcement Learning with Global Semantic Consistency<TITLE>End-to-End Reinforcement Learning for Neural Language Generation<TITLE>End-to-End Reinforcement Learning for Language Generation<TITLE>End-to-End Reinforcement Learning for NLP,<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Tasks: A Principled Approach<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Models: An Application to Textual<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Models: A Principled Approach.<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Models: A Principled Approach to<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Tasks: A Principled Perspective,<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Consistency<TITLE>End-to-End Reinforcement Learning for Language Generation with Userspecified Reward Functions<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Regularization<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Consistency<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Regularization,<TITLE>A Regularizer for Reinforcement Learning in Language Generation<TITLE>Improved Reinforcement Learning for Language Generation<TITLE>Improved Reinforcement Learning in Language Generation<TITLE>A Regularizer for Reinforcement Learning<TITLE>Reinforcement Learning for Language Generation,<TITLE> Efficient Reinforcement Learning for Language Generation<TITLE> Highvariance Gradients for Reinforcement Learning<TITLE> Efficient Reinforcement Learning for Natural Language Generation<TITLE> Learning to Reinforce Language Generation<TITLE> Efficient Reinforcement Learning for Neural Language Generation,<TITLE>Reinforcement Learning for Natural Language Processing with Optimal Transport<TITLE>Reinforcement Learning with Optimal Transport for Natural Language Processing<TITLE>Reinforcement Learning with Optimal Transport for Neural Language Generation<TITLE>Reinforcement Learning with Optimal Transport for Natural Language Generation<TITLE>Reinforcement Learning for Language Generation with Optimal Transport
68,Lexicon-Based Methods for Sentiment Analysis,"We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation. SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text’s opinion towards its main subject matter. We show that SO-CAL’s performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.",<TITLE>Semantic Orientation CALculator: A Lexicon-Based Approach to Extracting Sentiment from Text<TITLE>Semantic Orientation CALculator: A Lexicon-based Approach to Extracting Sentiment from Text<TITLE>Semantic Orientation Calculator: A Lexicon-Based Approach to Extracting Sentiment from Text<TITLE>Semantic Orientation CALculator: A Lexicon-Based Approach to Sentiment Extraction from Text<TITLE>Semantic Orientation CALculator: A Lexicon-based Approach to Sentiment Extraction from Text,<TITLE>Automatic Sentiment Extraction from Text with Semantic Orientation Calculator (SO-CAL) and Lexicon<TITLE>Automatic Sentiment Extraction from Text with Semantic Orientation Calculator (SO-CAL) Using Diction<TITLE>Automatic Sentiment Extraction from Text with Semantic Orientation Calculator (SO-CAL): A Case Study<TITLE>Automatic Sentiment Extraction from Text with Semantic Orientation Calculator and Lexicon-Based Polarity Classification.<TITLE>Automatic Sentiment Extraction from Text with Semantic Orientation Calculator (SO-CAL) and Lexical,<TITLE>Semantic Orientation Calculator: A Lexicon-Based Approach to Sentiment Analysis<TITLE>Semantic Orientation Calculator: A Lexicon-based Approach to Sentiment Analysis<TITLE>Semantic Orientation Calculator: A Lexicon-Based Approach to Sentiment Classification<TITLE>A Lexicon-Based Approach to Sentiment Extraction from Text<TITLE>Semantic Orientation Calculation for Sentiment Classification,<TITLE>The Semantic Orientation Calculator for Sentiment Extraction<TITLE>The Semantic Orientation Calculator for Sentiment Extracting<TITLE>The Semantic Orientation Calculator for Text Extraction<TITLE>Semantic Orientation Calculator for Text Extraction<TITLE>A Lexical Approach to Extracting Sentiment from Text,<TITLE> A Lexicon-Based Approach to Sentiment Extraction from Text<TITLE> A Lexicon-Based Approach to Sentiment Extraction<TITLE> Extracting Sentiment from Text<TITLE> Extracting Sentiment from Text using Semantic Orientation Calculus<TITLE> Extracting Sentiment from Text Using Semantic Orientation Calculus,<TITLE>Lexicon-based Sentiment Analysis using the Semantic Orientation of Words<TITLE>Lexicon-based Sentiment Analysis using the Semantic Orientation Calculus<TITLE>Lexicon-Based Polarity Classification with Semantic Orientation<TITLE>Lexicon-Based Sentiment Extraction from Text<TITLE>Lexicon-Based Polarity Classification
69,Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization,"Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation. We present HYPERBAND, a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound. HYPERBAND is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations. We compare HYPERBAND with popular Bayesian Optimization methods on several hyperparameter optimization problems. We observe that HYPERBAND can provide more than an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems.",<TITLE>HyPERBAND: Adaptive Resource Allocation for Dynamic Hyperparameter Optimization<TITLE>HyPERBAND: Adaptive Resource Allocation for Neural Hyperparameter Optimization<TITLE>HyPERBAND: Adaptive Resource Allocation for Hyperparameter Optimization.<TITLE>HyPERBAND: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>HyPERBAND: Adaptive Resource Allocation for Neural Network and Kernel Learning,<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stopp<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stopping<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation with Early-Stoppping Methods.<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stop<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stopped,<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization.<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>Adaptive Resource Allocation for Hyperparameter Optimization.<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization,<TITLE>HYPERBAND: Accelerating Random Search via Bayesian Optimization<TITLE>HYPERBAND: Accelerating Random Search through Bayesian Optimization<TITLE>HYPERBAND: A Bayesian Optimization Approach to Random Search<TITLE>HYPERBAND: A Bayesian Optimization Approach<TITLE>Bayesian Optimization for Hyperparameter Optimization,<TITLE> HyPERBAND: Adaptive Random Search for Hyperparameter Optimization<TITLE> HyPERBAND: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE> HyPERBAND: Adaptive Random Search via Hyperparameter Optimization<TITLE> HyPERBAND: Adaptive Random Search via Adaptive Resource Allocation<TITLE> HyPERBAND: Adaptive Random Search Through Adaptive Resource Allocation,<TITLE>Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>Adaptive Resource Allocation for Hyperparameter Learning<TITLE>Adaptive Resource Allocation for Hyperparameters<TITLE>Adaptive Resource Allocation for Hyperparameter optimization<TITLE>Fast Bayesian Hyperparameter Optimization
70,Non-Negative Matrix Factorization Clustering on Multiple Manifolds,"Nonnegative Matrix Factorization (NMF) is a widely used technique in many applications such as clustering. It approximates the nonnegative data in an original high dimensional space with a linear representation in a low dimensional space by using the product of two nonnegative matrices. In many applications with data such as human faces or digits, data often reside on multiple manifolds, which may overlap or intersect. But the traditional NMF method and other existing variants of NMF do not consider this. This paper proposes a novel clustering algorithm that explicitly models the intrinsic geometrical structure of the data on multiple manifolds with NMF. The idea of the proposed algorithm is that a data point generated by several neighboring points on a specific manifold in the original space should be constructed in a similar way in the low dimensional subspace. A set of experimental results on two real world datasets demonstrate the advantage of the proposed algorithm.",<TITLE>Multi-Manifold Nonnegative Matrix Factorization.<TITLE>Multi-Manifold Nonnegative Matrix Factorization<TITLE>Multiple Manifolds with Nonnegative Matrix Factorization<TITLE>Nonnegative Matrix Factorization for Multiple Manifolds<TITLE>Multiple Manifold Nonnegative Matrix Factorization,<TITLE>Nonnegative Matrix Factorization on Manifolds with Intrinsic Geometrical Structure for Multiple Manifold Manif<TITLE>Nonnegative Matrix Factorization on Manifolds with Intrinsic Geometrical Structure for Nonnegative Manifold Cl<TITLE>Nonnegative Matrix Factorization on Manifolds with Intrinsic Geometrical Structure for Multiple Manifold Clust<TITLE>Nonnegative Matrix Factorization on Manifolds with Intrinsic Geometrical Structure for Nonnegative Manifold Represent<TITLE>Nonnegative Matrix Factorization on Manifolds with Intrinsic Geometrical Structures for Clustering Data.,<TITLE>Clustering on Multiple Manifolds with Nonnegative Matrix Factorization<TITLE>Clustering with Nonnegative Matrix Factorization on Multiple Manifolds<TITLE>Clustering on Multiple Manifolds with Nonnegative Matrix Factorization.<TITLE>Clustering on Multiple Manifolds with Nonnegative Matrix Factorizations<TITLE>Clustering on Multiple Manifolds with Nonnegative Matrix Factorization,<TITLE>Clustering Nonnegative Matrix Factorization for High Dimensional Space<TITLE>Neural Nonnegative Matrix Factorization for Clustering<TITLE>Neural Nonnegative Matrix Factorization<TITLE>Clustering Nonnegative Matrix Factorization<TITLE>Nonnegative Matrix Factorization,<TITLE> A Clustering Algorithm for Nonnegative Matrix Factorization<TITLE> A Clustering Algorithm for Multiple Mixtures of Nonnegative Matrices<TITLE> A Clustering Algorithm for Multiple Manifolds<TITLE> A Clustering Algorithm for Multi-Horned Nonnegative Matrix Factorization<TITLE> A Clustering Algorithm for Multiple Mixtures of Nonnegative Data,<TITLE>Clustering on Multiple Manifolds with Nonnegative Matrix Factorization<TITLE>Nonnegative Matrix Factorization on Multiple Manifolds for Clustering<TITLE>Nonnegative Matrix Factorization with Multiple Manifolds for Clustering<TITLE>Nonnegative Matrix Factorization on Multiple Manifolds<TITLE>Nonnegative Matrix Factorization for Multiple Manifolds
71,Immediate-Head Parsing for Language Models,"We present two language models based upon an “immediate-head” parser — our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammarbased language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediatehead language models.",<TITLE>Improvements in an Immediate-Head Language Model Based on a Grammatical Parser<TITLE>Improvements in an Immediate-Head Language Model Based on a Grammatical Language Model<TITLE>Improvements in an Immediate-Head Language Model Based on a Constituent C<TITLE>An Immediate-Head Parser for Grammatical Language Modeling<TITLE>An Immediate-Head Parser for Grammatical Language Models,<TITLE>Parsing with an Immediate-Head Parser and its Application to the trigram base-line Grammatical language model<TITLE>Parsing with an Immediate-Head Parser and its Application to the trigram base-line Grammatical Language Model<TITLE>Parsing with an Immediate-Head Parser and its Application to the trigram base-line grammatical language model<TITLE>Parsing with an Immediate-Head Parser and its Application to the Tragram Model Base-line Grammatical<TITLE>Parsing with an Immediate-Head Parser and its Application to the Tragram Model Base-line Grammar,<TITLE>An Immediate-Head Parser for Statistical Language Models<TITLE>An Immediate-Head Parser for Language Modeling<TITLE>Immediate-Head Language Models for Statistical Parsing<TITLE>An Immediate-Head Parser for Language Models<TITLE>Immediate-Head Language Models for Statistical Parsing,<TITLE>An “immediate-head” parser for immediate-head language models<TITLE>An “immediate-head” parser for grammatical language<TITLE>An “immediate-head” parser for immediate-head language<TITLE>An “immediate-head” parser<TITLE>An “immediate-head” Parser,<TITLE> Improving Grammatical Language Modeling by Immediate-Heading Parsing<TITLE> Improving Grammatical Language Models with Head-Driven Parsing<TITLE> Improving Grammatical Language Models with Initial-Head Language Models<TITLE> Improving Grammatical Language Modeling by Immediate-Heading Parser<TITLE> Improving Grammatical Language Models by Incorporating First-Order Logic,<TITLE>Two Immediate-Head Grammatical Language Models<TITLE>Improving Immediate-Head Grammatical Language Models<TITLE>Improved Immediate-Head Grammatical Language Models<TITLE>Immediate-Head Grammatical Language Models<TITLE>Immediate-Head Grammatical Language Modeling
72,A Hybrid Hierarchical Model for Multi-Document Summarization,"Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.",<TITLE>A Two Step Learning Approach to Extractive Summarization with Latent Attributes<TITLE>Towards Extractive Summarization as a Two Step Learning Problem<TITLE>A Two-Step Learning Approach to Extractive Summarization<TITLE>A Two Step Learning Approach to Extractive Summarization<TITLE>Two Step Learning for Extractive Summarization,<TITLE>A Two-Step Learning Approach to Extractive Summarization Using Hierarchical Topic Models and Lexical and Structural Information<TITLE>A Two-Step Learning Approach to Extractive Summarization Using Hierarchical Topic Models and Lexical and Structural Features<TITLE>A Two-Step Learning Approach to Extractive Summarization Based on Hierarchical Topic Modeling for Document Clustering<TITLE>A Two-Step Learning Approach to Extractive Summarization Using Hierarchical Topic Modeling and Lexical and Structural<TITLE>A Two-Step Learning Approach to Extractive Summarization Using Hierarchical Topic Models and Lexical and Semantic Information,<TITLE>Extractive Multi-Document Summarization as a Two-Step Learning Problem<TITLE>Extractive Multi-Document Summarization as a Two-Step Learning Approach<TITLE>Extractive Multi-Document Summarization as a Two Step Learning Problem<TITLE>Extractive Multi-Document Summarization as a Two Step Learning Approach<TITLE>Learning to Generate Summaries from Document Clusters,<TITLE>Extractive Multi-Document Summarization with Hierarchical Topic Model<TITLE>Generative Multi-Document Summarization for Document Clusters<TITLE>Generative Multi-Document Summarization<TITLE>Extractive Multi-Document Summarization<TITLE>Generative Multi-document Summarization,<TITLE> Extractive Summarization as a Two Step Learning Problem<TITLE> Extractive Multi-Document Summarization with Latent Attributes<TITLE> Extractive Multi-Document Summarization with Latent Characteristics<TITLE> Extractive Multi-Document Summarization with Latent Features<TITLE> Extractive Multi-Document Summarization with Latent Structures,<TITLE>Extractive Multi-Document Summarization Using Latent and Lexical Features<TITLE>Extractive Multi-Document Summarization with Latent and Lexical Features<TITLE>Extractive Multi-Document Summarization Using Hierarchical Topic Models and Regression<TITLE>Learning to Score Sentences for Extractive Multi-Document Summarization<TITLE>Extractive Multi-Document Summarization Using Hierarchical Topic Models
73,Generating Natural Language Summaries from Multiple On-Line Sources,"We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.",<TITLE>Summarization of Current Events in the Form of Briefings<TITLE>SummONS: Summarizing Current Events with Background Information<TITLE>SummONS: Summarizing Current Events with Historical Information<TITLE>Summarization of Current Events in the Form of Briefing<TITLE>Summarization of Current Events with Background Information,<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background and Multi-Document Summaries<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background and Multi-Document Summarization<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background Descriptions of Entities and Cont<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background Descriptions of Entities and Conf<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background Descriptions of Entities and General,<TITLE>Summons: A System for Historical Summarization of News Briefings<TITLE>Summons: A System for Historical Summarization of News and Current Events<TITLE>Summons: A System for Historical Summarization of News Briefing<TITLE>Summons: A System for Historical Summarization of News Briefings<TITLE>Summons: A System for Historical Summarization of News and Events,<TITLE>SUMMONS: A System for Summarization of News about Current Events<TITLE>SUMMONS: Synchronous Summarization of News about Current Events<TITLE>SUMMONS: A System for Summarization of Information about Current Events<TITLE>SUMMONS: Synchronous Summarization of Information about Events<TITLE>SUMMONS: A System for Summarization of Information about Events,<TITLE> Summarization of News About Current Events<TITLE> Summarization of News About Current Events with Background Information<TITLE> Summarization of News with Background Information<TITLE> Summarization of News about Current Events with Background Information<TITLE> Summarization of News About Current Events Using Briefing,<TITLE>SUMMONS: A Methodology for Semantic Summarization of News about Events<TITLE>SUMMONS: A Methodology for Summarizing News about Events in History<TITLE>SUMMONS: A Methodology for Summarization of News about Events<TITLE>SUMMONS: A Methodology for News Summarization of Historical Documents<TITLE>SUMMONS: A Methodology for Semantic Summarization of Historical Documents
74,Positional Encoding to Control Output Sequence Length,"Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider an additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder model preserves the length constraint. Unlike previous studies that learn length embeddings, the proposed method can generate a text of any length even if the target length is unseen in training data. The experimental results show that the proposed method is able not only to control generation length but also improve ROUGE scores.",<TITLE>A Simple but Effective Extension of a sinusoidal positional encoding for Abstractive Summarization<TITLE>A Simple but Effective Extension of a Sinusoidal positional encoding for Abstractive Summarization<TITLE>A Simple but Effective Extension of a sinusoidal positional encoding model for abstractive summarization<TITLE>A Simple but Effective Extension of a sinusoidal positional encoding for abstractive summarization<TITLE>Learning length embeddings for abstractive summarization with sinusoidal positional encoding.,<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal Position<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal Pos<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization with Length Constraints.<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal POS<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal positional,<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Encoder-Decoder Models<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Encoder-Decoder Model<TITLE>Learning to Generate Text of Any Length with a Sinusoidal Positioning Encoder-Decoder<TITLE>Learning to Generate Text of Any Length with a Sinusoidal Positioning Encoding<TITLE>Learning to Generate Text of Any Length with a Sinusoidal Positioning Encoding,<TITLE>Sinusoidal Positional Encoding for Neural Language Generation<TITLE>Sinusoidal Positional Encoding for Neural Language Generating<TITLE>A Sinusoidal Positional Encoding for Neural Language Generation<TITLE>Sinusoidal Positional Encoding for Neural Language Generation<TITLE>A Sinusoidal Positional Encoding for Natural Language Generation,<TITLE> A Sinusoidal Neural Encoder-Decoder Model for Abstractive Summarization<TITLE> Learning Length Constraints for Abstractive Summarization<TITLE> A Sinusoidal Perceptron for Abstractive Summarization<TITLE> Simplifying Abstractive Summarization with Sinusoidal Parameter Encoders<TITLE> Simplifying Abstractive Summarization with Sinusoidal Position Representations,<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Decoding<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Networks<TITLE>Learning Length Embeddings for Abstractive Summarization<TITLE>Learning Length Embeddings for Neural Stream Summarization<TITLE>Learning Length Embeddings for Neural Summarization
75,Pre-training of Recurrent Neural Networks via Linear Autoencoders,"We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the inputto-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.",<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-Training for Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoders<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoders.<TITLE>Pre-Training for Recurrent Neural Networks with Linear Autoencoders,<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and a<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and an<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and Applications<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and Application<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and Hidden,<TITLE>Pre-training of Linear Autoencoder Networks for Recurrent Neural Networks<TITLE>Pre-training of Linear Autoencoders for Recurrent Neural Networks<TITLE>Pre-training of Linear Autoencoders for Recurrent Neural Networks<TITLE>Pre-training of Linear Autoencoder Networks for Sequences<TITLE>Pre-training of Linear Autoencoders for Recurrent Neural Networks,<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-training for Recurrent Neural Networks using Linear Autoencoder Networks<TITLE>Pre-training Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-Training for Recurrent Neural Networks<TITLE>Pre-training for Recurrent Neural Networks,<TITLE> Training Recurrent Neural Networks with Linear Autoencoders<TITLE> Recurrent Neural Networks with Linear Autoencoders<TITLE> Recurrent Neural Networks for Polyphonic Music<TITLE> Training Recurrent Neural Networks with Linear Autoencoder Networks<TITLE> Pre-Training Recurrent Neural Networks for Polyphonic Music,<TITLE>Pre-training of Linear Autoencoder Networks for Sequences<TITLE>Pre-training of Linear Autoencoder Networks for Sequence Learning<TITLE>Pre-training Linear Autoencoders for Recurrent Neural Networks<TITLE>Pre-training Linear Autoencoder Networks for Sequences<TITLE>Pre-training Linear Autoencoder Networks for Sequence Learning
76,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the ""language agnostic"" status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",<TITLE>Language Agnostic: A quantitative study of the impact of language technologies on NLP<TITLE>Language Agnostic: A quantitative study of the impact of linguistic technologies on NLP<TITLE>Language Agnostic: A quantitative study of the impact of languages on NLP conferences<TITLE>Language Agnostic: A quantitative study of the impact of languages on NLP<TITLE>Language Agnostic: A Case Study in the ACL,"<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts.<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts (<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts over<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts with<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts -",<TITLE>The State of the Art of Multilingualism in NLP: A Quantitative Analysis<TITLE>The State of the Art of Multilingualism in NLP: A Quantitative Investigation<TITLE>The State of the Art of Multilingualism in NLP: A quantitative analysis<TITLE>The State of the Art of Multilingualism in the World of NLP<TITLE>The State of the Art of Multilingualism in NLP,"<TITLE>Understanding the Relation between Languages, Resources, and Their Representation in NLP Conférences<TITLE>Understanding the Relation between Languages, Resources, and Their Representation in NLP Conferences<TITLE>Understanding the Relation between Languages, Resources, and Their Representations in NLP Conferences<TITLE>The Relation between Languages, Resources, and Their Representation in NLP Conferences<TITLE>Understanding the Relation between Languages and Resource Representations in NLP Conferences",<TITLE> The Impact of Language Technology on Multilingualism and Linguistic Diversity<TITLE> The Impact of Language Technology on Multilingualism and Diversity<TITLE> The Role of Language Technology in Multilingualism and Diversity<TITLE> The Impact of Language Technology on Multilingualism and Linguistics<TITLE> The Impact of Language Technology on Multilingualism and Diversity in Conference,"<TITLE>""No Language is Left Behind"": A Case Study on the Representation of Languages in NLP Conferences<TITLE>""No Language is Left Behind"": An ACL study of the status of languages in NLP conferences<TITLE>""No Language is Left Behind"": A Case Study on the Status of Languages in NLP Conferences<TITLE>""No Language is Left Behind"": A Case Study on the Representation of Languages in NLP conferences<TITLE>""No Language is Left Behind"": A Case Study on the Status of Languages in NLP Conference"
77,Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification,"We introduce globally normalized convolutional neural networks for joint entity classification and relation extraction. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.",<TITLE>Global Normalization of Convolutional Neural Networks for Joint Entity Classification and Relation Extraction<TITLE>Global Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction<TITLE>Global Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction.<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction with Conditional Random Fields<TITLE>Global Normalization of Neural Networks for Joint Entity Classification and Relation Extraction,<TITLE>Globally Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction with Global Normalized Random Fields<TITLE>Globally Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction with Global Normalized CRFs<TITLE>Globally Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction with Global Normalized Convol<TITLE>Globally Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction with Global Normalized Random Field<TITLE>Globally Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction with Global Normalized Regularized,<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction with Conditional Random Fields<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction,<TITLE>Global Normalized Neural Networks for Joint Entity Classification and Relation Extraction<TITLE>Global Normalized Convolutional Neural Networks for Joint Entity Classification<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction.<TITLE>Global Normalization for Joint Entities Classification and Relation Extraction,<TITLE> Global Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction<TITLE> Global Normalization for Joint Entity Classification and Relation Extraction<TITLE> Global Normalized Neural Networks for Joint Entity Classification and Relation Extraction<TITLE> Global Normalization of Joint Entity Classification and Relation Extraction<TITLE> Global Normalization for Joint Entity Classification and Relation Extraction with Linear-Chain CRF,<TITLE>Globally Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction<TITLE>Global Normalized Convolutional Neural Networks for Joint Entity Classification and Relation Extraction<TITLE>Global Normalized Convolutional Neural Network for Joint Entity Classification and Relation Extraction<TITLE>Global Normalized Convolutional Neural Networks for Joint Entity and Relation Extraction<TITLE>Global Normalization for Joint Entity Classification and Relation Extraction
78,Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding,"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The generated representations are further ensembled and classified using a neural-based early fusion approach. Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.",<TITLE>Heterogeneous Multi-layer Network-based Representation of Tweets for Sentiment Classification<TITLE>A Heterogeneous Multi-layer Network-Based Representation of Tweets for Text Classification<TITLE>A Heterogeneous Multi-layer Network-based Representation of Tweets for Text Classification<TITLE>Heterogeneous Multi-layer Network Representation of Tweets for Sentiment Classification<TITLE>A Heterogeneous Multi-layer Network-Based Representation of Tweets,<TITLE>Heterogeneous Multi-Layer Network-based Representation of Tweets with Centrality Aware Random Walk for Sentiment Classification on<TITLE>Heterogeneous Multi-Layer Network-based Representation of Tweets with Centrality Aware Random Walk for Sentiment Classification on Twitter<TITLE>Heterogeneous Multi-Layer Network-based Representation of Tweets with Centrality Aware Random Walks for Sentiment Classification<TITLE>Heterogeneous Multi-Layer Network-based Representation of Tweets with Centrality Aware Random Walk for Multilingual Sentiment<TITLE>Heterogeneous Multi-Layer Network-based Representation of Tweets with Centrality Aware Random Walk for Sentiment Classification on Twe,<TITLE>Heterogeneous Multi-layer Network-based Representation of Tweets for Sentiment Classification<TITLE>Heterogeneous Multi-layer Network-Based Representation of Tweets for Sentiment Classification<TITLE>Heterogeneous Multi-layer Network-based Representation of Tweets for Sentiment Classification<TITLE>Heterogeneous Multi-layer Representation of Tweets for Sentiment Classification on Twitter<TITLE>Heterogeneous Multi-layer Network-based Representation of Tweets for Sentiment Classification,<TITLE>Multi-layer Network Representations for Sentiment Classification<TITLE>Multi-layer Network Representation for Sentiment Classification<TITLE>Centrality Sentiment Classification on Tweets<TITLE>Multi-layer Network Representation of Tweets<TITLE>Centrality Sentiment Classification on Twitter,<TITLE> Heterogeneous Multi-Layer Representation of Tweets for Sentiment Classification<TITLE> Heterogeneous Multi-Layer Network-based Representation of Tweets<TITLE> Heterogeneous Multi-layer Representation of Tweets for Sentiment Classification<TITLE> Heterogeneous Multi-Layer Network-Based Representation of Tweets<TITLE> Heterogeneous Multi-Layer Networks for Tweet Representation,<TITLE>Heterogeneous Multi-layer Network-based Representations of Tweets for Sentiment Classification<TITLE>A Heterogeneous Multi-layer Network-based Representation of Tweets for Sentiment Classification<TITLE>Heterogeneous Multi-Layer Network-based Representations of Tweets for Sentiment Classification<TITLE>Heterogeneous Multi-layer Network-based Representation of Tweets for Sentiment Classification<TITLE>Heterogeneous Multi-Layer Network-based Representation of Tweets for Sentiment Classification
79,Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification,"The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can leverage the rich resources in one language (source language) for sentiment classification in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text. This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed BSWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems.",<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification<TITLE>Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification<TITLE>Bilingual Sentiment Word Embeddings for Cross-Language CRSC<TITLE>Learning Bilingual Embeddings for Cross-Language Sentiment Classification<TITLE>Bilingual Embeddings for Cross-Language Sentiment Classification,<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification in a Resource-Scarce LSTM<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification with High-Quality Bilingual Embedding<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification in a Resource-Scarce Language with High<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification in a Resource-Scarce Language using B<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification in a Resource-Scarce Language Using B,<TITLE>Learning Bilingual Sentiment Word Embeddings for English-Chinese Cross-Language Classification<TITLE>Learning Bilingual Sentiment Word Embeddings for English-Chinese Cross-Language Sentiment Classification<TITLE>Learning Bilingual Sentiment Word Embeddings for English-Chinese Cross-Language Classification<TITLE>Learning Bilingual Sentiment Word Embeddings for English-Chinese Cross-Language Classification<TITLE>Learning Bilingual Sentiment Word Embeddings for English-Chinese Cross-Language Classification,<TITLE>Bilingual Sentiment Word Embeddings for English-Chinese CLSC<TITLE>Learning bilingual sentiment word embeddings for English-Chinese CLSC<TITLE>Learning Bilingual Sentiment Word Embeddings<TITLE>Bilingual Sentiment Word Embeddings<TITLE>Learning bilingual sentiment word embeddings,<TITLE> Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification<TITLE> Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification<TITLE> Cross-Language Sentiment Classification with Bilingual Word Embeddings<TITLE> Learning Bilingual Sentiment Word Embeddings<TITLE> Cross-Language Sentiment Classification with Bilingual Embeddings,<TITLE>Bilingual Sentiment Word Embeddings for English-Chinese Cross-Language Sentiment Classification<TITLE>Bilingual Sentiment Word Embeddings for English-Chinese Cross-language Sentiment Classification<TITLE>Bilingual Sentiment Word Embedding for English-Chinese Cross-Language Sentiment Classification<TITLE>Learning Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification<TITLE>Bilingual Sentiment Word Embeddings for Cross-Language Sentiment Classification
80,Multilingual Document Clustering: An Heuristic Approach Based on Cognate Named Entities,"This paper presents an approach for Multilingual Document Clustering in comparable corpora. The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora. One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources. However, it depends on the possibility of identifying cognate named entities between the languages used in the corpus. An additional advantage of the approach is that it does not need any information about the right number of clusters; the algorithm calculates it. We have tested this approach with a comparable corpus of news written in English and Spanish. In addition, we have compared the results with a system which translates selected document features. The obtained results are encouraging.",<TITLE>Multilingual Document Clustering in Comparable Corpora Using Heuristic Data<TITLE>Multilingual Document Clustering in Comparable Corpora Using Heuristic Information<TITLE>Multilingual Document Clustering in Comparable Corpora using Heuristic Data<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora.,<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Information<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristics<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Networks<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Dynamics<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Complex,<TITLE>Multilingual Document Clustering in Comparable Corpora using Cognate Named Entities<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora,<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparative Corpora<TITLE>Multilingual Document Clustering for Comparable Corpora<TITLE>Multilingual Document Clustering in comparable corpora<TITLE>Multilingual Document Clustering,<TITLE> Multilingual Document Clustering Using Comparable Corpora<TITLE> Multilingual Document Clustering Based on Cognate Name Identification<TITLE> Multilingual Document Clustering Based on Cognate Identification<TITLE> Multilingual Document Clustering Using Bilingual Information<TITLE> Multilingual Document Clustering Based on Cognate Identification,<TITLE>Comparing Document Clustering in Comparable Corpora Using Heuristics<TITLE>An Algorithm for Document Clustering in Comparable Corpora<TITLE>Comparing Document Clustering in Comparable Corpora<TITLE>Comparative Document Clustering in Comparable Corpora<TITLE>Document Clustering in Comparable Corpora
81,Classifying Temporal Relations with Rich Linguistic Knowledge,"We examine the task of temporal relation classification. Unlike existing approaches to this task, we (1) classify an event-event or eventtime pair as one of the 14 temporal relations defined in the TimeBank corpus, rather than as one of the six relations collapsed from the original 14; (2) employ sophisticated linguistic knowledge derived from a variety of semantic and discourse relations, rather than focusing on morpho-syntactic knowledge; and (3) leverage a novel combination of rule-based and learning-based approaches, rather than relying solely on one or the other. Experiments with the TimeBank corpus demonstrate that our knowledge-rich, hybrid approach yields a 15–16% relative reduction in error over a state-of-the-art learning-based baseline system.",<TITLE>Temporal Relation Classification Using Semantic and Discourse Knowledge<TITLE>Temporal Relation Classification Using Semantic and Discourse Information<TITLE>Temporal Relation Classification Using Semantic and Discourse Relations<TITLE>Temporal Relation Classification with Hybrid Linguistic Knowledge<TITLE>A Hybrid Approach to Temporal Relation Classification,<TITLE>A Knowledge-Rich Hybrid Temporal Relation Classification Based on Morpho-Syntactic Knowledge and Learning-Based Approaches<TITLE>A Knowledge-Rich Hybrid Temporal Relation Classification Based on Morpho-Syntactic Knowledge and Learning-Based Learning of<TITLE>A Knowledge-Rich Hybrid Temporal Relation Classification Based on Morpho-Syntactic Knowledge and Learning-Based Learning in<TITLE>A Knowledge-Rich Hybrid Temporal Relation Classification Based on Morpho-Syntactic Knowledge and Learning-Based Learning for<TITLE>A Knowledge-Rich Hybrid Temporal Relation Classification Based on Morpho-Syntactic Knowledge and Learning-Based Learning.,<TITLE>Classifying Temporal Relations using the TimeBank Corpus<TITLE>Classifying Temporal Relations using the TimeBank<TITLE>Classifying Temporal Relations Using the TimeBank<TITLE>Knowledge-Rich Temporal Relation Classification<TITLE>Knowledge-Rich Temporal Relation Classification,<TITLE>Learning to Classify Event-Events and Eventtime Pairs<TITLE>Learning to Classify Event-Event or Eventtime Pairs<TITLE>Learning to Classify Event-Event and Eventtime Pairs<TITLE>Classifying Event-Events and Eventtime Pairs<TITLE>Temporal Relation Classification,<TITLE> Temporal Relation Classification Using Semantic and Discourse Relations<TITLE> Temporal Relation Classification using Semantic and Discourse Relations<TITLE> Temporal Relation Classification by Combining Rule-Based and Learning-Based Models<TITLE> Temporal Relation Classification Using Rule-Based and Learning-Based Approaches<TITLE> Temporal Relation Classification by Combining Rule-Based and Learning-Based Methods,<TITLE>Temporal Relation Classification with Hybrid Lexical Knowledge<TITLE>Temporal Relation Classification with Knowledge Rich Hybrid Approaches<TITLE>A Hybrid Approach to Temporal Relation Classification<TITLE>Temporal Relation Classification with Hybrid Knowledge<TITLE>Hybrid Temporal Relation Classification
82,Using Search-Logs to Improve Query Tagging,"Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.",<TITLE>Part-of-Speech Tagging for Syntactic Search Queries<TITLE>Part-of-Speech Tagging for Syntactic Search Query Analysis<TITLE>Part-of-Speech Tagging for Syntactic Query Analysis<TITLE>Part-of-Speech Tagging for Search Queries<TITLE>Part-of-Speech Tagging for Search Query Analysis,<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Information Retrieval Queries<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Run-Time Information Retri<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Run-time Information Retri<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Information Retrieval to Qu<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Information Retrieval to Query,<TITLE>Part-of-Speech Tagging for Efficient Syntactic Query Analysis<TITLE>Part-of-Speech Transfer for Efficient Syntactic Query Analysis<TITLE>Part-of-Speech Tagging for Efficient Syntactic Query Analysis<TITLE>Part-of-Speech Tagging for Efficient Query Analysis<TITLE>Part-of-Speech Tagging for Efficient Syntactic Query Analysis,<TITLE>Syntactic Analysis of Search queries with Part-of-Speech Tags<TITLE>Syntactic Query Analysis with Part-of-Speech Tags<TITLE>Using Part-of-Speech Tags for Syntactic Search Analysis<TITLE>Syntactic Query Analysis with Part-of-Speech Tags.<TITLE>Syntactic Query Analysis with Part-of-speech Tags,<TITLE> Part-of-Speech Tagging for Search Queries<TITLE> Part-of-Speech Tagging for Query Analysis<TITLE> Part-of-Speech Tagging for Syntactic Query Analysis<TITLE> Part-of-Speech Tagging for Search Query Analysis<TITLE> Part-of-Speech Tagging for Search Queries via Transfer,"<TITLE>Training Query Analysis Models with Part-of-Speech Tagging Data<TITLE>A Simple, Efficient Approach to Part-of-Speech Search Comprehension<TITLE>A Simple, Efficient Approach to Part-of-Speech Search Segmentation<TITLE>Training Query Analysis Models with Part-of-Speech Tag Transfer<TITLE>Training Query Analysis Models with Part-of-Speech Tagging"
83,Learning Concept Abstractness Using Weak Supervision,"We introduce a weakly supervised approach for inferring the property of abstractness of words and expressions in the complete absence of labeled data. Exploiting only minimal linguistic clues and the contextual usage of a concept as manifested in textual data, we train sufficiently powerful classifiers, obtaining high correlation with human labels. The results imply the applicability of this approach to additional properties of concepts, additional languages, and resource-scarce scenarios.",<TITLE>Weakly Supervised Inference of Abstractness in Texts<TITLE>Weakly Supervised Interpretation of Abstractness in Texts<TITLE>Weakly Supervised Interpretation of Abstractness in Text<TITLE>Weakly Supervised Inference of Abstractness in Text<TITLE>Weakly Supervised Learning of Abstractness in Text,<TITLE>Weakly Supervised Learning of Abstractness of Words and Expressions from Labelled Data: An Application to Semantic Interpretation<TITLE>Weakly Supervised Learning of Abstractness of Words and Expressions from Labelled Data: A Case Study in Contextual Sem<TITLE>Weakly Supervised Learning of Abstractness of Words and Expressions from Labelled Data: A Case Study on Semantic Represent<TITLE>Weakly Supervised Learning of Abstractness of Words and Expressions from Labelled Data: An Application to Semantic Representation<TITLE>Weakly Supervised Learning of Abstractness of Words and Expressions from Labelled Data: A Case Study on Semantic Interpret,<TITLE>Weakly Supervised Inference of Abstractness of Words and Expressions Without Labeled Data<TITLE>Weakly Supervised Inference of Abstractness of Words and Expressions without Labeled Data<TITLE>Weakly Supervised Inference of Abstractness of Words and Expressions<TITLE>Weakly Supervised Inference of Abstractness of Words and Expressions<TITLE>Weakly Supervised Inference of Abstractness of Words and Expressions,<TITLE>Inferring Abstractness of Words and Expressions in Textual Data<TITLE>A weakly supervised approach to abstractness of words and expressions<TITLE>A weakly supervised approach to abstractness in textual data<TITLE>A weakly supervised approach to abstractness of words and expressions<TITLE>A weakly supervised approach to abstractness in textual data,<TITLE> Weakly Supervised Learning of Abstractness<TITLE> Weakly Supervised Learning of Abstract Meaning Representations<TITLE> Weakly Supervised Learning of Abstractness from Text<TITLE> Weakly Supervised Induction of Abstractness<TITLE> Weakly Supervised Learning of Abstractness from Text Data,<TITLE>Weakly Supervised Inference of Concept and Expression Abstractness<TITLE>Weakly Supervised Learning of Abstractness of Words and Expressions<TITLE>Weakly Supervised Inference of Concept and Expression Properties<TITLE>Weakly Supervised Learning of Concept and Expression Abstractness<TITLE>Weakly Supervised Learning of Concept and Expression Properties
84,Reducing Overfitting in Deep Networks by Decorrelating Representations,"One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",<TITLE>DeCov: Reducing Overfitting in Deep Neural Networks by Minimizing Cross-Covariance<TITLE>DeCov: Reducing Overfitting in Deep Neural Networks via Cross-Covariance and Generalization<TITLE>DeCov: Reducing Overfitting in Deep Neural Networks with Cross-Covariance<TITLE>DeCov: Reducing Overfitting and Generalization in Deep Neural Networks<TITLE>DeCov: Reducing Overfitting in Deep Neural Networks,<TITLE>DeCov: DeCov Regularized Training of Deep Neural Networks via Cross-Covariance and Generalization.<TITLE>DeCov: DeCov Regularized Training of Deep Neural Networks via Cross-Covariance Minimization and<TITLE>DeCov: DeCov Regularized Training of Deep Neural Networks with Cross-Covariance and Generalization.<TITLE>DeCov: DeCov Regularized Training of Deep Neural Networks via Cross-Covariance Minimization.<TITLE>DeCov: DeCov Regularized Training of Deep Neural Networks via Cross-Covariance and Generalization Loss,<TITLE>DeCov: A Cross-Covariance Regularizer for Deep Neural Networks<TITLE>DeCov: A Cross-Cov Regularizer for Deep Neural Networks<TITLE>DeCov: A Cross-Covariance Regularization for Deep Neural Networks<TITLE>DeCov: A Cross-Covariance Regularizer for Deep Learning<TITLE>DeCov: A Cross-Cov Regularizer for Deep Neural Networks,<TITLE>DeCov: Regularizing Deep Neural Networks for Overfitting<TITLE>DeCov: a Regularizer for Deep Neural Networks<TITLE>DeCov: A Regularizer for Deep Neural Networks<TITLE>A DeCov Regularizer for Deep Neural Networks<TITLE>DeCov: Regularizing Deep Neural Networks,<TITLE> DeCov: DeCov Regularization for Deep Neural Networks<TITLE> DeCov: DeCov Regularization for Training Deep Neural Networks<TITLE> DeCov: A New Regularizer for Deep Neural Networks<TITLE> DeCov: DeCov Regularized Deep Neural Networks<TITLE> DeCov: DeCov Regularizing Deep Neural Networks,<TITLE>DeCov: Regularizing Deep Neural Networks with Cross-Coding<TITLE>DeCov: A Neural Regularizer for Deep Neural Networks<TITLE>DeCov: Reducing Overfitting in Deep Neural Networks<TITLE>DeCov: A Deep Neural Network Regularizer<TITLE>DeCov: Regularizing Deep Neural Networks
85,Exploring Adaptor Grammars for Native Language Identification,"The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and fixed n) and unigram function words. To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classification. After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model.",<TITLE>Using Adaptor Grammars to Identify N-gram Collocations for Native Language Classification<TITLE>N-gram Collocations of arbitrary length for a Second Language<TITLE>Using Adaptor Grammars for Native Language Classification<TITLE>N-gram Collocations for Native Language Classification<TITLE>N-gram Collocation for Native Language Classification,<TITLE>Automatic Collocation of Native Language Texts using Adaptor Grammars and Syntax-Based Syntactic Language Modeling<TITLE>Automatic Collocation of Native Language Texts using Adaptor Grammars and Syntax-based Syntactic Language Modeling<TITLE>Automatic Collocation of Native Language Texts Using Syntax-Based Grammars and PoS Tags and Words with Application<TITLE>Automatic Collocation of Native Language Texts using Adaptor Grammars and Syntax-based Syntactic Language Models.<TITLE>Automatic Collocation of Native Language Texts using Adaptor Grammars and Syntax-Based Syntactic Language Models.,<TITLE>Learning Adaptor Grammars for Native Language Inference from Second Language Texts<TITLE>Learning Adaptor Grammars for Native Language Inference<TITLE>Adaptor Grammars for Native Language Inference<TITLE>Learning Adaptor Grammars for Native Language Identification<TITLE>Learning Adaptor Grammars for Native Language Detection,<TITLE>Identifying N-gram Collocations over PoS Tags and Words for Syntactic Language Models<TITLE>Combining Maximal and Induced Syntactic Language Models for Identifying N-gram Collocations<TITLE>Identifying N-gram Collocations over PoS Tags and Words for Syntax-Based Language Models<TITLE>Identifying N-gram Collocations over PoS Tags and Words for Syntax-based Language Models<TITLE>Identifying N-gram Collocations over PoS Tags and Words for Syntactic Language Model,<TITLE> Adaptor Grammars for Native Language Acquisition<TITLE> Adaptor Grammars for Native Language Identification<TITLE> Adaptor Grammars for Native Language Induction<TITLE> Adaptor Grammars for Native Language Authorship<TITLE> Adaptor Grammars for Native Language Learning,<TITLE>Learning to Induct Native Language Features into Syntactic Language Models<TITLE>Exploiting Syntactic Language Models for Native Language Identification<TITLE>Learning to Classify Native Language Using Syntactic Language Models<TITLE>Exploiting Syntactic Language Models for Native Language Induction<TITLE>Learning to Infer Native Language from PoS Tagging
86,Why-Question Answering using Intra- and Inter-Sentential Causal Relations,"In this paper, we explore the utility of intraand inter-sentential causal relations between terms or clauses as evidence for answering why-questions. To the best of our knowledge, this is the first work that uses both intraand inter-sentential causal relations for why-QA. We also propose a method for assessing the appropriateness of causal relations as answers to a given question using the semantic orientation of excitation proposed by Hashimoto et al. (2012). By applying these ideas to Japanese why-QA, we improved precision by 4.4% against all the questions in our test set over the current state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only.",<TITLE>Using Interra and Inter-Sentential Causal Relations for Japanese Why-Answer Answering<TITLE>Using Interra and Inter-Sentential Causal Relations as Evidence for Japanese Why-QA<TITLE>Japanese Why-Answer Answering Using Interra and Inter-Sentential Causal Relations<TITLE>Using Interra and Inter-Sentential Causal Relations for Japanese Why-Answer Extraction<TITLE>Using Interra and Inter-Sentential Causal Relations for Japanese Why-QA,<TITLE>Why-QA: Using Intra- and Inter-Sentential Causal Relations to Improve Precision for Japanese Why-Q<TITLE>Why-QA: Using Intra- and Inter-Sentential Causal Relations to Improve Precision for Japanese Why-quest<TITLE>Why-QA: Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why-questions.<TITLE>Why-QA: Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why-questions for<TITLE>Why-QA: Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why-questions as,<TITLE>Using Intra-and-Inter-Sentential Causal Relations for Japanese Why-QA<TITLE>Using Intra-and-Sentential Causal Relations for Japanese Why-Question Answering<TITLE>Using Intra-and-Sentential Causal Relations for Japanese Why-QA<TITLE>Using Causal Relations for Japanese Why-Question Answering<TITLE>Using Causal Relations for Japanese Why-QA,<TITLE>Using Intra-and Inter-Sentential Causal Relations for Japanese Why-QA<TITLE>Using Intra-and Inter-Sentential Causal Relations for Why-QA<TITLE>Using Intra-and Inter-Sentential Causal Relations for Why-QA<TITLE>Using Intra-Sentential Causal Relations for Why-QA<TITLE>Inter-Sentential Causal Relations for Japanese Why-QA,<TITLE> Why-QA Using Intra-and Inter-Sentential Causal Relations<TITLE> Why-QA Using Inter-Sentential Causal Relations<TITLE> Why-QA Using Inter-sentential Causal Relations<TITLE> Japanese Why-QA Using Inter-Sentential Causal Relations<TITLE> Why-QA Using Inter-sentential Causal Relations for Why-Questions,<TITLE>Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why-questions<TITLE>Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why-Questions<TITLE>Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why- Questions<TITLE>Using Intra- and Inter-Sentential Causal Relations to Answer Japanese Why-QA<TITLE>Using Intra- and Inter-Sentential Causal Relations for Japanese Why-QA
87,Joint Emotion Analysis via Multi-task Gaussian Processes,"We propose a model for jointly predicting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches.",<TITLE>Joint Predicting of Multiple Emotions in Natural Language Sentences<TITLE>Joint Predicting Multiple Emotions in Natural Language Sentences<TITLE>Joint Predicting of Emotions in Natural Language Sentences<TITLE>Joint Predicting of Multiple Emotions in News Headlines<TITLE>Joint Predicting Multiple Emotions in News Headlines,<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Gaussian Processes and Low-Rank Coregional<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Low-Rank Coregionalisation and Gaussian Process<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Low-Rank Coregionalisation and Vector-valued<TITLE>Jointly Predicting Multiple Emotions in Sentences using Gaussian Processes and Low-Rank Coregionalisation.<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Low-Rank Coregionalisation and Vector-Val,<TITLE>A Low-Rank Coregionalisation Model for Joint Prediction of Multiple Emotions in Natural Language<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences with Coregionalisation<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences,<TITLE>A Low-rank Coregionalisation Approach to Predicting Multiple Emotions in Natural Language Sentences<TITLE>A Low-rank Coregionalisation Approach for Predicting Multiple Emotions in Natural Language Sentences<TITLE>A Low-Rank Coregionalisation Approach for Predicting Multiple Emotions in Natural Language Sentences<TITLE>A Low-rank Coregionalisation Approach to Predicting Multiple Emotions<TITLE>A Low-rank Coregionalisation Approach for Predicting Multiple Emotions,<TITLE> Joint Predicting Multiple Emotions in Natural Language Sentences<TITLE> Joint Predicting Multiple Emotions in Natural Language<TITLE> Jointly Predicting Multiple Emotions in Natural Language<TITLE> Joint Predicting Multiple Emotions in Natural Language Text<TITLE> Jointly Predicting Multiple Emotions in Natural Language Text,<TITLE>A Low-Rank Co-Regionisation Approach for Joint Emotion Prediction<TITLE>A Low-Rank Co-Regionisation Approach to Joint Emotion Prediction<TITLE>A Low-Rank Co-regionalisation Approach to Joint Emotion Prediction<TITLE>A Low-Rank Co-Region Model for Joint Emotion Prediction<TITLE>Learning to Predict Multiple Emotions in Natural Language Sentences
88,Subspace Clustering with Irrelevant Features via Robust Dantzig Selector,"This paper considers the subspace clustering problem where the data contains irrelevant or corrupted features. We propose a method termed “robust Dantzig selector” which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations. To the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.",<TITLE>Robust Dantzig selector for subspace clustering with irrelevant features<TITLE>Robust Dantzig selector for subspace clustering with irrelevant features.<TITLE>Robust Dantzig Selector for the Subspace Clustering Problem<TITLE>Robust Dantzig selector for subspace clustering without irrelevant features<TITLE>Robust Dantzig Selector for Subspace Clustering,<TITLE>Robust Dantzig Selector: Identifying Irrelevant Features in Subspace Clustering with Robust Densities.<TITLE>Robust Dantzig Selector: Identifying Irrelevant Features in Subspace Clustering with Robust Densities and<TITLE>Robust Dantzig Selector: Identifying Irrelevant Features in Subspace Clustering with Robust Density Selection.<TITLE>Robust Dantzig Selector: Identifying Irrelevant Features in Subspace Clustering with Robust Density Search.<TITLE>Robust Dantzig Selector: Identifying Irrelevant Features in Subspace Clustering with Robust Density Networks.,<TITLE>Robust Dantzig Selector for Subspace Clustering with Irrelevant Features<TITLE>Robust Dantzig Selector for Subspace Clustering with Irrelevant Features.<TITLE>Robust Dantzig Selector for Subspace Clustering with Irrelevant Features<TITLE>Robust Dantzig Selector for Subspace Clustering with Irrelevant Features<TITLE>Robust Dantzig Selector for Subspace Clustering with Irrelevant Features,<TITLE>A Robust Dantzig Selector for Subspace Clustering with irrelevant features<TITLE>A Robust Dantzig Selector to Identify Subspace Clustering<TITLE>Robust Dantzig Selector to Identify Subspace Clustering<TITLE>A Robust Dantzig Selector for Subspace Clustering<TITLE>Robust Dantzig Selector for Subspace Clustering,<TITLE> Robust Dantzig Subspace Clustering<TITLE> Robust Subspace Clustering Using Robust Dantzig Selector<TITLE> Robust Subspace Clustering Using Robust Dantzig<TITLE> A Robust Subspace Clustering Algorithm<TITLE> Robust Subspace Clustering Using Robust Dantzig Selectors,<TITLE>Robust Subspace Clustering with Irrelevant Features<TITLE>Robust Subspace Clustering with Irrelevant Features.<TITLE>Robust Dantzig Selector for Subspace Clustering<TITLE>Robust Subspace Clustering with Irritating Features<TITLE>Robust Subspace Clustering
89,Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent,"Contrary to most natural language processing research, which makes use of static datasets, humans learn language interactively, grounded in an environment. In this work we propose an interactive learning procedure called Mechanical Turker Descent (MTD) and use it to train agents to execute natural language commands grounded in a fantasy text adventure game. In MTD, Turkers compete to train better agents in the short term, and collaborate by sharing their agents’ skills in the long term. This results in a gamified, engaging experience for the Turkers and a better quality teaching signal for the agents compared to static datasets, as the Turkers naturally adapt the training data to the agent’s abilities.",<TITLE>Turker Descent: Interactively Learning to execute Natural Language commands in a Fantasy Game<TITLE>Turker Descent: Interactive Learning of Natural Language commands grounded in a Fantasy Game<TITLE>Turker Descent: Interactive Learning of Natural Language commands grounded in an Environment<TITLE>Turker Descent: An Interactive Learning Procedure for Natural Language Processing<TITLE>Turker Descent: Interactive Training of Natural Language Agents,<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Dataset and Skill Sharing<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Datasets and Skill Sharing<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game Using Static Datasets and Util<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Dataset and Util<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Dataset and Teaching Skills,<TITLE>Turker Descent: Interactive Learning for Natural Language Processing with Mechanical Turkers<TITLE>Interactive Learning for Natural Language Processing with Mechanical Turker Descent<TITLE>Interactive Learning of Natural Language Agents with Mechanical Turker Descent<TITLE>Turker Descent: Interactive Learning for Natural Language Processing<TITLE>Turker Descent: Interactive Learning with Mechanical Turkers,<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Language Commands<TITLE>Mechanical Turker Descent: An Interactive Learning Approach to Natural Language<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Languages<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Language<TITLE>Mechanical Turker Descent,<TITLE> Turker Descent: Interactive Learning for Natural Language Utterance Games<TITLE> Turker Descent: Interactive Learning of Natural Language Utterance<TITLE> Learning to execute natural language commands grounded in a fantasy text game<TITLE> Turker Descent: Interactive Learning for Natural Language Utterance<TITLE> Turker Descent: Interactive Learning of Natural Language Utterance Games,<TITLE>Mechanical Turker Descent: Learning Language Interactively from Text Adventure Games<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Language from Text Games<TITLE>Mechanical Turker Descent: Learning Language Interactively from Text Games<TITLE>Mechanical Turker Descent: Learning Language Interactively from Text<TITLE>Interactive Learning of Natural Language with Mechanical Turker Descent
90,Learning Better Data Representation Using Inference-Driven Metric Learning,"We initiate a study comparing effectiveness of the transformed spaces learned by recently proposed supervised, and semisupervised metric learning algorithms to those generated by previously proposed unsupervised dimensionality reduction methods (e.g., PCA). Through a variety of experiments on different realworld datasets, we find IDML-IT, a semisupervised metric learning algorithm to be the most effective.",<TITLE>An Empirical Comparison of Supervised and Semisupervised Metric Learning Algorithms<TITLE>An Empirical Comparison of Supervised and Semisupervised Dimensionality Reduction Algorithms<TITLE>The Effectiveness of Supervised and Semisupervised Metric Learning for Dimensionality Reduction<TITLE>An Empirical Comparison of Supervised and Semisupervised Dimensionality Reduction Methods<TITLE>The Effectiveness of Supervised and Semisupervised Dimensionality Reduction Methods,<TITLE>A Study on Transformation Metric Learning for Unsupervised Dimensionality Reduction with Application to Supervised and Semi-Supervised Met<TITLE>A Study on Transformation Metric Learning for Unsupervised Dimensionality Reduction with Application to Supervised and Semi-Supervised Learning<TITLE>A Study on Transformation Metric Learning for Unsupervised Dimensionality Reduction via Supervised and Semi-Supervised MDPs<TITLE>A Study on Transformation Metric Learning for Unsupervised Dimensionality Reduction with Application to Supervised and Semi-Supervised M<TITLE>A Study on Transformation Metric Learning for Unsupervised Dimensionality Reduction via Supervised and Semi-Supervised Metric Represent,<TITLE>Semi-Supervised Metric Learning vs. Unsupervised Dimensionality Reduction: A Case Study on IDML-IT<TITLE>Semi-Supervised Metric Learning vs. Unsupervised Dimensionality Reduction Methods: A Case Study on IDML-<TITLE>Semi-Supervised Metric Learning versus Unsupervised Dimensionality Reduction: A Case Study on IDML-IT.<TITLE>Semisupervised Metric Learning vs. Unsupervised Dimensionality Reduction: A Case Study on IDML-IT and<TITLE>Semi-Supervised Metric Learning versus Unsupervised Dimensionality Reduction: A Case Study on IDML-IT and,<TITLE>IDML-IT: a Semi-Supervised Learning Algorithm<TITLE>IDML-IT: A Semi-Supervised Learning Algorithm<TITLE>IDML-IT: a Semi-supervised Learning Algorithm<TITLE>IDML-IT: a Semisupervised Learning Algorithm<TITLE>IDML-IT: A Semisupervised Learning Algorithm,<TITLE> Unsupervised Dimensionality Reduction with Transformed Spaces<TITLE> Semi-supervised Dimensionality Reduction with Transformed Spaces<TITLE> Semi-supervised Dimensionality Reduction for Statistical Machine Translation<TITLE> Unsupervised Dimensionality Reduction with Transformation Spaces<TITLE> Semi-supervised Dimensionality Reduction with Transformation Spaces,<TITLE>A Comparison of Supervised and Semi-Supervised Metric Learning Algorithms<TITLE>A Comparison of Supervised and Semi-Supervised Metric Learning Methods<TITLE>A Comparison of Supervised and Semi-supervised Metric Learning Algorithms<TITLE>A Comparison of Supervised and Semisupervised Metric Learning Algorithms<TITLE>Supervised and Semi-Supervised Metric Learning
91,An Infinity-sample Theory for Multi-category Large Margin Classification,"The purpose of this paper is to investigate infinity-sample properties of risk minimization based multi-category classification methods. These methods can be considered as natural extensions to binary large margin classification. We establish conditions that guarantee the infinity-sample consistency of classifiers obtained in the risk minimization framework. Examples are provided for two specific forms of the general formulation, which extend a number of known methods. Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem. Such conditional probability information will be useful for statistical inferencing tasks beyond classification.",<TITLE>On the infinity-sample properties of risk minimization based multi-category classification methods<TITLE>Infinite-Sample Properties of Risk Minimization Based Multi-Category Classification Methods<TITLE>The infinity-sample properties of risk minimization based multi-category classification methods.<TITLE>The infinity-sample properties of risk minimization for multi-category classification methods.<TITLE>The infinity-sample properties of risk minimization based multi-category classification methods,<TITLE>Infinite-sample multi-category classification with risk minimization and Conditional probability estimates. A general analysis of the general formulations<TITLE>Infinite-sample multi-category classification with risk minimization and Conditional probability estimates. A general analysis of the role of risk<TITLE>Infinite-sample multi-category classification with risk minimization and Conditional probability estimates. A general analysis of the general formulation<TITLE>Infinite-sample multi-category classification with risk minimization and Conditional probability estimates. A general analysis of the role of marginal<TITLE>Infinite-sample multi-category classification with risk minimization and Conditional probability estimates. A general analysis of the case of binary,<TITLE>Risk Minimization and Conditional Probability Estimation in Multi-category Classification<TITLE>Risk Minimization and Conditional Probability Estimation for Multi-category Classification<TITLE>Risk Minimization and Conditional Probability Estimation in Multi-Category Classification<TITLE>Risk Minimization and Conditional Probability Estimation in Multi- category Classification<TITLE>Risk Minimization and Conditional Probability Estimation in Multi-Classification,<TITLE>Infinity-Sample Properties of Risk Minimization based Multi-Category Classification<TITLE>Infinity-Sample Properties of Risk Minimization Based Multi-Category Classification<TITLE>Infinity-Sample Properties of Risk Minimization based Multi-category Classification<TITLE>Infinity-Sample Properties of Risk Minimization Based Multi-category Classification<TITLE>Risk Minimization based Multi-Category Classification,<TITLE> On the Consistency of Risk Minimization Based Multi-category Classification<TITLE> On the Consistency of Risk Minimization Based Multi-Category Classification<TITLE> On the Consistency of Risk Minimization<TITLE> On the Consistency of Risk Minimization Methods<TITLE> On the Consistency of Risk Minimization Classification,<TITLE>The infinity-sample consistency of risk based multi-category classification<TITLE>The infinity-sample consistency of multi-category classification methods.<TITLE>The infinity-sample consistency of multi-category classification methods<TITLE>The infinity-sample consistency of risk formulations for classification<TITLE>On the Consistency of Multi-Category Classification Methods
92,Detecting Speculations and their Scopes in Scientific Text,"Distinguishing speculative statements from factual ones is important for most biomedical text mining applications. We introduce an approach which is based on solving two sub-problems to identify speculative sentence fragments. The first sub-problem is identifying the speculation keywords in the sentences and the second one is resolving their linguistic scopes. We formulate the first sub-problem as a supervised classification task, where we classify the potential keywords as real speculation keywords or not by using a diverse set of linguistic features that represent the contexts of the keywords. After detecting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes.",<TITLE>Identifying Speculation Keyword Fragments in Biomedical Text Mining<TITLE>Identifying Speculation Keyword Fragments for Biomedical Text Mining<TITLE>Identifying Speculation Keyword Fragments Using Syntactic Structures<TITLE>Identifying Speculation Keywords in Biomedical Text Mining Applications<TITLE>Identifying Speculation Keywords in Biomedical Text Mining,<TITLE>Identifying Speculation Keywords in Sentence Fragments using Syntactic Structures and their Linguistic Scopes: An<TITLE>Identifying Speculation Keywords in Sentence Fragments using Syntactic Structures and their Linguistic Scopes: A<TITLE>Identifying Speculation Keywords in Sentence Fragments Using Syntactic Structures and their Linguistic Scopes: An<TITLE>Identifying Speculation Keywords in Sentence Fragments Using Syntactic Structures and their Linguistic Scopes: A<TITLE>Identifying Speculation Keywords in Sentence Fragments using Syntactic Structures and their Linguistic Scopes for Bi,<TITLE>Identifying Speculation in Biomedical Text Mining Using Linguistic Features<TITLE>Identifying Speculation in Biomedical Text Mining: A Two-Stage Approach<TITLE>Identifying Speculation in Biomedical Text Mining: A Two-Step Approach<TITLE>Identifying Speculation Keywords in Biomedical Text Mining<TITLE>Identifying Speculation in Biomedical Text Mining,<TITLE>Distinguishing Speculative Statements from Factual Keywords<TITLE>Distinguishing Speculative Statements from Factual Ones<TITLE>Distinguishing Spekulative Statements from Factual Keywords<TITLE>Distinguishing Spekulative Statements from Factual Ones<TITLE>Distinguishing Spekulative Statements from Factual Words,<TITLE> Identifying Spurious Sentences Using Syntactic Structures<TITLE> Identifying Spurious Sentences in Biomedical Text Mining<TITLE> Identifying Spurious Sentences with Syntactic Structures<TITLE> Identifying Spurious Sentences in Medical Text Mining Applications<TITLE> Identifying Spurious Sentences in Biomedical Text,<TITLE>Identifying Speculation in Factual Texts<TITLE>Identifying Speculation in Factual Text Mining<TITLE>Identifying Speculation in Factual Statements<TITLE>Identifying Speculation in Sentences<TITLE>Identifying Speculation in Text Mining
93,Improved Estimation of Entropy for Evaluation of Word Sense Induction,"Information-theoretic measures are among the most standard techniques for evaluation of clustering methods including word sense induction (WSI) systems. Such measures rely on sample-based estimates of the entropy. However, the standard maximum likelihood estimates of the entropy are heavily biased with the bias dependent on, among other things, the number of clusters and the sample size. This makes the measures unreliable and unfair when the number of clusters produced by different systems vary and the sample size is not exceedingly large. This corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems.",<TITLE>Evaluating Word Sense Induction Systems Using Entropy Estimation<TITLE>Information-Theoretic Evaluation of Word Sense Induction Systems<TITLE>Information-Theoretic Measures of Word Sense Induction Systems<TITLE>Information-theoretic Measures of Word Sense Induction Systems<TITLE>Information-Theoretic Measures of Word Sense Induction,<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Clustering Schemes<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Schem<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Systems.<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Systems with<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Scores,<TITLE>Evaluation of Word Sense Induction Systems Based on Sample-Based Entropy Estimation<TITLE>Evaluation of Word Sense Induction Systems: More Accurate Estimation of Entropy<TITLE>Evaluation of Word Sense Induction Systems: More Accurate Entropy Estimators<TITLE>Evaluation of Word Sense Induction Systems via Entropy Estimation<TITLE>Evaluation of Word Sense Induction Systems via Entropy Estimates,<TITLE>Information-Theoretic Measures for Word Sense Induction Evaluation<TITLE>Information-Theoretic Measures for Word Sentence Induction<TITLE>Information-Theoretic Measures for Word Sense Induction<TITLE>Evaluating Word Sentence Induction Systems<TITLE>Evaluating Word Sense Induction Systems,<TITLE> Evaluating Clustering Methods Using Word Sense Induction<TITLE> Evaluating Clustering Methods using Word Sense Induction<TITLE> Efficient Evaluation of Clustering Methods<TITLE> Evaluating Clustering Methods<TITLE> Estimating the Entropy of Clustering Methods,<TITLE>On the Accuracy of Entropy Estimation for Word Sense Induction<TITLE>On the Accuracy of Entropy Measures for Word Sense Induction<TITLE>Information-theoretic measures of entropy for word sense induction<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation<TITLE>Information-theoretic measures for word sense induction
94,Learning with Annotation Noise,"It is usually assumed that the kind of noise existing in annotated data is random classification noise. Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases. Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time.",<TITLE>Hard Training Cases in Random Classification Noise Generation<TITLE>Hard Training Cases for Random Classification Noise Detection<TITLE>Hard Training Cases for Random Classification Noise<TITLE>Hard Training Cases of Random Classification Noise<TITLE>Hard Training Cases in Random Classification Noise,<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Hardness<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Error Classification<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Hard Cases<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Hard Examples<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Error Detection,<TITLE>Automatic Generation of Hard Cases with Noisy Annotated Data<TITLE>Automatic Generation of Hard Cases with Unwanted Attention Slips<TITLE>Automatic Generation of Hard Cases with Bias-Based Annotation<TITLE>Automatic Generation of Hard Cases with Unwanted Annotation Errors<TITLE>Automatic Generation of Hard Cases with Noisy Annotations,<TITLE>Hard Training Cases with Unreliable Annotation Generation<TITLE>Unreliable Annotation Generation for Random Classification Noise<TITLE>Hard Training Cases with Unreliable Annotations<TITLE>Hard Training Cases with Unreliable Annotation<TITLE>A Random Classification Noise Model for Hard Training,<TITLE> Random Attention Slippage in Annotation<TITLE> Random Attention Slips in Annotation<TITLE> Random Attention Slippage and Hard Cases<TITLE> Random Attention Slips in Annotation Models<TITLE> Random Attention Slippage in Annotations,<TITLE>Training hard cases with unreliable annotations could result in incorrect prediction on uncontroversial cases<TITLE>Training hard-to-describe cases with unreliable annotations is not always random<TITLE>Training hard-to-decide cases with unreliable annotations is not always random<TITLE>Training hard-to-decidency cases with unreliable annotations<TITLE>Training hard cases with unreliable annotations could result in incorrect prediction
95,Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network,"This paper proposes a method for intrasentential subject zero anaphora resolution in Japanese. Our proposed method utilizes a Multi-column Convolutional Neural Network (MCNN) for predicting zero anaphoric relations. Motivated by Centering Theory and other previous works, we exploit as clues both the surface word sequence and the dependency tree of a target sentence in our MCNN. Even though the F-score of our method was lower than that of the state-of-the-art method, which achieved relatively high recall and low precision, our method achieved much higher precision (>0.8) in a wide range of recall levels. We believe such high precision is crucial for real-world NLP applications and thus our method is preferable to the state-of-the-art method.",<TITLE>A Multi-column Convolutional Neural Network for Zero Anaphora Resolution in Japanese<TITLE>A Multi-column Convolutional Neural Network for Zero Anaphora Resolution in Japanese.<TITLE>Japanese Zero Anaphora Resolution Using Multi-Column Convolutional Neural Networks<TITLE>Japanese Zero Anaphora Resolution Using Multi-column Convolutional Neural Networks<TITLE>A Multi-column Convolutional Neural Network for Zero Anaphora Resolution,<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using a Multi-Matrix<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using Clues from Cent<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using MCNNs and<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using Clues from Sent<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using Clues both the,<TITLE>Japanese Zero Anaphora Resolution with Multi-Column Convolutional Neural Networks<TITLE>Japanese Zero Anaphora Resolution Using Multi-Column Convolutional Neural Networks<TITLE>Japanese Zero Anaphora Resolution Using Multi-Column Convolutional Neural Network<TITLE>Japanese Zero Anaphora Resolution with Multi-Column Convolutional Neural Network<TITLE>Japanese Zero Anaphora Resolution with Multi-Column Convolutional Neural Network,<TITLE>Multi-Column Convolutional Neural Networks for Intrentential Subject Zero Anaphora Resolution<TITLE>A Multi-Column Convolutional Neural Network for Intrentential Subject Zero Anaphora Resolution<TITLE>Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution<TITLE>Multi-Column Convolutional Neural Network for Intrentential Subject Zero Anaphora Resolution<TITLE>Intrasentential Subject Zero Anaphora Resolution in Japanese,<TITLE> Intrasentential Subject Zero Anaphora Resolution in Japanese<TITLE> Intrasentential Subject Zero Anaphora Resolution with Multi-Column Convolutional Neural Networks<TITLE> Intrasentential Subject Zero Anaphora Resolution Using Multi-Column Convolutional Neural Networks<TITLE> Multi-column Convolutional Neural Network for Japanese Subject Zero Anaphora Resolution<TITLE> Intrasentential Subject Zero Anaphora Resolution Using Multi-column Convolutional Neural Networks,<TITLE>Intra-sentential Subject Zero Anaphora Resolution with Multi- Convolutional Neural Network<TITLE>Intra-sentential Subject Zero Anaphora Resolution with Multi- Convolutional Neural Networks<TITLE>Intra-Sentential Zero Anaphora Resolution with Multi- Convolutional Neural Network.<TITLE>Intra-sentential Zero Anaphora Resolution with Multi- Convolutional Neural Network.<TITLE>Intra-Sentential Zero Anaphora Resolution with Multi-Convolutional Neural Network.
96,Constructing Semantic Space Models from Parsed Corpora,Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations.,<TITLE>Semantic Spaces for Lexical Semantic Parsing<TITLE>Constructing Semantic Spaces for Lexical Semantics<TITLE>Constructing Semantic Spaces for Lexical Meaning<TITLE>Semantic Spaces for Lexical Semantics<TITLE>Semantic Spaces with Syntactic Relations,<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: A Case Study in Vector-Based Learning of Lexical Meaning<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: A Case Study in Vector-based Learning of Lexical Meaning<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: An Application to Modelling Lexical Meaning in Vector-based<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: A Case Study in Vector-based Models for Lexical Meaning<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: An Application to Modelling Lexical Meaning in Vector-Based,<TITLE>Semi-supervised Learning of Semantic Spaces with Syntactic Relations<TITLE>Semi-supervised Learning of Lexical Spaces with Syntactic Relations<TITLE>Semi-supervised Learning of Semantic Spaces using Lexical Relations<TITLE>Semi-supervised Learning of Semantic Spaces from Lexical Data<TITLE>Semi-supervised Learning of Lexical Space Models,<TITLE>Semantic Spaces for Syntactic Relation Models<TITLE>Semantic Spaces for Syntactic Relation Modeling<TITLE>Building Semantic Spaces with Syntactic Relations<TITLE>Constructing Semantic Spaces for Lexical Meaning<TITLE>Semantic Spaces for Syntactic Relations,<TITLE> Semantic Spaces for Lexical Relations<TITLE> Semantic Spaces for Word Co-occurrence Models<TITLE> Semantic Spaces for Lexical Roles<TITLE> Semantic Spaces for Lexical Semantic Representation<TITLE> Semantic Spaces for Lexical Semantics,<TITLE>A Formalisation of Vector-based Models of Semantic Spaces<TITLE>Constructing Semantic Spaces for Lexical Relations Using Syntactic Information<TITLE>Constructing Semantic Spaces for Lexical Relations with Syntactic Information<TITLE>A Formalisation of Vector-based Models of Semantic Properties<TITLE>Constructing Semantic Spaces for Lexical Relations
97,Experimental Evaluation of LTAG-Based Features for Semantic Role Labeling,"This paper proposes the use of Lexicalized Tree-Adjoining Grammar (LTAG) formalism as an important additional source of features for the Semantic Role Labeling (SRL) task. Using a set of one-vs-all Support Vector Machines (SVMs), we evaluate these LTAG-based features. Our experiments show that LTAG-based features can improve SRL accuracy significantly. When compared with the best known set of features that are used in state of the art SRL systems we obtain an improvement in F-score from 82.34% to 85.25%.",<TITLE>Lexicalized Tree-Adjoining Grammar as an Application to Semantic Role Labeling<TITLE>Using Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE>Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE>Using LTAG-based Features for Semantic Role Labeling<TITLE>Using LTAG-Based Features for Semantic Role Labeling,<TITLE>Semantic Role Labeling with LTAG-based Features and Lexicalized Tree-Adjoining Grammar (LTAG)<TITLE>Semantic Role Labeling with LTAG-based Features and Lexicalized Tree-Adjoining Grammar (LTAGs<TITLE>Semantic Role Labeling with LTAG-based Features and Lexicalized Tree-Adjoining Grammar (LTAG):<TITLE>Semantic Role Labeling with LTAG-based Features from Tree-Adjoining Grammars and Support Vector Machines: A<TITLE>Semantic Role Labeling with LTAG-based Features from Tree-Adjoining Grammars and Support Vector Machines using L,<TITLE>Using LTAG-based Features for Semantic Role Labeling<TITLE>Semantic Role Labeling with LTAG-based Features<TITLE>Semantic Role Labeling with LTAG-Based Features<TITLE>Semantic Role Labeling with LTAG-based Features<TITLE>Semantic Role Labeling with LTAG-based Features,<TITLE>Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE>Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling.<TITLE>Lexicalized Tree Adjoining Grammar for Semantic Role Labeling<TITLE>Lexicalized Tree-Adjoining Grammar<TITLE>Lexicalized Tree Adjoining Grammar,<TITLE> Lexicalized Tree-Adjoining Grammar Features for Semantic Role Labeling<TITLE> Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE> Lexicalized Tree-Adjoining Grammar Features Improve Semantic Role Labeling<TITLE> The Use of Lexicalized Tree-Adjoining Grammar<TITLE> Lexicalized Tree-Adjoining Grammar,<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalism for Improved SRL<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalisms for SRL<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalism for SRL Tasks<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalism for SRL<TITLE>Using Lexicalized Tree-Adjoining Grammar to Improve SRL Accuracy
98,ALLiS: a Symbolic Learning System for Natural Language Learning,"1 I n t r o d u c t i o n We present ALLiS, a learning system for identifying syntactic structures which uses theory refinement. When other learning techniques (symbolic or statistical) are widely used in Natural Language Learning, few applications use theory refinement (Abecker and Schmid, 1996), (Mooney, 1993). We would like to show that even a basic implementation of notions used in TR is enough to build an efficient machine learning system concerning the task of learning linguistic structures. ALLiS relies on the use of background knowledge and default values in order to build up an initial grammar and on the use of theory refinement in order to improve this grammar. This combination provides a good machine learning framework (efficient and fast) for Natural Language Learning. After presenting theory refinement (Section 2) and a general description of ALLiS (Section 3), we will show how each step of TR is applying in the specific case of learning linguistic structures (non-recursive phrases).",<TITLE>AlliS: A Machine Learning System for Linguistic Structures<TITLE>AlliS: Learning Linguistic Structures Using Theory Refinement<TITLE>AlliS: Theory Refinement for Learning Linguistic Structures<TITLE>AlliS: Learning Linguistic Structures with Theory Refinement<TITLE>AlliS: A Machine Learning System for Syntactic Structures,<TITLE>Learning Linguistic Structures Using Theory Refinement and Efficient Machine Learning for Natural Language Learning using Syntactic Structural Struct<TITLE>Learning Linguistic Structures Using Theory Refinement and Efficient Machine Learning for Natural Language Learning using Syntactic Structural Structure<TITLE>Learning Linguistic Structures Using Theory Refinement and Efficient Machine Learning for Natural Language Learning with Syntactic Structural Struct<TITLE>Learning Linguistic Structures Using Theory Refinement and Efficient Machine Learning for Natural Language Learning with Syntactic Structural Structure<TITLE>Learning Linguistic Structures Using Theory Refinement and Efficient Machine Learning for Natural Language Learning using Syntactic Structures.,<TITLE>AlliS: A Machine Learning System for Syntactic Structures Using Theory Refinement<TITLE>AlliS: A Machine Learning System for Syntactic Structures using Theory Refinement<TITLE>AlliS: A Machine Learning System for Syntactic Structures Using Theory Refinement<TITLE>AlliS: A Learning System for Syntactic Structures Using Theory Refinement<TITLE>A Machine Learning System for Syntactic Structures Using Theory Refinement,<TITLE>Identifying Syntactic Structures with Theory Raffinment<TITLE>Identifying Syntactic Structures using Theory Raffinment<TITLE>Identifying Syntactic Structures with Theory Refinement<TITLE>Identifying Syntactic Structures using Theory Raffination<TITLE>Identifying Syntactic Structures with Theory Raffination,<TITLE> Learning Linguistic Structures Using Theory Refinement<TITLE> ALLiS: A Learning System for Linguistic Structures<TITLE> ALLiS: A Learning System for Syntactic Structures<TITLE> ALLiS: A Learning System for Syntactic Structures Using Theory Refinement<TITLE> ALLiS: A Learning System for Syntactic Structures using Theory Refinement,<TITLE>AlliS: A Machine Learning System for Learning Linguistic Structures<TITLE>AlliS: A Machine Learning System for Linguistic Structures<TITLE>AlliS: A Machine Learning System for Syntactic Structures<TITLE>Learning Linguistic Structures Using Theory Refinement<TITLE>Learning Linguistic Structures using Theory Refinement
99,Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network,"Neural network models, based on the attentional encoder-decoder model, have good capability in abstractive text summarization. However, these models are hard to be controlled in the process of generation, which leads to a lack of key information. We propose a guiding generation model that combines the extractive method and the abstractive method. Firstly, we obtain keywords from the text by a extractive model. Then, we introduce a Key Information Guide Network (KIGN), which encodes the keywords to the key information representation, to guide the process of generation. In addition, we use a prediction-guide mechanism, which can obtain the long-term value for future decoding, to further guide the summary generation. We evaluate our model on the CNN/Daily Mail dataset. The experimental results show that our model leads to significant improvements.",<TITLE>Guiding Generation in Abstractive Text Summarization with Key Information Guide Networks<TITLE>Guiding Generation in Abstractive Text Summarization with Key Information Guide Network<TITLE>Guiding Generation of Abstractive Text Summarization with Key Information Guide Networks<TITLE>Guiding Generation for Abstractive Text Summarization with Key Information Guide Networks<TITLE>Guiding Generation of Abstractive Text Summarization with Key Information Guide Network,<TITLE>Key Information Guide Network for Abstractive Text Summarization: A Case Study on Keyword Generation with an Extractive Keyword<TITLE>Key Information Guide Network for Abstractive Text Summarization: A Case Study on Keyword Generation with an Extractive Neural Network<TITLE>Key Information Guide Network for Abstractive Text Summarization: A Case Study on Keyword Generation with an Extractive Model and<TITLE>Key Information Guide Network for Abstractive Text Summarization: A Case Study on Keyword Generation with an Extractive Neural Model<TITLE>Key Information Guide Network for Abstractive Text Summarization: A Case Study on Keyword Generation with an Extractive Model of,<TITLE>A Key Information Guide Network for Abstractive Text Summarization<TITLE>Key Information Guide Network for Abstractive Text Summarization<TITLE>Key Information Guide Network for Abstractive Text Summarization<TITLE>Key Information Guide Network for Abstractive Text Summarization<TITLE>Key Information Guide Network for Abstractive Text Summarization,<TITLE>Key Information Guide Networks for Abstractive Text Summarization<TITLE>Using Key Information Guide Networks for Text Summarization<TITLE>Key Information Guide Network for Abstractive Text Summarization<TITLE>Key Information Guide Networks for Text Summarization<TITLE>Key Information Guide Network for Text Summarization,<TITLE> Key Information Guide Network for Abstractive Text Summarization<TITLE> Key Information Guide Networks for Abstractive Text Summarization<TITLE> Key Information Guide Model for Abstractive Text Summarization<TITLE> Key Information Guide Network for Abstractive Summarization<TITLE> Key Information Guide Network for Text Summarization,<TITLE>Key Information Guide Network for Extractive Text Summarization<TITLE>Key Information Guide Network for Abstractive Text Summarization<TITLE>Key Information Guide Network for Text Summarization<TITLE>Guiding Generation for Abstractive Text Summarization<TITLE>Guiding Generation for Text Summarization
