,0,1,2,3,4,5,6
0,Learning Explicit Latent Semantic Annotations from Structured Tables and Texts bart_base,Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data,Grounded Language Learning with Latent Semantic Annotation bart_xsum,Learning Exact Latent Semantics from paired Structured Tables and Texts t5_small,Learning Explicit Latent Semantic Annotations for Grounded Language Learning pegasus_xsum, Latent Semantic Annotations for Grounded Language Learning gpt2,"Previous work on grounded language learning did not fully capture the semantics underlying the correspondences between structured world state representations and texts, especially those between numerical values and lexical terms. In this paper, we attempt at learning explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of values and texts. We model the joint probability of data fields, texts, phrasal spans, and latent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance. As a by-product, we leverage the induced annotations to extract templates for language generation. Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework."
1,Semi-Supervised and Semi-supervised Sense Disambiguation using Label Propagation and Clustering Based bart_cnn,Supervised and Semi-Supervised Sense Disambiguation Using Model Order Identification bart_base,Model Order Identification for Sense Disambiguation with Mixed Data bart_xsum,Partially Supervised Sense Disambiguation by Learning Sense Number from Tagged and Untagged Corpora,Model Order Identification for Undefined Sentiment Disambiguation t5_small,Model Order Identification for Semi-Supervised and Fully-Supervised Sense Annotation pegasus_xsum,"Supervised and semi-supervised sense disambiguation methods will mis-tag the instances of a target word if the senses of these instances are not defined in sense inventories or there are no tagged instances for these senses in training data. Here we used a model order identification method to avoid the misclassification of the instances with undefined senses by discovering new senses from mixed data (tagged and untagged corpora). This algorithm tries to obtain a natural partition of the mixed data by maximizing a stability criterion defined on the classification result from an extended label propagation algorithm over all the possible values of the number of senses (or sense number, model order). Experimental results on SENSEVAL-3 data indicate that it outperforms SVM, a one-class partially supervised classification algorithm, and a clustering based model order identification algorithm when the tagged data is incomplete."
2,Hawkes Processes for Classifying Temporal Textual Data Using Rumour Stance Classification of Social Media and the World Wide Web bart_cnn,Using Hawkes Processes to Classify Temporal Textual Data Sequences for Twitter Rumour Stance Classification bart_xsum,Hawkes Processes for Classifying Temporal Textual Data Sequences t5_small,Classifying Temporal Textual Data Using Hawkes Processes bart_base,Hawkes Processes for Classification of Temporal Textual Data pegasus_xsum,Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter,"Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content."
3,Learning Phrase Embeddings by Composing Word Representations for NLP bart_base,Learning Composition Models for Phrase Embeddings,Learning Word Embeddings for Phrase Structure and Context pegasus_xsum, Unsupervised Learning of Phrase Embeddings gpt2,Unsupervised Learning of Phrase Embeddings for NLP Tasks using Word Structure and Context from Large Datasets bart_cnn,Unsupervised Learning of Phrase Embedding Representations bart_xsum,"Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use."
4,Learning Visually Grounded Sentence Representations,Grounded Sentence Representations and Improved Word Embeddings bart_base, Learning Grounded Sentence Representations gpt2,Grounded Sentence Representations: An Empirical Comparison of Image Features and Word Embeddings bart_xsum,Structural Sentence Representations Using Sentence Encoding t5_small,Grounded Sentence Representations: An Investigation pegasus_xsum,"We investigate grounded sentence representations, where we train a sentence encoder to predict the image features of a given caption-i.e., we try to ""imagine"" how a sentence would be depicted visually-and use the resultant features as sentence representations. We examine the quality of the learned representations on a variety of standard sentence representation quality benchmarks, showing improved performance for grounded models over non-grounded ones. In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings."
5,Unsupervised Unfolding of Phrase-based Translation Model for Domain Adaptation bart_xsum,Unsupervised SMT Induction of Phrase-Based Translation Models from Monolingual Corpora with Application to Domain Adaptation bart_cnn,An Unsupervised Method for Domain Adaptation in Statistical Machine Translation bart_base, Unsupervised Phrase-Based Translation Model for Domain Adaptation gpt2,Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation,Inducing a Simple Word-Based Translation Model from the Monolingual Corpora t5_small,"Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality."
6,Learning to Generate Cohesive and Efficient Games using Content from Online Games bart_base,Generating Interactive Worlds with Text,Multiplayer Text Adventure Games with Compositional Neural Networks pegasus_xsum, Cohesive and Interesting World Creation Using Neural Networks gpt2,Machine Learning for World Creation using Multiplayer Text Adventure Game Environments t5_small,Learning to Generate Cohesive Games Using Neural Network-based Content from the Online World-Building Task in Multi-player Text bart_cnn,"Procedurally generating cohesive and interesting game environments is challenging and time-consuming. In order for the relationships between the game elements to be natural, common-sense has to be encoded into arrangement of the elements. In this work, we investigate a machine learning approach for world creation using content from the multiplayer text adventure game environment LIGHT (Urbanek et al. 2019). We introduce neural network based models to compositionally arrange locations, characters, and objects into a coherent whole. In addition to creating worlds based on existing elements, our models can generate new game content. Humans can also leverage our models to interactively aid in worldbuilding. We show that the game environments created with our approach are cohesive, diverse, and preferred by human evaluators compared to other machine learning based world construction algorithms."
7,Synthetic and Natural Noise Both Break Neural Machine Translation,Robust Character-Based Neural Machine Translation t5_small,Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise: A Case Study on Noisy Texts in N bart_cnn,Character-Based Neural Machine Translation Fails to Translate Noisy Texts pegasus_xsum,Character-Based Neural Machine Translation with Noisy Texts bart_base,Robust Neural Machine Translation with Synthetic and Natural Sources of Noisy Data bart_xsum,"Character-based neural machine translation (NMT) models alleviate out-ofvocabulary issues, learn morphology, and move us closer to completely end-toend translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise."
8,On the Universality of Online Mirror Descent,Optimal Regret Guarantees for Mirror Descent bart_xsum,Mirror Descent can always achieve an optimal regret guarantee pegasus_xsum,Optimal Regret Guarantees for Mirror Descent bart_base,Online Mirror Descent: Almost Optimal Regret Guarantee Guarantees for Convex Online Learning with Limited Supervision. bart_cnn, Optimal Regret Guarantees for Mirror Descent gpt2,"We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee."
9,Dropout Meets DropConnect: Regularizing Fully-Connected Neural Networks pegasus_xsum,DropConnect: Regularizing Fully-Connected Networks via Randomized Subset Selection and Multiple Drop-Connecting Data Aggregation bart_cnn,DropConnect: A Generalization of Dropout for Regularizing Neural Networks bart_base,DropConnect: A Generalization of Dropout in Neural Networks t5_small,Regularization of Neural Networks using DropConnect,DropConnect: A Generalization of Dropout for Fully-Connected Neural Networks bart_xsum,"We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models."
10, A Synchronous Parser Based on Two successive Monolingual Parses gpt2,Two monolingual parses are better than one (synchronous parse),A Two-Parse Synchronous Parsing Algorithm for Binary SCFG t5_small,A Two-Parallel Synchronous Parsing Algorithm Based on Successive Monolingual Parsing and Pruned Search bart_cnn,Efficient Synchronous Parsing with Two Syntactic Monolingual Parses pegasus_xsum,Efficient Synchronous Parsing with Two Monolingual Parses bart_xsum,"We describe a synchronous parsing algorithm that is based on two successive monolingual parses of an input sentence pair. Although the worst-case complexity of this algorithm is and must be O(n) for binary SCFGs, its average-case run-time is far better. We demonstrate that for a number of common synchronous parsing problems, the two-parse algorithm substantially outperforms alternative synchronous parsing strategies, making it efficient enough to be utilized without resorting to a pruned search."
11,Semantic Frame Identification with Distributed Representations of Predicates and their Syntactic Context bart_base,Semantic Frame Identification with Distributed Word Representations,Semantic Frame Identification with Distributed Representations of Predicates and Their Syntactic Context t5_small,Semantic Frame Identification Using Distributed Representations of Predicates and their Syntactic Context bart_xsum, Semantic Frame Identification Using Distributional Representations gpt2,Semantic Frame Identification using Syntactic and Semantic Representations of Predicates and their Syntactic Context Using Word Embeddings bart_cnn,"We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work."
12,Similarity Measures for Contextual Word Representations t5_small,Similarity Analysis of Contextual Word Representation Models, Contextual Word Representation Models gpt2,Comparing Neural Architectures of Contextual Word Representations pegasus_xsum,Similarity Analysis of Contextual Word Representations: A Case Study on Information Localization in the Lens of Similarity Analysis. bart_cnn,Similarity Analysis of Contextual Word Representations in Deep Learning bart_xsum,"This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks."
13,Dependency-based Categorial Grammars pegasus_xsum,Synthesizing Semantic Phenomena in Categorial Grammar bart_base,Coupling CCG and Hybrid Logic Dependency Semantics,Combinatory Categorial Grammar: A Dependency-Based Perspective on Language Meaning t5_small,Dependency-Based Categorial Grammar with Hybrid Logic bart_xsum, Compositional Semantics with Hybrid Logic and Situated Inference gpt2,"Categorial grammar has traditionally used the λ-calculus to represent meaning. We present an alternative, dependency-based perspective on linguistic meaning and situate it in the computational setting. This perspective is formalized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally."
14, A Sense-Level Lexicon for Opinion Inference gpt2,A Sense-Level Lexicon for Opinion Inference pegasus_xsum,+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference,A Graph-based Lexicon of +/--Effect Events bart_xsum,A Sense-Level Lexicon for Opinion Inference in NLP bart_base,A Sense-Level Lexicon of Events Using WordNet1 Relations and Gloss Information for Opinion Inference on Entities +/- bart_cnn,"Recently, work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (+/-effect events). This paper addresses methods for creating a lexicon of such events, to support such work on opinion inference. Due to significant sense ambiguity, our goal is to develop a sense-level rather than word-level lexicon. To maximize the effectiveness of different types of information, we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set."
15,Integrating Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis: A Case Study on Non-Expert Annotations bart_cnn, Improving Contextual Opinion Analysis Using Word Sense Disambiguation gpt2,Scaling up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis bart_xsum,Integrated Subjectivity Word Sentiment Disambiguation into Contextual Opinion Analysis t5_small,Improving the Impact of Subjectivity Word Sense Disambiguation on Contextual Opinion Analysis,Scaling up Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis pegasus_xsum,"Subjectivity word sense disambiguation (SWSD) is automatically determining which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD has been shown to improve the performance of contextual opinion analysis, but only on a small scale and using manually developed integration rules. In this paper, we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis."
16,Robust Zero-Shot Cross-Domain Slot Filling with Example Values,Robust and Robust Slot Filling for Task-Oriented Dialog Systems with Zero-shot Modeling of Sem bart_cnn,Learning Semantic Representations of Slot Filling Models for Task-Oriented Dialog Systems pegasus_xsum, Learning Semantic Representations of Slots gpt2,Learning Semantic Representations of Slots for Task-oriented Dialogue Systems t5_small,Robust Zero-Shot Slot Filling for Task-Oriented Dialog Systems bart_base,"Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting."
17,A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment,A Joint Latent Variable Model for Unsupervised Word Sense Induction bart_xsum,Joint Sense-Topic Modeling for Unsupervised Word Sense Induction with Latent Variables and Neural Word Embedd bart_cnn,A Sense-Topic Model for Word Sense Induction bart_base,A Sense-topic Model for Word Sensitive Induction t5_small,A Sense-topic Model for Word Sense Induction pegasus_xsum,"Word sense induction (WSI) seeks to automatically discover the senses of a word in a corpus via unsupervised methods. We propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly. Topics are informed by the entire document, while senses are informed by the local context surrounding the ambiguous word. We also discuss unsupervised ways of enriching the original corpus in order to improve model performance, including using neural word embeddings and external corpora to expand the context of each data instance. We demonstrate significant improvements over the previous state-of-the-art, achieving the best results reported to date on the SemEval-2013 WSI task."
18,A Probabilistic Model of Rumour Frequency Prediction Using Log-Gaussian Cox Processes and Multi-Task Learning in Social bart_cnn, Rumour Frequency Modeling Using Log-Gaussian Processes gpt2,A Probabilistic Model of rumour prevalence based on a Point Process bart_base,A Multi-Task Learning Approach to Predicting the Frequency of Rumours on Social Media pegasus_xsum,Point Process Modelling of Rumour Dynamics in Social Media,A Multi-Task Learning Model of rumour prevalence on social media t5_small,"Rumours on social media exhibit complex temporal patterns. This paper develops a model of rumour prevalence using a point process, namely a log-Gaussian Cox process, to infer an underlying continuous temporal probabilistic model of post frequencies. To generalize over different rumours, we present a multi-task learning method parametrized by the text in posts which allows data statistics to be shared between groups of similar rumours. Our experiments demonstrate that our model outperforms several strong baseline methods for rumour frequency prediction evaluated on tweets from the 2014 Ferguson riots."
19,Agnostic Active Learning: Confidence-Rate Prediction and Consistency of Label Complexity in an Agnostic Hypothesis bart_cnn, Agnostic Active Learning gpt2,Agnostic Active Learning with Confidence-rated Prediction bart_xsum,Agnostic Active Learning with Confidence-Ranking Prediction bart_base,Agnostic Active Learning with Disagreement-Based Models t5_small,Beyond Disagreement-Based Agnostic Active Learning,"We study agnostic active learning, where the goal is to learn a classifier in a prespecified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithm for this problem is disagreement-based active learning, which has a high label requirement. Thus a major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions; first, a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and second, a novel confidence-rated predictor."
20,Randomized Deep Reinforcement Learning with Monte Carlo Inference bart_xsum,A Randomized Neural Network for Deep Reinforcement Learning t5_small,Randomization and Adaptation of Deep Reinforcement Learning bart_base,Randomized Convolutional Neural Networks for Deep Reinforcement Learning pegasus_xsum,Pokaxpoka: Randomized Deep Reinforcement Learning with Robust Features Invariant Across Diverse and Randomized Environments bart_cnn,Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning,"Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose. Code is available at github.com/pokaxpoka/netrand."
21,An Improved EM Method for Unsupervised Word Sense Disambiguation pegasus_xsum,Unsupervised Word Sense Disambiguation Using Expectation Maximization Algorithm bart_xsum,Improving Word Sense Disambiguation Using ExpectationMaximization and EM Algorithm for Japanese Dictionary Task in Sense bart_cnn,Improving Unsupervised Learning for Noun Word Sense Disambiguation Problems bart_base,Unsupervised learning of word sense disambiguation rules by estimating an optimum iteration number in the EM algorithm, Improving Word Sense Disambiguation Using Expectation Maximization gpt2,"In this paper, we improve an unsupervised learning method using the ExpectationMaximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for verb WSD problems."
22,State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations,State Reification for Robust Neural Nets pegasus_xsum, State Reification for Neural Networks gpt2,State Reification for Robust Generalization in Neural Networks bart_base,State Reification for Robust Neural Net Generalization with Adversarial Training of Finite-Labeled Data via Hidden State bart_cnn,State Reification for Neural Nets to Generalize t5_small,"Machine learning promises methods that generalize well from finite labeled data. However, the brittleness of existing neural net approaches is revealed by notable failures, such as the existence of adversarial examples that are misclassified despite being nearly identical to a training example, or the inability of recurrent sequence-processing nets to stay on track without teacher forcing. We introduce a method, which we refer to as state reification, that involves modeling the distribution of hidden states over the training data and then projecting hidden states observed during testing toward this distribution. Our intuition is that if the network can remain in a familiar manifold of hidden space, subsequent layers of the net should be well trained to respond appropriately. We show that this state-reification method helps neural nets to generalize better, especially when labeled data are sparse, and also helps overcome the challenge of achieving robust generalization with adversarial training."
23,"A Fast, Compact, Accurate Model for Language Identification of Codemixed Text",Fine-Grained Multilingual Language Identification pegasus_xsum,Multilingual Language Identification with a Global Constrained Decoder bart_base,Fine-Grained Multilingual Language Identification with a Feed-Forward Network bart_xsum, Fine-Grained Multilingual Language Identification gpt2,Multilingual Language Identification with Feed-forward Networks t5_small,"We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents, social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification."
24,Unsupervised Bootstrapping of Text Categories Using Latent Semantic Space and Gaussian Mixture bart_xsum,A Generalized Bootstrapping Algorithm for Text Categorization Using Latent Semantic Space and Application to Unsupervised bart_cnn, Generalized Bootstrapping for Text Categorization gpt2,Unsupervised Bootstrapping of Text Categorization Using Latent Semantic Space bart_base,A Generalized Bootstrapping Algorithm t5_small,Investigating Unsupervised Learning for Text Categorization Bootstrapping,"We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-theart performance using only the category names as initial seeds."
25,A Probabilistic Model for Semantic Role Induction from Unannotated Text bart_xsum,Inducing Semantic Role Clusters with Manual Annotations t5_small,Unsupervised Induction of Semantic Roles, Inducing Semantic Roles from Unannotated Text gpt2,Inducing Semantic Roles from Text for Semantic Role Labeling bart_base,Inducing Semantic Roles of Verbal Arguments from Unannotated Text pegasus_xsum,"Datasets annotated with semantic roles are an important prerequisite to developing highperformance role labeling systems. Unfortunately, the reliance on manual annotations, which are both difficult and highly expensive to produce, presents a major obstacle to the widespread application of these systems across different languages and text genres. In this paper we describe a method for inducing the semantic roles of verbal arguments directly from unannotated text. We formulate the role induction problem as one of detecting alternations and finding a canonical syntactic form for them. Both steps are implemented in a novel probabilistic model, a latent-variable variant of the logistic classifier. Our method increases the purity of the induced role clusters by a wide margin over a strong baseline."
26,LastWords: The LastWords Column (Sproat 2010) - An Accurate Description of the Indus Script Debate in Science bart_cnn, Last Words: The Last Words Newsletter gpt2,The Indus script debate in LastWords t5_small,"Commentary and Discussion: Entropy, the Indus Script, and Language: A Reply to R. Sproat",Last Words: Correcting the Errors of Richard Sproat pegasus_xsum,Last Words: An Analysis of the Reviewing Practice of General Science Journal bart_base,"In a recent LastWords column (Sproat 2010), Richard Sproat laments the reviewing practices of “general science journals” after dismissing our work and that of Lee, Jonathan, and Ziman (2010) as “useless” and “trivially and demonstrably wrong.” Although we expect such categorical statements to have already raised some red flags in the minds of readers, we take this opportunity to present a more accurate description of our work, point out the straw man argument used in Sproat (2010), and provide a more complete characterization of the Indus script debate. A separate response by Lee and colleagues in this issue provides clarification of issues not covered here."
27,"Subjectivity, Information Locality, and Information Gain in English t5_small","Subjectivity, Information Gain, and Lexical Complexity: The Case of Adjective Ordering in English-to- bart_cnn",Predicting Order of Unseen Adjectives in English bart_base,What Makes for Good Adjectives? A Case Study in English pegasus_xsum,What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks,What determines the order of adjectives in English phrases such as big blue box? bart_xsum,"We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun. We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension. The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce. We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks. We find that subjectivity, information locality, and information gain are all strong predictors, with some evidence for a two-factor account, where subjectivity and information gain reflect a factor involving semantics, and information locality reflects collocational preferences."
28,Nash Equilibria in Multi-Agent Systems: A Nash Equilibrium Algorithm for Graphical Games and Applications to Multi bart_cnn,A Compact Algorithm for Computing Nash Equilibria in Graphical Games t5_small, Computing Equilibria in Graphical Games with Compact Representations gpt2,Computing Nash Equilibria in Multi-Agent Games pegasus_xsum,"An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games",Efficient Nash Equilibria for Multi-Agent Games bart_xsum,"We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games."
29,Algorithms for Optimizing the Ratio of Submodular Functions, Submodular Optimization with Bounded Approximation gpt2,Bridging the Gap Between Submodular Functions: A New Problem and Algorithms pegasus_xsum,Tight Bounds for Minimizing Ratio of Two Submodular Functions: Tightness Bounds and Scalable Algorith bart_cnn,Minimizing the Ratio of Two Submodular Functions bart_xsum,Submodular Ratio Minimization via Bounded Approximation bart_base,"We investigate a new optimization problem involving minimizing the Ratio of two Submodular (RS) functions. We argue that this problem occurs naturally in several real world applications. We then show the connection between this problem and several related problems including minimizing the difference between submodular functions (Iyer & Bilmes, 2012b; Narasimhan & Bilmes, 2005), and to submodular optimization subject to submodular constraints (Iyer & Bilmes, 2013). We show that RS optimization can be solved with bounded approximation factors. We also provide a hardness bound and show that our tightest algorithm matches the lower bound up to a log factor. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms."
30,An effective Discourse Parser that uses Rich Linguistic Information, Modeling Discourse Structure with Compositional Semantics gpt2,First-Order Logical Learning for Rhetorical Relation Classification t5_small,First-Order Logic Learning for Discourse Relation Classification bart_xsum,Exploiting Compositional Semantics and Discourse Structure Data for Rhetorical Relations bart_base,A Logic Learning Approach to Determine Rhetorical Relations between Discourse Segments with Compositional Semantics and Discourse Structure bart_cnn,"This paper presents a first-order logic learning approach to determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report a statistically significant improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIPPER and Naive Bayes. For discourse parsing, our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline."
31,Context-Aware and Context-Agnostic Models for Detecting Vague Content in Privacy Policies t5_small,Identifying Inconsistently Grown Words and Sentences in Privacy Policies bart_base,Detecting and Improving the User Experience of Privacy Policies pegasus_xsum, Identifying Discrete Words and Sentences in Privacy Policies gpt2,Identifying vague content in private policies for detecting vague words and sentences using auxiliary-classifier generative adversarial networks. bart_cnn,Automatic Detection of Vague Words and Sentences in Privacy Policies,"Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, privacy policies are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in privacy policies. We construct the first corpus of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving vagueness and improving the usability of privacy policies."
32, Syntactic Syntactic SMT with Soft Features gpt2,Syntactic SMT with Bilingual Translation Features bart_xsum,A Discriminative Target Sentence Generation System for Syntactic Statistical Machine Translation bart_base,Syntactic SMT Using a Discriminative Text Generation Model,A Novel Architecture for Syntactic Machine Translation with BLEU-based Syntax Features and Bilingual Translation Features from IW bart_cnn,Syntactic Machine Translation with Soft Syntax Features pegasus_xsum,"We study a novel architecture for syntactic SMT. In contrast to the dominant approach in the literature, the system does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems."
33,Relation-Level Distant Supervision with Relation Extraction Models bart_base, Improving Distant Supervision for Relation Extraction gpt2,Multi-Instance Distant Supervision for Relation Extraction with Application to Sentence-Level Relation Level Relation Classification bart_cnn,Multi-Instance Multi-Label Models for Distant Supervision t5_small,Learning Relation Extraction from Distant Supervision with Multi-Instance Multi-label Models bart_xsum,Infusion of Labeled Data into Distant Supervision for Relation Extraction,"Distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models. However, in some cases a small amount of human labeled data is available. In this paper, we demonstrate how a state-of-theart multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve."
34,Information Gain: High-dimensional feature selection for text classification using round-robin Scheduling and Pitfall Problem Solving Problems bart_cnn,Applying Round-robin Scheduling to Information Gain for Text Classification bart_base,A Pitfall and Solution in Multi-Class Feature Selection for Text Classification ,Round-robin Scheduling for High-Dimensional Feature Selection in Text Classification bart_xsum,Round-robin Scheduling for High-dimensional Feature Selection t5_small, A Pitfall-Free Approach to Feature Selection for Text Classification gpt2,"Information Gain is a well-known and empirically proven method for high-dimensional feature selection. We found that it and other existing methods failed to produce good results on an industrial text classification problem. On investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes. In this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task. Based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods. Empirical evaluation on 19 datasets shows substantial improvements."
35,Neighborhood Representations for Knowledge Base Completion: A Mixture of Its Neighborhood in the Knowledge Base and TransE Model bart_cnn,Neighborhood Mixture Model for Knowledge Base Completion,TransE: Embedding Neighborhoods for Knowledge Base completion t5_small, Entity Representations for Knowledge Base Completion gpt2,A Novel Entity Representation as a mixture of its neighborhood in knowledge base pegasus_xsum,Neighborhood Information Improves TransE: A Knowledge Base Completion Model bart_xsum,"Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE—a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks."
36,TypeSQL: Slot Filling for Relational Database Query Retrieval via Schema Type-SQL Formalization and Information bart_cnn, TypeSQL: A Slot Filling Approach for Relational Databases gpt2,TypeSQL: A Slot Filling Approach to Interacting with Relational Databases t5_small,TypeSQL: A Slot Filling Approach to Interacting with Relational Databases bart_base,Type Interact: Learning to Interact with Natural Language via Slot Filling and Type Information pegasus_xsum,TypeSQL: Knowledge-Based Type-Aware Neural Text-to-SQL Generation,"Interacting with relational databases through natural language helps users with any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper, we present a novel approach TypeSQL which formats the problem as a slot filling task in a more reasonable way. In addition, TypeSQL utilizes type information to better understand rare entities and numbers in the questions. We experiment this idea on the WikiSQL dataset and outperform the prior art by 6% in much shorter time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL can reach 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model."
37,Transition-based Neural Constituent Parsing, Neural Constituent Parsing with Rich Syntactic Features gpt2,Transition-based Neural Constituent Parsing with Rich Syntactic Structures and Memory of Long Heterosyllabic bart_cnn,Transition-Based Neural Constituent Parsing with Neural Network Structure bart_base,Transition-based Neural Constituent Parsing with Stack and Queue pegasus_xsum,Transition-based Constituent Parsing with Neural Networks t5_small,"Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters."
38,cpSGD: Communication-efficient and differentially-private distributed SGD, Communication Efficient Distributed Gradient Descent gpt2,Communication Efficiency and Differential Privacy in Distributed Stochastic Gradient Descent using O(log Log(nd)) bart_cnn,Communication Efficiency and Differential Privacy in Distributed Learning. t5_small,Communication Efficient Distributed Stochastic Gradient Descent for Mobile Devices bart_xsum,Communication Efficiency and Differential Privacy for Distributed Stochastic Gradient Descent bart_base,"Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For d variables and n ⇡ d clients, the proposed method uses O(log log(nd)) bits of communication per client per coordinate and ensures constant privacy. We also improve previous analysis of the Binomial mechanism showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest."
39, Estimating the Range of Skill of a Two-Player Game gpt2,Estimating the Range of Skill in a Competitive Two-Player Game: Some Asymptotic Bounds and General Asym bart_cnn,On the Range of Skill of Natural Games pegasus_xsum,Estimating the Range of Skill of a Natural Game Tree bart_base,On Range of Skill,Estimating the Range of Skill of a Natural Game bart_xsum,"At AAAI’07, Zinkevich, Bowling and Burch introduced the Range of Skill measure of a two-player game and used it as a parameter in the analysis of the running time of an algorithm for finding approximate solutions to such games. They suggested that the Range of Skill of a typical natural game is a small number, but only gave heuristic arguments for this. In this paper, we provide the first methods for rigorously estimating the Range of Skill of a given game. We provide some general, asymptotic bounds that imply that the Range of Skill of a perfectly balanced game tree is almost exponential in its size (and doubly exponential in its depth). We also provide techniques that yield concrete bounds for unbalanced game trees and apply these to estimate the Range of Skill of Tic-Tac-Toe and Heads-Up Limit Texas Hold’em Poker. In particular, we show that the Range of Skill of Tic-Tac-Toe is more than 100,000."
40,Stack-propagation: Improved Representation Learning for Syntax,Learning Stack-propagation for Dependency Parsing and Tagging with POS Taggers from Hand-tuned Templates bart_cnn,Part-of-Speech Models for Dependency Parsing and Tagging with Stack Propagation bart_xsum,Stack-Propagation: Learning POS-based Representations for Dependency Parsing pegasus_xsum,Learning Part-of-Speech Representations from Hand-Tuned Templates t5_small, Stack-propagation: Learning Syntax Models with Rich Representations gpt2,"Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call “stack-propagation”. We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model."
41,Dynamic Feature Selection for Dependency Parsing t5_small, Dynamic Feature Selection for Graph-based Dependency Parsing gpt2,Dynamic Feature Selection for Graph-based Dependency Parsing pegasus_xsum,Dynamic Feature Selection for Graph-based Dependency Parsing with Imitation Learning and Sequential Decision-Making Techniques on 7 Languages bart_cnn,Dynamic Feature Selection for Dependency Parsing with Imitation Learning bart_xsum,Dynamic Feature Selection for Dependency Parsing,"Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates."
42,Fine-Grained Opinion Extraction with Joint Inference pegasus_xsum,Joint Inference for Fine-grained Opinion Extraction: Identifying Opinion Entities and Relations in a Pipelined bart_cnn, Joint Inference for Fine-Grained Opinion Extraction gpt2,Joint Inference for Finegrained Opinion Extraction t5_small,Joint Inference for Fine-Grained Opinion Extraction bart_base,Joint Inference for Fine-grained Opinion Extraction,"This paper addresses the task of finegrained opinion extraction – the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction."
43,Evaluating the Performance of Twitter Sentiment Similarity Measurement for Word Polarity Classification: An Empirical Study on bart_cnn,An Empirical Study on the Performance of Twitter Data for Word Polarity Classification pegasus_xsum,Is Twitter A Better Corpus for Measuring Sentiment Similarity?,Evaluating the Performance of Different Corpora in Twitter Sentiment Similarity Measurement bart_base,Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement for Word Polarity Classification bart_xsum, Comparison of Corpora for Word Polarity Classification gpt2,"Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods."
44,Homophonic Pun Generation with Lexically Constrained Rewriting,Punning: A Computational Approach to Pun Modeling bart_base,Generating a Pun Sentence given a pair of homophones t5_small, Generating Pun Sentences with Explicit and Negative Constraints gpt2,Generating Pun Sentences with Semantic Incongruity pegasus_xsum,Punning with Explicit Positive and Negative Constraints: An Error Analysis of Pun Generation for Novelty-Based Dialogues bart_cnn,"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models."
45, Cross-lingual Semantic Relatedness Using Wikipedia gpt2,Crosslingual Semantic Relatedness Using Wikipedia Sources bart_base,Crosslingual Semantic Relatedness Using Wikipedia Information t5_small,Crosslingual Semantic Relatedness Using Wikipedia Links pegasus_xsum,Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge,Crosslingual Semantic Relatedness Using Wikipedia Links bart_xsum,"In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness."
46,Learning Optimal Message Policies for Text Generation in Games pegasus_xsum,Learning What to Talk about in Descriptive Games, A Markov Decision Process for Discourse Generation gpt2,A Markov Decision Process for Text Generation Using Planning t5_small,Markov Decision Processes for Text Generation in Descriptive Games: An Optimal Message Policy can be Defined and Simulation bart_cnn,Text Generation as a Markov Decision Process bart_base,"Text generation requires a planning module to select an object of discourse and its properties. This is specially hard in descriptive games, where a computer agent tries to describe some aspects of a game world. We propose to formalize this problem as a Markov Decision Process, in which an optimal message policy can be defined and learned through simulation. Furthermore, we propose back-off policies as a novel and effective technique to fight state dimensionality explosion in this framework."
47,Reranking Sequence-to-Sequence Dialogue Generation with Linguistic Information pegasus_xsum,Reranking Sequence-to-Sequence Models for Dialog Generation t5_small,Linguistically-Driven Sequence-to-Sequence Model for Improved Response Informativeness and Reasonableness bart_base,Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation,Linguistically Motivated Informativeness and Plausibility Reranking for Sequence-to-Sequence Dialogue Generation bart_xsum,Linguistically-Driven Response Informativeness in Open-Domain Dialogue Generation Models with Frequency-Related Intrinsic bart_cnn,"Sequence-to-sequence models for open-domain dialogue generation tend to favor generic, uninformative responses. Past work has focused on word frequency-based approaches to improving specificity, such as penalizing responses with only common words. In this work, we examine whether specificity is solely a frequency-related notion and find that more linguistically-driven specificity measures are better suited to improving response informativeness. However, we find that forcing a sequence-to-sequence model to be more specific can expose a host of other problems in the responses, including flawed discourse and implausible semantics. We rerank our model's outputs using externally-trained classifiers targeting each of these identified factors. Experiments show that our final model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses."
48, TASER: The Team Agent Simulator for Efficient Exploration gpt2,A Benchmark for Cooperative Learning Agents,TASER: Team Agent Simulator for Efficient Research in Cooperative Multi-Agent Systems with Application to Collaborative Task Labeling bart_cnn,TASER: The Team Agent Simulator for Efficient Research bart_xsum,TASER: A Team Agent Simulator for Efficient Research t5_small,TASER: The Team Agent Simulator for Efficient Multi-Agent Learning pegasus_xsum,"Introduction Cooperative multi-agent systems are of current interest due to their relevance to both robotics and networking. Researchers often create machine learning environments to explore these domains; however, the lack of reuse of previous environments prevents comparisons between the works of research groups. Further, while a growing understanding of the relationships between problem domains and machine learning techniques has emerged over time, there have been few attempts to systematically measure the performance of machine learning techniques as domain characteristics vary. Our primary goal is to create TASER, the Team Agent Simulator for Efficient Research—a non-trivial simulation system aimed at the efficient comparison of machine learning algorithms given a wide variety of conditions, concentrating on cooperative tasks. Our secondary goal is the adoption of TASER by other researchers."
49,Minimum Robust Multi-Submodular Cover for Fairness: Information Propagation and Bicriteria Approximation bart_cnn,Minimum Robust Multi-Submodular Cover for Fairness bart_xsum,Minimum Robust Multi-Submodular Cover for Fairness,Minimum Robust Multi-Submodular Cover for Fairness bart_base,Minimum Robust Multi-Submodular Cover for Fairness pegasus_xsum, Minimum Robust Multi-Submodular Cover for Fairness gpt2,"In this paper, we study a novel problem, Minimum Robust Multi-Submodular Cover for Fairness (MINRF), as follows: given a ground set V ; m monotone submodular functions f1, ..., fm; m thresholds T1, ..., Tm and a non-negative integer r, MINRF asks for the smallest set S such that for all i ∈ [m], min|X|≤r fi(S \X) ≥ Ti. We prove that MINRF is inapproximable within (1− ) lnm; and no algorithm, taking fewer than exponential number of queries in term of r, is able to output a feasible set to MINRF with high certainty. Three bicriteria approximation algorithms with performance guarantees are proposed: one for r = 0, one for r = 1, and one for general r. We further investigate our algorithms’ performance in two applications of MINRF, Information Propagation for Multiple Groups and Movie Recommendation for Multiple Users. Our algorithms have shown to outperform baseline heuristics in both solution quality and the number of queries in most cases."
