,title,abstract,bart_base,bart_cnn,bart_xsum,t5_small,gpt2,pegasus_xsum
0,Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data,"Previous work on grounded language learning did not fully capture the semantics underlying the correspondences between structured world state representations and texts, especially those between numerical values and lexical terms. In this paper, we attempt at learning explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of values and texts. We model the joint probability of data fields, texts, phrasal spans, and latent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance. As a by-product, we leverage the induced annotations to extract templates for language generation. Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework.",<TITLE>Learning Explicit Latent Semantic Annotations from Structured Tables and Texts<TITLE>Learning Latent Semantic Annotations from Structured Tables and Texts<TITLE>Learning Explicit Latent Semantics from Structured Tables and Texts<TITLE>Latent Semantic Annotation for Grounded Language Generation<TITLE>Learning Explicit Latent Semantics for Grounded Language Generation,<TITLE>Learning Explicit Latent Semantic Annotations from Structured Tables and Texts with an Adapted Semi-hidden Markov Model for<TITLE>Learning Explicit Latent Semantic Annotations from Structured Tables and Texts with an Adapted Semi-hidden Markov Model for Ground<TITLE>Learning Explicit Latent Semantic Annotations from Structured Tables and Texts with an Adapted Semi-Hidden Markov Model for Ground<TITLE>Learning Explicit Latent Semantic Annotations from Structured Tables and Texts with an Adapted Semi-Hidden Markov Model for<TITLE>Learning Explicit Latent Semantic Annotations from Structured Tables and Texts with an Adapted Markov Model for Grounded Language,<TITLE>Grounded Language Learning with Latent Semantic Annotation<TITLE>Grounded Language Learning with Latent Semantic Annotations<TITLE>Grounded Language Learning with Latent Semantic Annotation<TITLE>Grounded Language Learning with Latent Semantic Annotations<TITLE>Grounded Language Learning with Latent Semantic Annotations,<TITLE>Learning Exact Latent Semantics from paired Structured Tables and Texts<TITLE>Learning Exact Latent Semantic Annotations from paired Structured Texts<TITLE>Learning Latent Semantic Annotations from paired Structured Texts<TITLE>Learning explicit latent semantic annotations from paired structured tables and texts<TITLE>Learning explicit semantic annotations from paired structured tables and texts,<TITLE> Latent Semantic Annotations for Grounded Language Learning<TITLE> Learning Explicit Latent Semantic Annotations from Paired Tables and Texts<TITLE> Learning Explicit Latent Semantic Annotations from Packed Tables and Texts<TITLE> Learning Explicit Latent Semantic Annotation from Paired Tables and Texts<TITLE> Learning Explicit Latent Semantic Annotations from Paired Table and Text,<TITLE>Learning Explicit Latent Semantic Annotations for Grounded Language Learning<TITLE>Learning Explicit Latent Semantic Annotations for Grounded Language Generation<TITLE>Learning Latent Semantic Annotations from Pairwise Structured Tables and Text<TITLE>Explicit Latent Semantic Annotations for Grounded Language Learning<TITLE>Explicit Latent Semantic Annotations for Grounded Language Generation
1,Partially Supervised Sense Disambiguation by Learning Sense Number from Tagged and Untagged Corpora,"Supervised and semi-supervised sense disambiguation methods will mis-tag the instances of a target word if the senses of these instances are not defined in sense inventories or there are no tagged instances for these senses in training data. Here we used a model order identification method to avoid the misclassification of the instances with undefined senses by discovering new senses from mixed data (tagged and untagged corpora). This algorithm tries to obtain a natural partition of the mixed data by maximizing a stability criterion defined on the classification result from an extended label propagation algorithm over all the possible values of the number of senses (or sense number, model order). Experimental results on SENSEVAL-3 data indicate that it outperforms SVM, a one-class partially supervised classification algorithm, and a clustering based model order identification algorithm when the tagged data is incomplete.",<TITLE>Supervised and Semi-Supervised Sense Disambiguation Using Model Order Identification<TITLE>Supervised and Semi-Supervised Sense Disambiguation with Mixed Data.<TITLE>Supervised and Semi-Supervised Sense Disambiguation with Mixed Data<TITLE>Supervised and Semi-supervised Sense Disambiguation with Mixed Data<TITLE>Supervised and Semi-Supervised Sense Disambiguation Using Mixed Data,<TITLE>Semi-Supervised and Semi-supervised Sense Disambiguation using Label Propagation and Clustering Based<TITLE>Semi-Supervised and Semi-supervised Sense Disambiguation Using Label Propagation and Clustering Based<TITLE>Semi-Supervised and Semi-supervised Sense Disambiguation using Label Propagation and Model Order Identification.<TITLE>Semi-Supervised and Semi-supervised Sense Disambiguation Using Label Propagation and Model Order Identification.<TITLE>Semi-Supervised and Semi-supervised Sense Disambiguation using Label Propagation and Clustering based,<TITLE>Model Order Identification for Sense Disambiguation with Mixed Data<TITLE>Model Order Identification for Sense Disambiguation Using Mixed Data<TITLE>Modeling Order Identification for Sense Disambiguation<TITLE>Model Order Identification for Sense Disambiguation<TITLE>Model Order Identification for Sense Disambiguation,<TITLE>Model Order Identification for Undefined Sentiment Disambiguation<TITLE>Model Order Identification of Undefined Instances with Mixed Data<TITLE>Model Order Identification for Sense Disambiguation with Mixed Data<TITLE>Model Order Identification for Undefined Instances with Mixed Data<TITLE>Model Order Identification for Sense Disambiguation,<TITLE> Unsupervised and Semi-Supervised Sense Disambiguation Using Label Propagation<TITLE> Unsupervised and Semi-Supervised Sense Disambiguation using Label Propagation<TITLE> Unsupervised Sense Disambiguation Using Label Propagation<TITLE> Semi-Supervised Sense Disambiguation Using Label Propagation<TITLE> Unsupervised and Semi-Supervised Sense Disambiguation via Label Propagation,<TITLE>Model Order Identification for Semi-Supervised and Fully-Supervised Sense Annotation<TITLE>Model Order Identification for Incomplete Semi-Supervised and Fully-Supervised Sense Classification<TITLE>Model Order Identification for Semi-Supervised and Fully-Supervised Sense Parser<TITLE>Model Order Identification for Incomplete Supervised and Semi-Supervised Sense Classification<TITLE>Model Order Identification for Semi-Supervised and Fully-Supervised Sense Classification
2,Hawkes Processes for Continuous Time Sequence Classification: an Application to Rumour Stance Classification in Twitter,"Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content.",<TITLE>Classifying Temporal Textual Data Using Hawkes Processes<TITLE>Using Hawkes Processes for Temporal Text Classification on Twitter<TITLE>Using Hawkes Processes to Classify Temporal Texts<TITLE>Using Hawkes Processes for Temporal Text Classification<TITLE>Hawkes Processes for Temporal Text Classification,<TITLE>Hawkes Processes for Classifying Temporal Textual Data Using Rumour Stance Classification of Social Media and the World Wide Web<TITLE>Hawkes Processes for Classifying Temporal Textual Data Using Rumour Stance Classification of Social Media and the Web as a<TITLE>Hawkes Processes for Classifying Temporal Textual Data Using Rumour Stance Classification of Social Media and the Web using Haw<TITLE>Hawkes Processes for Classifying Temporal Textual Data Using Rumour Stance Classification of Social Media and the Web as an<TITLE>Hawkes Processes for Classifying Temporal Textual Data Using Rumour Stance Classification of Social Media and the Web's Information,<TITLE>Using Hawkes Processes to Classify Temporal Textual Data Sequences for Twitter Rumour Stance Classification<TITLE>Using Hawkes Processes to Classify Temporal Textual Data Sequences for Rumour Stance Classification<TITLE>Using Hawkes Processes to Classify Temporal Textual Data Sequences of Twitter<TITLE>Using Hawkes Processes to Classify Temporal Textual Data in Social Media<TITLE>Using Hawkes Processes for Classifying Temporal Textual Data Sequences,<TITLE>Hawkes Processes for Classifying Temporal Textual Data Sequences<TITLE>Hawkes Processes for Temporal Textual Data Classification<TITLE>Hawkes Processes for Temporal and Textual Data Classification<TITLE>Hawkes Processes for Temporal Textual Data Classification<TITLE>Hawkes Processes for Classification of Temporal Textual Data,<TITLE> Using Hawkes Processes for Classifying Temporal Textual Data<TITLE> Hawkes Processes for Classifying Temporal Textual Data<TITLE> Hawkes Processes for Temporal Textual Classification<TITLE> Hawkes Processes for Classifying Sequences of Temporal Data<TITLE> Hawkes Processes for Temporal Text Classification,<TITLE>Hawkes Processes for Classification of Temporal Textual Data<TITLE>Hawkes Processes for Temporal Sequence Classification of Twitter Data<TITLE>Hawkes Processes for Classification of Temporal Text Sequences<TITLE>Hawkes Processes for Classification of Temporal Data Sequences<TITLE>Hawkes Processes for Temporal Text Classification
3,Learning Composition Models for Phrase Embeddings,"Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use.",<TITLE>Learning Phrase Embeddings by Composing Word Representations for NLP<TITLE>Learning Phrase Embeddings by Composing Word Representations with Context<TITLE>Learning Phrase Embeddings by Composing Word Representations<TITLE>Unsupervised Phrase Embeddings for NLP<TITLE>Learning Phrase Embeddings for NLP,<TITLE>Unsupervised Learning of Phrase Embeddings for NLP Tasks using Word Structure and Context from Large Datasets<TITLE>Unsupervised Learning of Phrase Embeddings for NLP Tasks Using Word Structure and Context from Large Datasets<TITLE>Unsupervised Learning of Phrase Embeddings for NLP Tasks using Phrase Structure and Context from Large Datas<TITLE>Unsupervised Learning of Phrase Embeddings for NLP Tasks using Word Structure and Context from Large Dataset<TITLE>Unsupervised Learning of Phrase Embeddings for NLP Tasks with Context-Aware Distributed Representation Learning,<TITLE>Unsupervised Learning of Phrase Embedding Representations<TITLE>Unsupervised Learning of Phrase Embeddings<TITLE>Unsupervised Learning of Phrase Embeddings<TITLE>Unsupervised Learning of Phrase Embeddings<TITLE>Learning to Construct Phrase Embeddings,<TITLE>Learning Word Embeddings for Phrase Structure and Context<TITLE>Learning Phrase Embeddings for NLP Tasks<TITLE>Unsupervised Phrase Embeddings for Words<TITLE>Learning Phrase Embeddings for Words<TITLE>Learning Word Embeddings for Phrases,<TITLE> Unsupervised Learning of Phrase Embeddings<TITLE> Unsupervised Phrase Embeddings<TITLE> Unsupervised Phrase Embedding Learning<TITLE> Learning Phrase Embeddings from Word Embeddings<TITLE> Unsupervised Learning of Phrase Embeddings Using Features,<TITLE>Learning Word Embeddings for Phrase Structure and Context<TITLE>Learning Phrase Embeddings by Compositional Language Modeling<TITLE>Learning Word Embeddings for Phrase-Length Text<TITLE>Learning Phrase Embeddings by Compositional Language Models<TITLE>Learning Phrase Embeddings with Structure and Context
4,Learning Visually Grounded Sentence Representations,"We investigate grounded sentence representations, where we train a sentence encoder to predict the image features of a given caption-i.e., we try to ""imagine"" how a sentence would be depicted visually-and use the resultant features as sentence representations. We examine the quality of the learned representations on a variety of standard sentence representation quality benchmarks, showing improved performance for grounded models over non-grounded ones. In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings.",<TITLE>Grounded Sentence Representations and Improved Word Embeddings<TITLE>Grounded Sentence Representations: An Empirical Study<TITLE>Learning Grounded Sentence Representations<TITLE>Grounded Sentence Representation Learning<TITLE>Grounded Sentence Representations,<TITLE>Grounded Sentence Representations for Grounded Grounding: A Case Study in Non-Grounded Text Representation Quality Est<TITLE>Grounded Sentence Representations for Grounded Grounding: A Case Study in Non-Grounded Text Representation Learning.<TITLE>Grounded Sentence Representations for Grounded Grounding: A Case Study in Non-Grounded Text Representation Quality Evaluation<TITLE>Grounded Sentence Representations for Grounded Grounding: A Case Study in Non-Grounded Text Representation Quality Assessment<TITLE>Grounded Sentence Representations for Grounded Grounding: A Case Study in Non-Grounded Text Representation Quality Measures,<TITLE>Grounded Sentence Representations: An Empirical Comparison of Image Features and Word Embeddings<TITLE>Grounded Sentence Representations: An Empirical Study on Image Features and Word Embeddings<TITLE>Grounded Sentence Representations: A Case Study on Image Features and Word Embeddings<TITLE>Grounded Sentence Representations: An Empirical Investigation<TITLE>Grounded Sentence Representations: An Empirical Study,<TITLE>Structural Sentence Representations Using Sentence Encoding<TITLE>Structural Sentence Representations Using Sentence Coding<TITLE>Structural Sentence Representations: Learning to Predict Image Features<TITLE>Structural Sentence Representations<TITLE>Grounding for Sentence Representations,<TITLE> Learning Grounded Sentence Representations<TITLE> Grounded Sentence Representations for Grounded Visual Reasoning<TITLE> Grounded Sentence Representations for Grounded Image Representation<TITLE> Improving Grounded Sentence Representations with Sentence Features<TITLE> Grounded Sentence Representations for Grounded Caption Generation,<TITLE>Grounded Sentence Representations: An Investigation<TITLE>Grounded Sentence Representations: An Evaluation<TITLE>Learning Grounded Sentence Representations<TITLE>Grounding Sentence Representations<TITLE>Grounded Sentence Representations
5,Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation,"Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality.",<TITLE>An Unsupervised Method for Domain Adaptation in Statistical Machine Translation<TITLE>An Unsupervised Approach to Domain Adaptation in Statistical Machine Translation<TITLE>An Unsupervised Method for Domain Adaptation in Machine Translation<TITLE>Unsupervised Domain Adaptation of Statistical Machine Translation Models<TITLE>Unsupervised Domain Adaptation for Statistical Machine Translation,<TITLE>Unsupervised SMT Induction of Phrase-Based Translation Models from Monolingual Corpora with Application to Domain Adaptation<TITLE>Unsupervised SMT Induction of Phrase-Based Translation Models from Monolingual Corpora with Application to Domain Adapt<TITLE>Unsupervised SMT Induction of Phrase-Based Translation Models from Monolingual Corpora for Domain Adaptation in<TITLE>Unsupervised SMT Induction of Phrase-Based Translation Models from Monolingual Corpora for Domain Adaptation with<TITLE>Unsupervised SMT Induction of Phrase-based Translation Models from Monolingual Corpora for Domain Adaptation in,<TITLE>Unsupervised Unfolding of Phrase-based Translation Model for Domain Adaptation<TITLE>Domain Adaptation of Statistical Machine Translation with Monolingual Corpora<TITLE>Domain Adaptation for Statistical Machine Translation with Monolingual Corpora<TITLE>Domain Adaptation of Statistical Machine Translation Using Unsupervised SMT<TITLE>Domain Adaptation of Statistical Machine Translation using Unsupervised SMT,<TITLE>Inducing a Simple Word-Based Translation Model from the Monolingual Corpora<TITLE>Inducing a Phrase-Based Translation Model from the Monolingual Corpora<TITLE>Inducing a Simple Word-Based Translation Model from the Monolingual corpora<TITLE>Inducing a Simple Word-Based Translation Model from Monolingual Corpora<TITLE>Inducing a Phrase-Based Translation Model from Monolingual Corpora,<TITLE> Unsupervised Phrase-Based Translation Model for Domain Adaptation<TITLE> Unsupervised Phrase-Based Translation Model From Monolingual Corpora<TITLE> Unsupervised Phrase-Based Translation Model<TITLE> Unsupervised Phrase-Based Translation Model Using Monolingual Corpora<TITLE> Unsupervised Phrase-Based Translation Model for Unsupervised Machine Translation,<TITLE>Unsupervised Phrase-Based Translation Model from the Monolingual Corpora<TITLE>Unsupervised Phrase-Based Translation Model from Monolingual Corpora<TITLE>Unsupervised Statistical Machine Translation from Monolingual Corpora for Domain Adaptation<TITLE>Unsupervised Statistical Machine Translation from Monolingual Corpora<TITLE>Improving Statistical Machine Translation with Monolingual Corpora
6,Generating Interactive Worlds with Text,"Procedurally generating cohesive and interesting game environments is challenging and time-consuming. In order for the relationships between the game elements to be natural, common-sense has to be encoded into arrangement of the elements. In this work, we investigate a machine learning approach for world creation using content from the multiplayer text adventure game environment LIGHT (Urbanek et al. 2019). We introduce neural network based models to compositionally arrange locations, characters, and objects into a coherent whole. In addition to creating worlds based on existing elements, our models can generate new game content. Humans can also leverage our models to interactively aid in worldbuilding. We show that the game environments created with our approach are cohesive, diverse, and preferred by human evaluators compared to other machine learning based world construction algorithms.",<TITLE>Learning to Generate Cohesive and Efficient Games using Content from Online Games<TITLE>Towards Cohesive and Efficient World Construction in Text-Adventure Games<TITLE>Towards Cohesive and Efficient World Construction in Online Games<TITLE>Learning to Generate Cohesive and Efficient Games from Online Games<TITLE>Towards Cohesive and Efficient World Construction in Text Games,<TITLE>Learning to Generate Cohesive Games Using Neural Network-based Content from the Online World-Building Task in Multi-player Text<TITLE>Learning to Generate Cohesive Games Using Neural Network-based Content from the Online World-Building Game Environment in Multi-player<TITLE>Learning to Generate Cohesive Games Using Neural Network-based Content from the Online World-Building Task in Multi-Agent Game<TITLE>Learning to Generate Cohesive Games Using Neural Network-based Content from the Online World-Building Task in Multi-Game En<TITLE>Learning to Generate Cohesive Games Using Neural Network-based Content from the Online World-Building Task in Multi-player Game,<TITLE>Creating Cohesive and Diverse Game Environments with Neural Networks<TITLE>Creating Cohesive and Diverse Game Environments using Neural Networks<TITLE>Creating Cohesive and Diverse Game Environments Using Neural Networks<TITLE>Creating Cohesive and Interesting Game Environments with Neural Networks<TITLE>Neural World Construction with LIGHT,<TITLE>Machine Learning for World Creation using Multiplayer Text Adventure Game Environments<TITLE>A Machine Learning Approach for World Creation using Multiplayer Text Adventure Games<TITLE>Machine Learning for World Creation using Multiplayer Text Adventure Games<TITLE>A Machine Learning Approach for World Creation<TITLE>A Machine Learning Approach to World Creation,<TITLE> Cohesive and Interesting World Creation Using Neural Networks<TITLE> Compositional World Creation Using Neural Networks<TITLE> Compositional World Creation Using Content Representation<TITLE> World Creation Using Content Representation<TITLE> Compositional World Creation Using Content Representations,<TITLE>Multiplayer Text Adventure Games with Compositional Neural Networks<TITLE>Multiplayer Text Adventure Game Environment Creation with Neural Networks<TITLE>Multiplayer Text Adventure Game Environment Creation with Neural Network<TITLE>Multiplayer Text Adventure World Creation with Neural Networks<TITLE>Neural World Construction for Text Adventure Games
7,Synthetic and Natural Noise Both Break Neural Machine Translation,"Character-based neural machine translation (NMT) models alleviate out-ofvocabulary issues, learn morphology, and move us closer to completely end-toend translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",<TITLE>Character-Based Neural Machine Translation with Noisy Texts<TITLE>Character-based Neural Machine Translation with Noisy Texts<TITLE>Character-Based Neural Machine Translation with Synthetic Noise<TITLE>Character-Based Neural Machine Translation with Noisy Data<TITLE>Character-Based Neural Machine Translation with Noise,"<TITLE>Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise: A Case Study on Noisy Texts in N<TITLE>Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise: A Case Study on Noisy Texts, Out<TITLE>Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise: A Case Study on Noisy Texts in English<TITLE>Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise: A Case Study on Noisy Texts, Structure<TITLE>Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise: A Case Study on Noisy Texts in the",<TITLE>Robust Neural Machine Translation with Synthetic and Natural Sources of Noisy Data<TITLE>Robust Character-Based Neural Machine Translation with Synthetic and Natural Sources of Noise<TITLE>Robust Neural Machine Translation with Synthetic and Natural Sources of Noise<TITLE>Robust Neural Machine Translation with Synthetic and Natural Noise<TITLE>Robust Neural Machine Translation with Noisy Texts,<TITLE>Robust Character-Based Neural Machine Translation<TITLE>Strong Character-Based Neural Machine Translation Models<TITLE>Robust Neural Machine Translation Models<TITLE>Strong Character-Based Neural Machine Translation<TITLE>Character-Based Neural Machine Translation,<TITLE> Character-based Neural Machine Translation with Natural Sources of Noise<TITLE> Character-Based Neural Machine Translation with Natural Sources of Noise<TITLE> Robust Character-Based Neural Machine Translation<TITLE> Robust Character-based Neural Machine Translation<TITLE> Character-based Neural Machine Translation,<TITLE>Character-Based Neural Machine Translation Fails to Translate Noisy Texts<TITLE>Character-Based Neural Machine Translation Fails to Translate Even moderately noisy texts<TITLE>Character-based Neural Machine Translation Fails to Translate Even moderately noisy texts<TITLE>Robust Neural Machine Translation with Synthetic and Natural Sources of Noise<TITLE>Robust Neural Machine Translation with Synthetic and Natural Noise Sources
8,On the Universality of Online Mirror Descent,"We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.",<TITLE>Optimal Regret Guarantees for Mirror Descent<TITLE>Optimal Regret Guarantees for Online Learning Problems<TITLE>Optimal Regret Guarantees in Mirror Descent<TITLE>Optimal Regret Guarantees for Online Learning<TITLE>Optimal Regret Guarantees in Online Learning,<TITLE>Online Mirror Descent: Almost Optimal Regret Guarantee Guarantees for Convex Online Learning with Limited Supervision.<TITLE>Online Mirror Descent: Almost Optimal Regret Guarantee Guarantees for Convex Online Learning with Application to Reinforcement<TITLE>Online Mirror Descent: Almost Optimal Regret Guarantee Guarantees for Convex Online Learning with Limited Reinforcement Learning<TITLE>Online Mirror Descent: Almost Optimal Regret Guarantee Guarantees for Convex Online Learning with Limited Supervision and<TITLE>Online Mirror Descent: Almost Optimal Regret Guarantee Guarantees for Convex Online Learning with Limited Reinforcement.,<TITLE>Optimal Regret Guarantees for Mirror Descent<TITLE>Optimal Regret Guarantees for Mirror Descent Problems<TITLE>Optimal Regret Guarantee for Mirror Descent<TITLE>Optimal Regret Guarantees for Mirror Descent<TITLE>Optimal Regret Guarantee for Mirror Descent,<TITLE>Mirror Descent: An Online Class of Convex Learning Problems<TITLE>Mirror Descent: An Online Learning Problem for Convex Online Learning<TITLE>Mirror Descent: Optimal Regret Guarantee for Online Learning<TITLE>Mirror Descent: A General Class of Convex Online Learning<TITLE>Mirror Descent: A General Class of Online Learning Problems,<TITLE> Optimal Regret Guarantees for Mirror Descent<TITLE> Optimal Regret Guarantee for Mirror Descent<TITLE> Monte Carlo Online Learning with Mirror Descent<TITLE> Online Learning with Mirror Descent<TITLE> On Mirror Descent,<TITLE>Mirror Descent can always achieve an optimal regret guarantee<TITLE>Nearly Optimal Regret Guarantees for Online Learning Problems<TITLE>Nearly Optimal Regret Guarantees for Mirror Descent<TITLE>Nearly Optimal Regret Guarantees for Online Learning<TITLE>Optimal Regret Guarantees for Mirror Descent
9,Regularization of Neural Networks using DropConnect,"We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.",<TITLE>DropConnect: A Generalization of Dropout for Regularizing Neural Networks<TITLE>DropConnect: A Generalization of Dropout for Regularizing Deep Networks<TITLE>DropConnect: A Generalization of Dropout for Deep Neural Networks<TITLE>DropConnect: A Generalization of Dropout for Neural Networks<TITLE>DropConnect: A Generalization of Dropout in Neural Networks,<TITLE>DropConnect: Regularizing Fully-Connected Networks via Randomized Subset Selection and Multiple Drop-Connecting Data Aggregation<TITLE>DropConnect: Regularizing Fully-Connected Networks via Randomized Subset Selection and Multiple Drop-Connecting Data for Image<TITLE>DropConnect: Regularizing Fully-Connected Networks via Randomized Subset Selection and Multiple Drop-Connecting Data for Classification<TITLE>DropConnect: Regularizing Fully-Connected Networks via Randomized Subset Selection and Multiple Drop-Connecting Data for General<TITLE>DropConnect: Regularizing Fully-Connected Networks via Randomized Subset Selection and Multiple Drop-Connecting Data for 3,<TITLE>DropConnect: A Generalization of Dropout for Fully-Connected Neural Networks<TITLE>DropConnect: A Generalization of Dropout for Fully-Connected Networks<TITLE>DropConnect: A Generalization of Dropout<TITLE>DropConnect: A Generalization of Dropout<TITLE>DropConnect: A Generalization of Dropout,<TITLE>DropConnect: A Generalization of Dropout in Neural Networks<TITLE>DropConnect: Regularizing Large Fully-Connected Layers<TITLE>DropConnect: a Generalization of Dropout<TITLE>DropConnect: A Generalization of Dropout<TITLE>DropConnect: Generalization of Dropout,<TITLE> DropConnect: Regularizing Fully-Connected Neural Networks<TITLE> DropConnect: Regularizing Fully-Connected Networks<TITLE> DropConnect: Regularizing Fully-Connected Deep Networks<TITLE> DropConnect: A Generalization of Dropout<TITLE> DropConnect: Regularizing Deep Neural Networks,<TITLE>Dropout Meets DropConnect: Regularizing Fully-Connected Neural Networks<TITLE>Dropout Meets DropConnect: Regularizing Large-Scale Neural Networks<TITLE>Dropout Meets DropConnect: Regularizing Fully-Connected Networks<TITLE>DropConnect: Regularizing Fully-Connected Neural Networks<TITLE>DropConnect: Regularizing Fully-Connected Networks
10,Two monolingual parses are better than one (synchronous parse),"We describe a synchronous parsing algorithm that is based on two successive monolingual parses of an input sentence pair. Although the worst-case complexity of this algorithm is and must be O(n) for binary SCFGs, its average-case run-time is far better. We demonstrate that for a number of common synchronous parsing problems, the two-parse algorithm substantially outperforms alternative synchronous parsing strategies, making it efficient enough to be utilized without resorting to a pruned search.",<TITLE>A Two-Parsing Algorithm for Synchronous Parsing Problems<TITLE>A Two-Parsing Algorithm for Synchronous Parsing.<TITLE>A Two-Parsing Algorithm for Synchronous Parsing<TITLE>Monolingual Synchronous Parsing with Two Sentence Pairs<TITLE>Two-parse Synchronous Parsing with Monolingual Parses,<TITLE>A Two-Parallel Synchronous Parsing Algorithm Based on Successive Monolingual Parsing and Pruned Search<TITLE>A Two-Parallel Synchronous Parsing Algorithm Based on Successive Monolingual Parsing and its Application to<TITLE>A Two-Parallel Synchronous Parsing Algorithm Based on Successive Monolingual Dependency Parsing and Pr<TITLE>A Two-Parallel Synchronous Parsing Algorithm Based on Successive Monolingual Parsing and Pruned Struct<TITLE>A Two-Parallel Synchronous Parsing Algorithm Based on Successive Monolingual Dependency Parsing and the,<TITLE>Efficient Synchronous Parsing with Two Monolingual Parses<TITLE>Efficient Synchronous Parsing with Two Monolingual Parsers<TITLE>Efficient Synchronous Parsing Using Two Monolingual Parsing<TITLE>Efficient Synchronous Parsing with Monolingual Parsing<TITLE>Synchronous Parsing with Monolingual Parsing,<TITLE>A Two-Parse Synchronous Parsing Algorithm for Binary SCFG<TITLE>Two-Parse Synchronous Parsing with Input Sentence Pairs<TITLE>A Two-Parse Synchronous Parsing Algorithm<TITLE>Two-Parse Synchronous Parsing for Binary SCFGs<TITLE>Two-Parse Synchronous Parsing,<TITLE> A Synchronous Parser Based on Two successive Monolingual Parses<TITLE> Monolingual Parsing with Synchronous Parsing Algorithms<TITLE> A Synchronous Parser Based on Two successive Monolingual Parsers<TITLE> Monolingual Parsing with Synchronous Parsing<TITLE> A Synchronous Parser Based on Two successive Monolingual Parsing,<TITLE>Efficient Synchronous Parsing with Two Syntactic Monolingual Parses<TITLE>Synchronous Parsing with Two-Parsed Monolingual Dependencies<TITLE>Synchronous Parsing with Two-Parsed Monolingual Parsers<TITLE>Synchronous Parsing with Two Syntactic Monolingual Parses<TITLE>Two-Parse Synchronous Parsing
11,Semantic Frame Identification with Distributed Word Representations,"We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work.",<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification using Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and Syntactic Context<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and Syntactic Context,<TITLE>Semantic Frame Identification using Syntactic and Semantic Representations of Predicates and their Syntactic Context Using Word Embeddings<TITLE>Semantic Frame Identification using Syntactic and Semantic Representations of Predicates and their Syntactic Context with Word Embeddings<TITLE>Semantic Frame Identification using Syntactic and Semantic Representations of Predicates and their Syntactic Context Using Word Embedding<TITLE>Semantic Frame Identification using Syntactic and Semantic Representations of Predicates and their Syntactic Context based on Word Embedd<TITLE>Semantic Frame Identification using Syntactic and Semantic Representations of Predicates and their Syntactic Context with Word Embedding,<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and Syntactic Contexts<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and Syntactic Contexts<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and Syntactic Context,<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and Their Syntactic Context<TITLE>Semantic Frame Identification using Distributed Representations of Predicates and Their Syntactic Context<TITLE>Semantic Frame Identification using Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification with Distributed Representations of Predicates,<TITLE> Semantic Frame Identification Using Distributional Representations<TITLE> Semantic Frame Identification Using Distributed Representations of Predicates and Their Syntax<TITLE> Semantic Frame Identification Using Distributed Representations<TITLE> Semantic Frame Identification via Distributional Representations<TITLE> Semantic Frame Identification Using Distributed Representations of Predicates,<TITLE>Distributed Representations of Predicates and their Syntactic Context for Semantic Frame Identification<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and their Syntactic Context<TITLE>Distributed Representations of Predicates and Their Syntactic Context for Semantic Frame Identification<TITLE>Semantic Frame Identification with Distributed Representations of Predicates and their Syntactic Context<TITLE>Semantic Frame Identification Using Distributed Representations of Predicates and Their Syntactic Context
12,Similarity Analysis of Contextual Word Representation Models,"This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.",<TITLE>Contextual Word Representation Models from Similarity Analysis<TITLE>Contextual Word Representation Models with Similarity Analysis<TITLE>Contextual Word Representation Models with Similarity Measures<TITLE>Contextual Word Representations with Similarity Analysis<TITLE>Similarity in Contextual Word Representation Models,<TITLE>Similarity Analysis of Contextual Word Representations: A Case Study on Information Localization in the Lens of Similarity Analysis.<TITLE>Similarity Analysis of Contextual Word Representations: A Case Study on Information Localization in the Light of Similarity Analysis.<TITLE>Similarity Analysis of Information Localization in Contextual Word Representations: A Case Study on Similarity Analysis and Design Factors for<TITLE>Similarity Analysis of Information Localization in Contextual Word Representations: A Case Study on Similarity Analysis and Design Factors Affect<TITLE>Similarity Analysis of Contextual Word Representations: A Case Study on Information Localization in the Light of Similarity and Domain,<TITLE>Similarity Analysis of Contextual Word Representations in Deep Learning<TITLE>Similarity Analysis of Deep Contextual Word Representations and Attention<TITLE>Similarity Analysis of Deep Contextual Word Representations<TITLE>Similarity Analysis of Deep Contextual Word Representations<TITLE>Similarity Analysis of Contextual Word Representations,<TITLE>Similarity Measures for Contextual Word Representations<TITLE>Similarity Models for Contextual Word Representations<TITLE>Similarity Models for Contextual Word Representation<TITLE>Similarity Analysis for Contextual Word Representations<TITLE>Similarity Analysis in Contextual Word Representations,<TITLE> Contextual Word Representation Models<TITLE> Contextual Word Representations and Attention<TITLE> Contextual Word Representation Models for Similarity Analysis<TITLE> Contextual Word Representation Modeling<TITLE> Contextual Word Representation Modeling with Neural Networks,<TITLE>Comparing Neural Architectures of Contextual Word Representations<TITLE>Comparing Neural Architectures in Contextual Word Representations<TITLE>Comparing Neural Architectures for Contextual Word Representations<TITLE>Similarity Analysis of Contextual Word Representations<TITLE>Similarity Analysis of Contextual Word Representation Models
13,Coupling CCG and Hybrid Logic Dependency Semantics,"Categorial grammar has traditionally used the Î»-calculus to represent meaning. We present an alternative, dependency-based perspective on linguistic meaning and situate it in the computational setting. This perspective is formalized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally.",<TITLE>Synthesizing Semantic Phenomena in Categorial Grammar<TITLE>Compositional Semantics in Combinatory Categorial Grammar<TITLE>A Hybrid Logic Approach to Combinatory Categorial Grammar<TITLE>A Hybrid Logic Approach to Combinatory Categorial Grammars<TITLE>A Hybrid Logic for Combinatory Categorial Grammar,<TITLE>A Dependency Perspective on the Combinatory Categorial Grammar: An Application to Interpretation of Linguistic Meaning<TITLE>A Dependency Perspective on the Combinatory Categorial Grammar: An Application to Interpretation and Interpretation of Meaning<TITLE>A Dependency Perspective on the Combinatory Categorial Grammar: An Application to Interpretation and Semantic Parsing<TITLE>A Dependency Perspective on the Combinatory Categorial Grammar: An Application to Interpretation and Semantic Interpretation<TITLE>A Dependency Perspective on the Combinatory Categorial Grammar: An Application to Interpretation of Linguistic Sem,<TITLE>Dependency-Based Categorial Grammar with Hybrid Logic<TITLE>Dependency-Based Categorial Grammar and Hybrid Logic<TITLE>Dependency-Based Categorial Grammar with Hybrid Logic<TITLE>Computational Categorial Grammar and Hybrid Logic<TITLE>Computational Categorial Grammar and Hybrid Logic,<TITLE>Combinatory Categorial Grammar: A Dependency-Based Perspective on Language Meaning<TITLE>Combinatory Categorial Grammar: A Dependency-Based Perspective<TITLE>Combinatory Categorial Grammar for Compositional Interpretation<TITLE>Combinatory Categorial Grammar for Compositional Interpretations<TITLE>Combinatory Categorial Grammar,<TITLE> Compositional Semantics with Hybrid Logic and Situated Inference<TITLE> Compositional Semantics with Hybrid Logic<TITLE> Compositional Semantics with Hybrid Logic and Situating<TITLE> Integrating Logic and Semantics into a Unified Parsing Model<TITLE> Integrating Logic and Semantics into a Unified Parsing Framework,<TITLE>Dependency-based Categorial Grammars<TITLE>Dependency-Based Categorial Grammars<TITLE>Dependency-based Categorial Grammar<TITLE>Dependency-Based Categorial Grammar<TITLE>Hybrid Logic and Combinatory Categorial Grammar
14,+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference,"Recently, work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (+/-effect events). This paper addresses methods for creating a lexicon of such events, to support such work on opinion inference. Due to significant sense ambiguity, our goal is to develop a sense-level rather than word-level lexicon. To maximize the effectiveness of different types of information, we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set.",<TITLE>A Sense-Level Lexicon for Opinion Inference in NLP<TITLE>A Lexicon of Opinions for Opinion Inference<TITLE>A Sense-Level Lexicon for Opinion Inference<TITLE>A Lexicon of Opinions in NLP<TITLE>A Lexicon of Events for Opinion Inference,<TITLE>A Sense-Level Lexicon of Events Using WordNet1 Relations and Gloss Information for Opinion Inference on Entities +/-<TITLE>A Sense-Level Lexicon of Events Using WordNet1 Relations and Gloss Information for Opinion Inference on Entities +-<TITLE>A Sense-Level Lexicon of Events Using WordNet1 Relations and Gloss Information for Opinion Inference in NLP Models:<TITLE>A Sense-Level Lexicon of Events Using WordNet1 Relations and Gloss Information for Opinion Inference in NLP Models.<TITLE>A Sense-Level Lexicon of Events Using WordNet1 Relations and Gloss Information for Opinion Inference in NLP Models using,<TITLE>A Graph-based Lexicon of +/--Effect Events<TITLE>A Graph-based Lexicon for +/--Effect Events<TITLE>A Graph-based Lexicon of +/--effect Events<TITLE>A Graph-based Lexicon of +/-effect Events<TITLE>A Lexicon of +/--Effect Events,<TITLE>A Lexical of Opinion Inferences in NLP<TITLE>Towards a Lexical of Opinion Inference<TITLE>A Lexical of Opinion Inferences for NLP<TITLE>A Lexical of Opinion Inference in NLP<TITLE>A Lexical of Opinion Inference for NLP,<TITLE> A Sense-Level Lexicon for Opinion Inference<TITLE> Semi-supervised Lexicon Induction for Opinion Inference<TITLE> Semi-Supervised Lexicon Induction for Opinion Inference<TITLE> A Sense-Level Lexicon for Opinions about Opinions<TITLE> A Sense-Level Lexicon for Opinions about Event Types,<TITLE>A Sense-Level Lexicon for Opinion Inference<TITLE>Creating a Lexicon for Opinion Inference in NLP<TITLE>Creating a Lexicon for Opinion Inference<TITLE>Creating a Lexicon of Opinion Inference<TITLE>Creating a Lexicon of Effect Events
15,Improving the Impact of Subjectivity Word Sense Disambiguation on Contextual Opinion Analysis,"Subjectivity word sense disambiguation (SWSD) is automatically determining which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD has been shown to improve the performance of contextual opinion analysis, but only on a small scale and using manually developed integration rules. In this paper, we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis.",<TITLE>Scaling up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis<TITLE>Scaling Up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis<TITLE>Improving Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis<TITLE>Scale Up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis<TITLE>Better Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis,<TITLE>Integrating Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis: A Case Study on Non-Expert Annotations<TITLE>Integrating Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis: A Case Study on Non-Expert An<TITLE>Integrating Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis: A Case Study on the Integration of Subjectivity<TITLE>Integrating Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis: A Case Study on Non-expert Annotations<TITLE>Integrating Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis: A Case Study on Non-expert An,<TITLE>Scaling up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis<TITLE>Scaling up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis<TITLE>Scaling up Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis<TITLE>Scaling up Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis<TITLE>Scaling up Subjectivity Word Sense Disambiguation for Contextual Opinion Analysis,<TITLE>Integrated Subjectivity Word Sentiment Disambiguation into Contextual Opinion Analysis<TITLE>Integrated Subjectivity Word Sentiment Disambiguation in Contextual Opinion Analysis<TITLE>Integrated Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis<TITLE>Integrated Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis.<TITLE>Integrated Subjectivity Word Sense Disambiguation in Contextual Opinion Analysis,<TITLE> Improving Contextual Opinion Analysis Using Word Sense Disambiguation<TITLE> Enhancing Contextual Opinion Analysis with Word Sense Disambiguation<TITLE> Modeling Subjectivity Word Sense Disambiguation<TITLE> Improving Contextual Opinion Analysis with Word Sense Disambiguation<TITLE> Improving Contextual Opinion Analysis,<TITLE>Scaling up Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis<TITLE>Scaling Subjectivity Word Sense Disambiguation into Contextual Opinion Analysis<TITLE>Improving the Integration of Subjectivity Word Sense Disambiguation into Contextual Opinions<TITLE>Subjectivity Word Sense Disambiguation Improves Contextual Opinion Analysis<TITLE>Improving the Integration of Subjectivity Word Sense Disambiguation into Contextual Opinion
16,Robust Zero-Shot Cross-Domain Slot Filling with Example Values,"Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting.",<TITLE>Robust Zero-Shot Slot Filling for Task-Oriented Dialog Systems<TITLE>Learning Semantic Representations of Slot Attributes for Task-Oriented Dialog Systems<TITLE>Learning Semantic Representations of Slot Attributes for Task-Oriented Dialog System<TITLE>Zero-Shot Slot Filling for Task-Oriented Dialog Systems<TITLE>Zero-shot Slot Filling for Task-Oriented Dialog Systems,<TITLE>Robust and Robust Slot Filling for Task-Oriented Dialog Systems with Zero-shot Modeling of Sem<TITLE>Robust and Robust Slot Filling for Task-Oriented Dialog Systems with Zero-Shot Modeling of Sem<TITLE>Robust and Robust Slot Filling for Task-Oriented Dialog Systems with Semantic Representations of Spots<TITLE>Robust and Robust Slot Filling for Task-Oriented Dialog Systems with Semantic Representations of Slots<TITLE>Robust and Robust Slot Filling for Task-Oriented Dialog Systems with Zero-shot Modeling of Sp,<TITLE>Learning Transferable Semantic Representations for Robust Slot Filling in Task-Oriented Dialogs<TITLE>Learning Semantic Representations for Robust Slot Filling in Task-Oriented Dialog Systems<TITLE>Learning Semantic Representations for Robust Slot Filling in Task-Oriented Dialog Systems<TITLE>Learning Semantic Representations for Robust Slot Filling in Task-Oriented Dialogs<TITLE>Learning Semantic Representations for Robust Slot Filling in Task-Oriented Dialogs,<TITLE>Learning Semantic Representations of Slots for Task-oriented Dialogue Systems<TITLE>Learning Semantic Representations of Slots for Task-Oriented Dialog<TITLE>Learning Semantic Representations of Slots for Task-oriented Dialog Systems<TITLE>Slot Representations for Task-Oriented Dialogue Systems<TITLE>Slot Representations for Task-Oriented Dialog Systems,<TITLE> Learning Semantic Representations of Slots<TITLE> Learning Semantic Representations of Slot Filling Models<TITLE> Learning Semantic Representations of Slot Filling<TITLE> Learning Semantic Representations for Slot Filling<TITLE> Learning Semantic Representations of Slot Filling Agents,<TITLE>Learning Semantic Representations of Slot Filling Models for Task-Oriented Dialog Systems<TITLE>Learning Semantic Representations of Slot Filling Models for Task-Oriented Dialog System<TITLE>Learning Semantic Representations of Slot Filling Models for Task-oriented Dialog Systems<TITLE>Learning Robust Slot Filling Representations for Task-Oriented Dialog Systems<TITLE>Learning Robust Slot Filling Representations for Task-Oriented Dialog System
17,A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment,"Word sense induction (WSI) seeks to automatically discover the senses of a word in a corpus via unsupervised methods. We propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly. Topics are informed by the entire document, while senses are informed by the local context surrounding the ambiguous word. We also discuss unsupervised ways of enriching the original corpus in order to improve model performance, including using neural word embeddings and external corpora to expand the context of each data instance. We demonstrate significant improvements over the previous state-of-the-art, achieving the best results reported to date on the SemEval-2013 WSI task.",<TITLE>A Sense-Topic Model for Word Sense Induction<TITLE>Word Sense Induction with Sense-Topic Modeling<TITLE>Word Sense Induction with Sense-Topic Models<TITLE>Word Sense Induction with Sense-Topic Model<TITLE>Unsupervised Word Sense Induction,<TITLE>Joint Sense-Topic Modeling for Unsupervised Word Sense Induction with Latent Variables and Neural Word Embedd<TITLE>Joint Sense-Topic Modeling for Unsupervised Word Sense Induction with Latent Variables and Neural Word Representations<TITLE>Joint Sense-Topic Modeling for Unsupervised Word Sense Induction with Latent Variables and Neural Word Representation<TITLE>A Sense-Topic Model for Word Sense Induction with Latent Variables and Neural Word Embeddings in Contextual Context<TITLE>Joint Sense-Topic Modeling for Unsupervised Word Sense Induction with Latent Variables and External Contextual Information,<TITLE>A Joint Latent Variable Model for Unsupervised Word Sense Induction<TITLE>A Sense-Topic Model for Unsupervised Word Sense Induction.<TITLE>A Sense-Topic Model for Unsupervised Word Sense Induction<TITLE>A Joint Latent Variable Model for Word Sense Induction<TITLE>A Sense-Topic Model for Word Sense Induction,<TITLE>A Sense-topic Model for Word Sensitive Induction<TITLE>A Sense-topic Model for Word Sensitivity Induction<TITLE>A Sense-topic Model for Word Sentiment Induction<TITLE>Unsupervised Word Sentiment Induction with Sense-topic Model<TITLE>A Sense-topic Model for Word Sentence Induction,<TITLE> Unsupervised Sense-Topic Modeling for Word Sense Induction<TITLE> A Sense Topic Model for Word Sense Induction<TITLE> Unsupervised Word Sense Induction with Latent Variable Models<TITLE> Unsupervised Word Sense Induction with Latent Topic Models<TITLE> Unsupervised Sense-Topic Modeling for Word Senses,<TITLE>A Sense-topic Model for Word Sense Induction<TITLE>Unsupervised Word Sense Induction with Sense Topic Models<TITLE>Improving Word Sense Induction with Sense-topic Models<TITLE>Word Sense Induction with Sense-topic Models<TITLE>Word Sense Induction with Sense Topic Models
18,Point Process Modelling of Rumour Dynamics in Social Media,"Rumours on social media exhibit complex temporal patterns. This paper develops a model of rumour prevalence using a point process, namely a log-Gaussian Cox process, to infer an underlying continuous temporal probabilistic model of post frequencies. To generalize over different rumours, we present a multi-task learning method parametrized by the text in posts which allows data statistics to be shared between groups of similar rumours. Our experiments demonstrate that our model outperforms several strong baseline methods for rumour frequency prediction evaluated on tweets from the 2014 Ferguson riots.",<TITLE>A Probabilistic Model of rumour prevalence based on a Point Process<TITLE>A Probabilistic Model of rumour prevalence in social media<TITLE>A Probabilistic Model of Rumour prevalence in Social Media<TITLE>A Probabilistic Model of rumour prevalence on Twitter<TITLE>A Probabilistic Model of rumour prevalence in Twitter,<TITLE>A Probabilistic Model of Rumour Frequency Prediction Using Log-Gaussian Cox Processes and Multi-Task Learning in Social<TITLE>A Multi-Task Learning Model of Rumour Prevalence using Log-Gaussian Cox Processes and its Application to Predict<TITLE>A Probabilistic Model of Rumour Frequency Prediction Using Log-Gaussian Cox Processes and Multi-Task Learning for Social<TITLE>A Multi-Task Learning Model of Rumour Prevalence using Log-Gaussian Cox Processes and its Application to Context<TITLE>A Probabilistic Model of Rumour Frequency Prediction Using Log-Gaussian Cox Processes and Multi-Task Learning for Twitter,<TITLE>A Log-Gaussian Cox Process Model of Rumour Frequency in Social Media Posts<TITLE>Rumour Frequency Prediction on Social Media with Log-Gaussian Cox Processes<TITLE>A Log-Gaussian Cox Process Model of Rumour Frequency on Social Media<TITLE>A Log-Gaussian Cox Process Model of Rumour Frequency in Social Media<TITLE>Rumour Frequency Prediction Using Log-Gaussian Cox Processes,<TITLE>A Multi-Task Learning Model of rumour prevalence on social media<TITLE>A Multi-Task Learning Model of rumour prevalence in social media<TITLE>A Multi-Task Learning Model for rumour prevalence on social media<TITLE>A Multi-Task Learning Model of Rumour Prevalence<TITLE>A Multi-Task Learning Model of rumour prevalence,<TITLE> Rumour Frequency Modeling Using Log-Gaussian Processes<TITLE> A Multi-Task Learning Method for Rumour Frequency Estimation<TITLE> A Multi-Task Model of Rumour Frequency Prediction<TITLE> A Multi-Task Model of Rumour Frequency<TITLE> Rumour Frequency Modeling with Log-Gaussian Cox Process,<TITLE>A Multi-Task Learning Approach to Predicting the Frequency of Rumours on Social Media<TITLE>A Multi-Task Learning Approach to Predicting the Frequency of Rumours in Social Media<TITLE>A Multi-Task Learning Approach to Predicting the Frequency of Rumours on Twitter<TITLE>A Log-Gaussian Cox Model for Predicting Rumours on Social Media<TITLE>A Log-Gaussian Cox Model for Predicting Rumours on Twitter
19,Beyond Disagreement-Based Agnostic Active Learning,"We study agnostic active learning, where the goal is to learn a classifier in a prespecified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithm for this problem is disagreement-based active learning, which has a high label requirement. Thus a major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions; first, a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and second, a novel confidence-rated predictor.",<TITLE>Agnostic Active Learning with Confidence-Ranking Prediction<TITLE>Agnostic Active Learning with Constraint-Ranking<TITLE>Agnostic Active Learning with No Label Constraints<TITLE>Agnostic Active Learning with Confidence-rated Prediction<TITLE>Consistent Agnostic Active Learning,<TITLE>Agnostic Active Learning: Confidence-Rate Prediction and Consistency of Label Complexity in an Agnostic Hypothesis<TITLE>Agnostic Active Learning: Confidence-Rate Prediction and Consistency of Label Complexity in an Agnostic Hypotheses<TITLE>Agnostic Active Learning: Confidence-Rate Prediction and Consistency of Label Complexity in an Agnostic Classification Algorithm<TITLE>Agnostic Active Learning: Confidence-Rate Prediction and Consistency of Label Complexity in an Agnostic Hypothese<TITLE>Agnostic Active Learning: Confidence-Rate Prediction and Consistency of Label Complexity in an Agnostic Hypotactic,<TITLE>Agnostic Active Learning with Confidence-rated Prediction<TITLE>Agnostic Active Learning with Confidence-Rated Prediction<TITLE>Agnostic Active Learning with Confidence-rated Prediction<TITLE>Agnostic Active Learning with Confidence-Rated Prediction<TITLE>Agnostic Active Learning with Confidence-rated Prediction,<TITLE>Agnostic Active Learning with Disagreement-Based Models<TITLE>Agnostic Active Learning with Disagreement-Based Modeling<TITLE>Agnostic Active Learning for Prespecified Hypothesis Classification<TITLE>Agnostic Active Learning for Prespecified Hypothese Classification<TITLE>Agnostic Active Learning,<TITLE> Agnostic Active Learning<TITLE> Agnostic Active Learning for General Classification<TITLE> Efficient Agnostic Active Learning<TITLE> Agnostic Active Learning with Confidence-Rated Prediction<TITLE> Agnostic Active Learning with Confidence-Based Prediction,<TITLE>A Consistent Active Learning Algorithm for Interactive Classification<TITLE>A Consistent Active Learning Algorithm with Guarantees<TITLE>Agnostic Active Learning for Interactive Classification<TITLE>Active Learning in Agnostic Context<TITLE>A Consistent Active Learning Algorithm
20,Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning,"Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose. Code is available at github.com/pokaxpoka/netrand.",<TITLE>Randomization and Adaptation of Deep Reinforcement Learning<TITLE>Randomization of Deep Reinforcement Learning Agents<TITLE>Randomization for Deep Reinforcement Learning<TITLE>Randomization in Deep Reinforcement Learning<TITLE>Randomized Deep Reinforcement Learning,<TITLE>Pokaxpoka: Randomized Deep Reinforcement Learning with Robust Features Invariant Across Diverse and Randomized Environments<TITLE>Pokaxpoka: Randomized Deep Reinforcement Learning with Robust Features Invariant across Diverse and Randomized Environments<TITLE>Pokaxpoka: Randomized Deep Reinforcement Learning with Robust Features Invariant Across Dense and Randomized Environments<TITLE>Pokaxpoka: Randomized Deep Reinforcement Learning with Robust Features Invariant Across Diverse and Randomized Domains<TITLE>Pokaxpoka: Randomized Deep Reinforcement Learning with Robust Features Invariant Across Diverse and Randomized Data Aug,<TITLE>Randomized Deep Reinforcement Learning with Monte Carlo Inference<TITLE>Randomized Deep Reinforcement Learning with Monte Carlo Inference.<TITLE>Randomized Deep Reinforcement Learning with Monte Carlo Inference<TITLE>Randomized Deep Reinforcement Learning with Monte Carlo Inference<TITLE>Randomized Neural Network for Deep Reinforcement Learning,<TITLE>A Randomized Neural Network for Deep Reinforcement Learning<TITLE>A Randomized Network for Deep Reinforcement Learning<TITLE>Randomization of Deep Reinforcement Learning Agents<TITLE>Randomization of Deep Reinforcement Learning<TITLE>Randomized Deep Reinforcement Learning,<TITLE> Convolutional Neural Networks for Generalization in Deep Reinforcement Learning<TITLE> Randomized Reinforcement Learning with Convolutional Neural Networks<TITLE> Convolutional Neural Networks for Generalizing Deep Reinforcement Learning<TITLE> Randomized Convolutional Neural Networks for Generalization<TITLE> Convolutional Neural Networks for Generalization,<TITLE>Randomized Convolutional Neural Networks for Deep Reinforcement Learning<TITLE>Improving Deep Reinforcement Learning with Randomization and Monte Carlo<TITLE>Improving Deep Reinforcement Learning by Randomizing Observations<TITLE>Netrand: Randomized Convolutional Neural Networks<TITLE>Improving Deep Reinforcement Learning with Randomization
21,Unsupervised learning of word sense disambiguation rules by estimating an optimum iteration number in the EM algorithm,"In this paper, we improve an unsupervised learning method using the ExpectationMaximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for verb WSD problems.",<TITLE>Improving Unsupervised Learning for Noun Word Sense Disambiguation Problems<TITLE>Improving Unsupervised Learning for Noun Word Sense Disambiguation Problem<TITLE>Improving Unsupervised Learning for Noun Word Sense Disambiguation<TITLE>Improving Unsupervised Learning for Word Sense Disambiguation Problems<TITLE>Unsupervised Learning of Noun Word Sense Disambiguation Problems,<TITLE>Improving Word Sense Disambiguation Using ExpectationMaximization and EM Algorithm for Japanese Dictionary Task in Sense<TITLE>Improving Word Sense Disambiguation Problems Using ExpectationMaximization and EM Algorithm for Japanese Dictionary Task-O<TITLE>Improving Word Sense Disambiguation Using ExpectationMaximization and EM Algorithm for Japanese Dictionary Task in Senses<TITLE>Improving Word Sense Disambiguation with ExpectationMaximization and EM Algorithm for Japanese Dictionary Task in Sense<TITLE>Improving Word Sense Disambiguation with ExpectationMaximization and EM Algorithm for Japanese Dictionary Task in Senses,<TITLE>Unsupervised Word Sense Disambiguation Using Expectation Maximization Algorithm<TITLE>Improved EM Algorithm for Word Sense Disambiguation Using Expectation Maximization<TITLE>Unsupervised Word Sense Disambiguation Using Expectation Maximization for Japanese<TITLE>Unsupervised Word Sense Disambiguation Using Expectation Maximization<TITLE>Unsupervised Word Sense Disambiguation Using Expectation Maximization Algorithm,<TITLE>An Unsupervised Learning Method for Word Sense Disambiguation<TITLE>Unsupervised Learning for Word Sense Disambiguation Problems<TITLE>Unsupervised Text Classification with ExpectationMaximization<TITLE>Unsupervised Learning for Word Sentiment Disambiguation<TITLE>Unsupervised Learning for Word Sense Disambiguation,<TITLE> Improving Word Sense Disambiguation Using Expectation Maximization<TITLE> Improving Unsupervised Word Sense Disambiguation Using Expectation Maximization<TITLE> Improving Unsupervised Word Sense Disambiguation with Expectation Maximization<TITLE> Improved Unsupervised Word Sense Disambiguation Using Expectation Maximization<TITLE> Improving Word Sense Disambiguation with Expectation Maximization,<TITLE>An Improved EM Method for Unsupervised Word Sense Disambiguation<TITLE>An Unsupervised Learning Method for Word Sense Disambiguation Problems<TITLE>Improved EM Method for Unsupervised Word Sense Disambiguation Problems<TITLE>An Improved EM Method for Word Sense Disambiguation Problems<TITLE>Improved EM Method for Unsupervised Word Sense Disambiguation
22,State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations,"Machine learning promises methods that generalize well from finite labeled data. However, the brittleness of existing neural net approaches is revealed by notable failures, such as the existence of adversarial examples that are misclassified despite being nearly identical to a training example, or the inability of recurrent sequence-processing nets to stay on track without teacher forcing. We introduce a method, which we refer to as state reification, that involves modeling the distribution of hidden states over the training data and then projecting hidden states observed during testing toward this distribution. Our intuition is that if the network can remain in a familiar manifold of hidden space, subsequent layers of the net should be well trained to respond appropriately. We show that this state-reification method helps neural nets to generalize better, especially when labeled data are sparse, and also helps overcome the challenge of achieving robust generalization with adversarial training.",<TITLE>State Reification for Robust Generalization in Neural Networks<TITLE>State Reification for Robust Generalization in Neural Nets<TITLE>State Reification for Robust Generalization with Deep Learning<TITLE>State Reification for Robust Generalization in Deep Learning<TITLE>State Reification for Generalization in Deep Neural Networks,<TITLE>State Reification for Robust Neural Net Generalization with Adversarial Training of Finite-Labeled Data via Hidden State<TITLE>State Reification for Robust Neural Net Generalization with Adversarial Training of Finite-Labeled Data from Finite<TITLE>State Reification for Robust Neural Net Generalization with Adversarial Training of Finite-Labeled Sequences and Hidden<TITLE>State Reification for Robust Neural Net Generalization with Adversarial Training of Finite-Labeled Data from Sparse<TITLE>State Reification for Robust Neural Net Generalization with Adversarial Training of Finite-Labeled Data via Hidden Space,<TITLE>State-Reification for Robust Generalization in Neural Nets<TITLE>State Reification for Robust Generalization from Sparse Data<TITLE>State Reification for Robust Generalization in Sparse Data<TITLE>State-Reification for Robust Generalization in Neural Networks<TITLE>State Reification for Robust Generalization in Neural Nets,<TITLE>State Reification for Neural Nets to Generalize<TITLE>State-Reification for Neural Nets<TITLE>State-Reification for Neural Networks<TITLE>State Reification for Neural Nets<TITLE>State Reification for Neural Networks,<TITLE> State Reification for Neural Networks<TITLE> State Reification for Neural Net Generalization<TITLE> State Reification: Improving Neural Net Generalization<TITLE> State Reification for Neural Nets<TITLE> State Reification for Neural Network Generalization,<TITLE>State Reification for Robust Neural Nets<TITLE>State Reification for Deep Neural Networks<TITLE>State Reification for Neural Net Training<TITLE>State Reification for Deep Neural Nets<TITLE>State Reification for Neural Nets
23,"A Fast, Compact, Accurate Model for Language Identification of Codemixed Text","We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents, social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification.",<TITLE>Multilingual Language Identification with a Global Constrained Decoder<TITLE>Multilingual Language Identification with a Global Constraint<TITLE>Language Identification with a Global Constrained Decoder<TITLE>Fine-Grained Multilingual Language Identification<TITLE>Fine-grained Multilingual Language Identification,"<TITLE>Fine-Grained Multilingual Language Identification with a Fast, Accurate, and Accurate Feed-Forward Network with a Global<TITLE>Fine-Grained Multilingual Language Identification with a Fast, Accurate, and Accurate Feed-Forward Decoding Network.<TITLE>Fine-Grained Multilingual Language Identification with a Fast, Accurate, and Accurate Feed-Forward Network with a Glob<TITLE>Fine-Grained Multilingual Language Identification with a Fast, Accurate, and Accurate Feed-Forward Network with a Min<TITLE>Fine-Grained Multilingual Language Identification with a Fast, Accurate, and Accurate Feed-Forward Network with a Gram",<TITLE>Fine-Grained Multilingual Language Identification with a Feed-Forward Network<TITLE>Fine-Grained Multilingual Language Identification with a Feed-Forward Model<TITLE>Fine-Grained Multilingual Language Identification with a Feed-Forward Networks<TITLE>Fine-Grained Multilingual Language Identification with Feed-Forward Networks<TITLE>Fine-grained Multilingual Language Identification with Feed-Forward Networks,<TITLE>Multilingual Language Identification with Feed-forward Networks<TITLE>Multilingual Language Identification via Feed-forward Networks<TITLE>Multilingual Language Identification via Feed-forward Network<TITLE>Multilingual Language Identification with Feed-forward Network<TITLE>Feed-forward Multilingual Language Identification,<TITLE> Fine-Grained Multilingual Language Identification<TITLE> Multilingual Language Identification via Feed-Forward Networks<TITLE> Multilingual Language Identification with Feed-Forward Networks<TITLE> Multilingual Language Identification by Feed-Forward Networks<TITLE> Multilingual Multilingual Language Identification,<TITLE>Fine-Grained Multilingual Language Identification<TITLE>Fine-Grained Multilanguage Language Identification<TITLE>Fine-Grained Semantic Language Identification<TITLE>Fine-Grained Multilingual Language Identification<TITLE>Fine-grained Multilingual Language Identification
24,Investigating Unsupervised Learning for Text Categorization Bootstrapping,"We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-theart performance using only the category names as initial seeds.",<TITLE>Unsupervised Bootstrapping of Text Categorization Using Latent Semantic Space<TITLE>Unsupervised Bootstrapping of Text Categorization using Latent Semantic Space<TITLE>Unsupervised Bootstrapping of Text Categorization with Generalized Seed Features<TITLE>Unsupervised Bootstrapping of Text Categorization with Generalized Features<TITLE>Unsupervised Bootstrapping of Categories Using Latent Semantic Space,<TITLE>A Generalized Bootstrapping Algorithm for Text Categorization Using Latent Semantic Space and Application to Unsupervised<TITLE>A Generalized Bootstrapping Algorithm for Text Categorization Using Latent Semantic Space and Application to Unlabeled<TITLE>A Generalized Bootstrapping Algorithm for Text Categorization Using Latent Semantic Space and Application to Classification of Category<TITLE>A Generalized Bootstrapping Algorithm for Text Categorization Using Latent Semantic Space and Application to Classification of Categories<TITLE>A Generalized Bootstrapping Algorithm for Text Categorization Using Latent Semantic Space and Application to Classification of Un,<TITLE>Unsupervised Bootstrapping of Text Categories Using Latent Semantic Space and Gaussian Mixture<TITLE>Unsupervised Bootstrapping of Text Categories using Latent Semantic Space and Gaussian Mixture<TITLE>Unsupervised Bootstrapping of Text Categories Using Latent Semantic Space and Gaussian Mixtures<TITLE>Unsupervised Bootstrapping of Text Categories using Latent Semantic Space and Gaussian Mixtures<TITLE>Unsupervised Bootstrapping of Text Categories Using Latent Semantic Space and Gaussian Mixture,<TITLE>A Generalized Bootstrapping Algorithm<TITLE>Generalized Bootstrapping for Category Descriptions<TITLE>Generalized Bootstrapping with Seed Features<TITLE>Generalized Bootstrapping for Seed Features<TITLE>Generalized Bootstrapping by Seed Features,<TITLE> Generalized Bootstrapping for Text Categorization<TITLE> A Generalized Bootstrapping Algorithm for Text Categorization<TITLE> Generalized Bootstrapping Algorithms for Text Categorization<TITLE> Generalized Bootstrapping Algorithm for Text Categorization<TITLE> A Generalized Bootstrapping Method for Text Categorization,<TITLE>A Generalized Seed-Based Approach to Text Categorization<TITLE>A Generalized Seed-Based Algorithm for Text Categorization<TITLE>Unsupervised Text Categorization Using Generalized Seed Feature Selection<TITLE>Unsupervised Text Categorization Using Generalized Seed Features.<TITLE>Unsupervised Text Categorization Using Generalized Seed Features
25,Unsupervised Induction of Semantic Roles,"Datasets annotated with semantic roles are an important prerequisite to developing highperformance role labeling systems. Unfortunately, the reliance on manual annotations, which are both difficult and highly expensive to produce, presents a major obstacle to the widespread application of these systems across different languages and text genres. In this paper we describe a method for inducing the semantic roles of verbal arguments directly from unannotated text. We formulate the role induction problem as one of detecting alternations and finding a canonical syntactic form for them. Both steps are implemented in a novel probabilistic model, a latent-variable variant of the logistic classifier. Our method increases the purity of the induced role clusters by a wide margin over a strong baseline.",<TITLE>Inducing Semantic Roles from Text for Semantic Role Labeling<TITLE>Inducing Semantic Roles of Verbal Arguments from Text<TITLE>Inducing Semantic Roles for Semantic Role Labeling<TITLE>Inducing Semantic Roles from Unannotated Text<TITLE>Inducing Semantic Roles from Text,<TITLE>Inducing Semantic Roles from Unannotated Text: A Probabilistic Model for Semi-supervised Dataset<TITLE>Inducing Semantic Roles from Unannotated Text: A Probabilistic Model for Role Induction in Datasets<TITLE>Inducing Semantic Roles from Unannotated Text: A Probabilistic Model for the Induction of Auxiliary Arg<TITLE>Inducing Semantic Roles from Unannotated Text: A Probabilistic Model for the Induction of Auxiliary Ver<TITLE>Inducing Semantic Roles from Unannotated Text: A Probabilistic Model for the Induction of Auxiliary Argument,<TITLE>A Probabilistic Model for Semantic Role Induction from Unannotated Text<TITLE>A Probabilistic Model for Inducing Semantic Roles of Argument Arguments<TITLE>Semantic Role Induction with a Probabilistic Model<TITLE>A Probabilistic Model for Semantic Role Induction<TITLE>Semantic Role Induction from Unannotated Text,<TITLE>Inducing Semantic Role Clusters with Manual Annotations<TITLE>Inducing Semantic Roles of Verbal Arguments from Text<TITLE>Inducing Semantic Role Clusters with Manual Annotation<TITLE>Inducing Semantic Roles of Verbal Arguments<TITLE>Inducing Semantic Role Clusters from Text,<TITLE> Inducing Semantic Roles from Unannotated Text<TITLE> Inducing Semantic Role Induction from Unannotated Text<TITLE> Semantic Role Induction Using Latent Variable Models<TITLE> Semantic Role Induction using Latent Variable Models<TITLE> Inducing Semantic Role Induction Using Probabilistic Models,<TITLE>Inducing Semantic Roles of Verbal Arguments from Unannotated Text<TITLE>Inducing Semantic Roles from Unannotated Text<TITLE>Induction of Semantic Roles from Unannotated Text<TITLE>Inducing Semantic Roles of Verbal Arguments from Text<TITLE>Inducing Semantic Roles of Verbal Arguments
26,"Commentary and Discussion: Entropy, the Indus Script, and Language: A Reply to R. Sproat","In a recent LastWords column (Sproat 2010), Richard Sproat laments the reviewing practices of âgeneral science journalsâ after dismissing our work and that of Lee, Jonathan, and Ziman (2010) as âuselessâ and âtrivially and demonstrably wrong.â Although we expect such categorical statements to have already raised some red flags in the minds of readers, we take this opportunity to present a more accurate description of our work, point out the straw man argument used in Sproat (2010), and provide a more complete characterization of the Indus script debate. A separate response by Lee and colleagues in this issue provides clarification of issues not covered here.",<TITLE>Last Words: An Analysis of the Reviewing Practice of General Science Journal<TITLE>Last Words: An Analysis of the Reviewing Practice of General Science Fiction<TITLE>Last Words: An Analysis of the Reviewing Practices of General Science Journal<TITLE>Last Words: An Analysis of the Reviewing Practice of General Science journals<TITLE>Last Words: An Analysis of the Reviewing Practice of Scientific Publications,<TITLE>LastWords: The LastWords Column (Sproat 2010) - An Accurate Description of the Indus Script Debate in Science<TITLE>LastWords: The LastWords Column (Sproat 2010) - An Accurate Description of the Indus Script Debate in Scientific<TITLE>LastWords: The LastWords Column (Sproat 2010) - An Accurate Description of the Indus Script Debate and Article<TITLE>LastWords: The LastWords Column (Sproat 2010) - An Accurate Description of the Indus Script Debate in Reading<TITLE>LastWords: The LastWords Column (Sproat 2010) - An Accurate Description of the Indus Script Debate and its,"<TITLE>Sproat 2010: A Response to Richard Sproat's Criticism of Lee, Jonathan, and Ziman<TITLE>Sproat 2010: A Response to Richard Sproat's Criticism of Lee, Jonathan, and Ziman<TITLE>Sproat 2010: A Response to Richard Sproat's Criticism of Our and Lee's Reviews<TITLE>Sproat 2010: A Response to Richard Sproat and LastWords<TITLE>LastWords Response to Sproat (2010)",<TITLE>The Indus script debate in LastWords<TITLE>The Indus Script Debate<TITLE>The Indus Script debate<TITLE>The Indus script debate<TITLE>Indus script debates,<TITLE> Last Words: The Last Words Newsletter<TITLE> Last Words: The Last Words Argument in Scientific Literature<TITLE> Last Words: The Last Words Column<TITLE> Last Words: The Last Words of Science<TITLE> Last Words: The Last Words Argument in the Last Words,<TITLE>Last Words: Correcting the Errors of Richard Sproat<TITLE>Last Words Revisited: Correcting the Errors in Last Words<TITLE>Last Words Revisited: Correcting Errors in Last Words Commentary<TITLE>Last Words Revisited: Correcting Errors in Last Words<TITLE>Last Words: Correcting Errors in Last Words
27,What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks,"We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun. We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension. The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce. We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks. We find that subjectivity, information locality, and information gain are all strong predictors, with some evidence for a two-factor account, where subjectivity and information gain reflect a factor involving semantics, and information locality reflects collocational preferences.",<TITLE>Predicting Order of Unseen Adjectives in English<TITLE>Predicting the Order of Adjectives in English<TITLE>Predicting the Order of Unseen Adjectives<TITLE>Predicting Order of Adjectives in English<TITLE>Predicting Adjective Order in English,"<TITLE>Subjectivity, Information Gain, and Lexical Complexity: The Case of Adjective Ordering in English-to-<TITLE>Subjectivity, Information Gain, and Lexical Complexity: The Case of Adjective Ordering in Large-Scale English<TITLE>Subjectivity, Information Gain, and Lexical Complexity: The Case of Adjective Ordering in the Big-Box<TITLE>Subjectivity, Information Gain, and Lexical Complexity: The Case of Adjective Ordering in Large-Scale Ph<TITLE>Subjectivity, Information Gain, and Lexical Complexity: The Case of Adjective Ordering in the Big-Squ",<TITLE>What determines the order of adjectives in English phrases such as big blue box?<TITLE>Big Blue Box: Predicting Adjective Order in English<TITLE>Big Blue Box: Predicting Adjective Order in English<TITLE>What determines the order of adjectives in English?<TITLE>What determines the order of adjectives in English?,"<TITLE>Subjectivity, Information Locality, and Information Gain in English<TITLE>Predicting Orders of Adjectives in English<TITLE>Quantitative Prediction of Adjectives in English<TITLE>Subjectivity, Information Locality, and Information Gain<TITLE>Subjectivity and Information Gain in English",<TITLE> What determines the preferred order of adjectives in English?<TITLE> What determines the preferred order of adjectives?<TITLE> What determines the preferred order of adjectives in English<TITLE> What determines the preferred order of adjectives<TITLE> What determines the preferred order of adjectives in English? A quantitative analysis,<TITLE>What Makes for Good Adjectives? A Case Study in English<TITLE>What determines the preferred order of adjectives in English?<TITLE>What determines the preferred order of adjectives in English<TITLE>What Makes for Good Adjectives?<TITLE>What determines the preferred order of adjectives?
28,"An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games","We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games.",<TITLE>Computing Nash Equilibria in Multi-Agent Games<TITLE>Computing Nash Equilibria in Multi-agent Games<TITLE>A Compact Representation for Non-Trivial Games<TITLE>Computing Nash Equilibria in Graphical Games<TITLE>Nash Equilibria in Graphical Games,<TITLE>Nash Equilibria in Multi-Agent Systems: A Nash Equilibrium Algorithm for Graphical Games and Applications to Multi<TITLE>Nash Equilibria in Multi-Agent Systems: A Nash Equilibrium Algorithm for Computing Graphical Games and Its Application<TITLE>Nash Equilibria in Multi-Agent Systems: A Nash Equilibrium Algorithm for Computing Graphical Games and Its Applications<TITLE>Nash Equilibria in Multi-Agent Systems: A Nash Equilibrium Algorithm for Computing Graphical Games and its Application<TITLE>Nash Equilibria in Multi-Agent Systems: A Nash Equilibrium Algorithm for Computing Graphical Games and its Applications,<TITLE>Efficient Nash Equilibria for Multi-Agent Games<TITLE>Efficient Nash Equilibria for Multi-Agent Systems<TITLE>Efficient Nash Equilibria in Graphical Games<TITLE>Efficient Nash Equilibria for Graphical Games<TITLE>Efficient Nash Equilibria for Multi-Agent Games,<TITLE>A Compact Algorithm for Computing Nash Equilibria in Graphical Games<TITLE>A Compact Algorithm for Computing Nash Equilibria in graphical games<TITLE>A Compact Representation for Nash Equilibria in Graphic Games<TITLE>A Compact Representation for Nash Equilibria in Graphical Games<TITLE>A Compact Representation for Nash Equilibria in Graphical Games,<TITLE> Computing Equilibria in Graphical Games with Compact Representations<TITLE> Nash Equilibria in Graphical Games<TITLE> Computing Nash Equilibria in Graphical Games<TITLE> A Convex Algorithm for Computing Nash Equilibria<TITLE> Computing Equilibria in Graphical Games,<TITLE>Computing Nash Equilibria in Multi-Agent Games<TITLE>Computing Nash Equilibria in Non-trivial Games<TITLE>Nash Equilibria in Multi-Agent Games<TITLE>Nash Equilibria in Non-trivial Games<TITLE>Nash Equilibria in Games
29,Algorithms for Optimizing the Ratio of Submodular Functions,"We investigate a new optimization problem involving minimizing the Ratio of two Submodular (RS) functions. We argue that this problem occurs naturally in several real world applications. We then show the connection between this problem and several related problems including minimizing the difference between submodular functions (Iyer & Bilmes, 2012b; Narasimhan & Bilmes, 2005), and to submodular optimization subject to submodular constraints (Iyer & Bilmes, 2013). We show that RS optimization can be solved with bounded approximation factors. We also provide a hardness bound and show that our tightest algorithm matches the lower bound up to a log factor. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.",<TITLE>Submodular Ratio Minimization via Bounded Approximation<TITLE>Submodular Optimization with Bounded Approximation Factors<TITLE>Submodular Optimization with Bounded Approximation Factor<TITLE>Submodular Optimization via Bounded Approximation<TITLE>Minimizing Ratio of Submodular Functions,<TITLE>Tight Bounds for Minimizing Ratio of Two Submodular Functions: Tightness Bounds and Scalable Algorith<TITLE>Tight Bounds for Minimizing Ratio of Two Submodular Functions via Bounded Approximation and Submodularity<TITLE>Tight Bounded Approximation of Submodular Functions via Minimizing the Ratio of Two Submodularity Functions.<TITLE>Tight Bounds for Minimizing Ratio of Two Submodular Functions: Tightness Bounds and Scalable Algorithm<TITLE>Tight Bounded Approximation of Submodular Functions via Minimizing the Ratio of Two Submodularity Functions and,<TITLE>Minimizing the Ratio of Two Submodular Functions<TITLE>Minimizing the Ratio of Two Submodular Functions.<TITLE>Minimizing Ratio of Two Submodular Functions.<TITLE>Minimizing the Ratio of Two Submodular Functions<TITLE>Minimizing Ratio of Two Submodular Functions,<TITLE>Minimizing the Ratio of Submodular Functions<TITLE>Minimizing Ratio of Submodular Functions<TITLE>Minimizing the Ratio of Submodular functions<TITLE>Minimization of Submodular Functions<TITLE>Minimizing Submodular Functions,<TITLE> Submodular Optimization with Bounded Approximation<TITLE> Submodular Optimization via Bounded Approximation<TITLE> Submodular Optimization with Bounded Approximation Factor<TITLE> Submodular Optimization with Bounded Approximation Factors<TITLE> Submodular Optimization Using Bounded Approximation,<TITLE>Bridging the Gap Between Submodular Functions: A New Problem and Algorithms<TITLE>Scalable Submodular Maximization with Constraints<TITLE>On the Ratio of Two Submodular Functions.<TITLE>On the Ratio of Two Submodular Functions<TITLE>Scalable Submodular Maximization
30,An effective Discourse Parser that uses Rich Linguistic Information,"This paper presents a first-order logic learning approach to determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report a statistically significant improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIPPER and Naive Bayes. For discourse parsing, our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline.",<TITLE>Exploiting Compositional Semantics and Discourse Structure Data for Rhetorical Relations<TITLE>Exploiting Compositional Semantics and Discourse Structure Data for Relation Classification<TITLE>Exploiting Compositional Semantics and Segmentation Data for Discourse Relations<TITLE>First-Order Rhetorical Relations between Discourse Segments<TITLE>First-Order Logic Learning for Discourse Relations,<TITLE>A Logic Learning Approach to Determine Rhetorical Relations between Discourse Segments with Compositional Semantics and Discourse Structure<TITLE>A Logic Learning Approach to Determine Rhetorical Relations between Discourse Segments using Compositional Semantics and Discourse Structure<TITLE>A Logic Learning Approach to Determine Rhetorical Relationships between Discourse Segments with Compositional Semantics and Discourse<TITLE>A Logic Learning Approach to Determine Rhetorical Relations between Discourse Segments using Compositional Semantics and Lexical Information<TITLE>A Logic Learning Approach to Determine Rhetorical Relations between Discourse Segments with Compositional Semantics and Lexical Information,<TITLE>First-Order Logic Learning for Discourse Relation Classification<TITLE>First-Order Logic Learning for Discourse Relations Classification<TITLE>First-Order Logic Learning for Discourse Structure Classification<TITLE>First-Order Logic Learning for Discourse Relations Classification<TITLE>First-Order Logic Learning for Discourse Structure Classification,<TITLE>First-Order Logical Learning for Rhetorical Relation Classification<TITLE>First-Order Logical Learning for Discourse Parsing<TITLE>First-Order Logistic Learning for Discourse Parsing<TITLE>First-Order Logical Learning for Rhetorical Relations<TITLE>First-Order Logical Learning for Discourse Segment Relations,<TITLE> Modeling Discourse Structure with Compositional Semantics<TITLE> Modeling Discourse Relations with Compositional Semantics and Segment Structure<TITLE> Modeling Discourse Relations with Compositional Semantics<TITLE> Modeling Discourse Structure for Argumentation Parsing<TITLE> Semi-Supervised Learning of Discourse Relationships,<TITLE>Classifying Rhetorical Relations with First-Order Logic<TITLE>Classifying Rhetorical Relations Using First-Order Logic<TITLE>Classifying Rhetorical Relations between Discourse Segments<TITLE>Discourse Relation Classification using First-Order Logic Learning<TITLE>Discourse Relation Classification using First-Order Logic
31,Automatic Detection of Vague Words and Sentences in Privacy Policies,"Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, privacy policies are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in privacy policies. We construct the first corpus of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving vagueness and improving the usability of privacy policies.",<TITLE>Identifying Inconsistently Grown Words and Sentences in Privacy Policies<TITLE>Identifying Ambiguous Content in Privacy Policies: An Empirical Study<TITLE>Identifying Inconsistently Subjective Content in Privacy Policies<TITLE>Identifying and Combining Ambiguous Content in Privacy Policies<TITLE>Identifying Ambiguous Content in Privacy Policies,<TITLE>Identifying vague content in private policies for detecting vague words and sentences using auxiliary-classifier generative adversarial networks.<TITLE>Identifying vague content in private policies for detecting vague words and sentences using auxiliary-classifier generative adversarial neural networks<TITLE>Identifying vague content in private policies for detecting vague words and sentences using auxiliary-classifier generative adversarial networks on<TITLE>Identifying vague content in private policies for detecting vague words and sentences using auxiliary-classifier generative adversarial Networks.<TITLE>Identifying vague content in private policies for detecting vague words and sentences using auxiliary-classifier generative adversarial networks:,<TITLE>Detecting Vagueness in Privacy Policies: A Corpus and Empirical Studies<TITLE>Detecting Vagueness in Privacy Policies: A Case Study on Website Privacy Policies<TITLE>Detecting Vagueness in Privacy Policies: A Case Study on Web Privacy Policies<TITLE>Automatic Vagueness Detection in Privacy Policies<TITLE>Detecting Vagueness in Website Privacy Policies,<TITLE>Context-Aware and Context-Agnostic Models for Detecting Vague Content in Privacy Policies<TITLE>Context-Aware and Context-Agnostic Models for Identifying Vague Content in Privacy Policies<TITLE>Context-Aware and Context-Agnostic Models for Detecting Vagueness in Privacy Policies<TITLE>Identifying Vague Content in Privacy Policys<TITLE>Identifying Vague Content in Privacy Policies,<TITLE> Identifying Discrete Words and Sentences in Privacy Policies<TITLE> Identifying Incorrect Content in Privacy Policies<TITLE> Identifying Discrete Content in Privacy Policies<TITLE> Identifying Discrete Text in Privacy Policies<TITLE> Towards Understanding Textual Privacy Policies,<TITLE>Detecting and Improving the User Experience of Privacy Policies<TITLE>Detecting and Improving the User Interface of Privacy Policies<TITLE>Identifying and Improving Privacy Policies with Linguistic Information<TITLE>Detecting and Improving the User Experience of Privacy Policy<TITLE>Detecting and Improving the User Interface of Privacy Policy
32,Syntactic SMT Using a Discriminative Text Generation Model,"We study a novel architecture for syntactic SMT. In contrast to the dominant approach in the literature, the system does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems.",<TITLE>A Discriminative Target Sentence Generation System for Syntactic Statistical Machine Translation<TITLE>A Discriminative Target Sentence Generation Model for Syntactic Statistical Machine Translation<TITLE>A Discriminative Target Sentence Generation System for Syntactic Machine Translation<TITLE>A Discriminative Target Sentence Generation Model for Syntactic Machine Translation<TITLE>A Discriminative Target Sentence Generation Approach for Syntactic Machine Translation,<TITLE>A Novel Architecture for Syntactic Machine Translation with BLEU-based Syntax Features and Bilingual Translation Features from IW<TITLE>A Novel Architecture for Syntactic Machine Translation with BLEU-style Syntax Features and Bilingual Translation Features from IW<TITLE>A Novel Architecture for Syntactic Machine Translation with BLEU-based Syntax Features and Bilingual Translation Features from Target Synt<TITLE>A Novel Architecture for Syntactic Machine Translation with BLEU-based Syntax Features and Bilingual Translation Features from Source and<TITLE>A Novel Architecture for Syntactic Machine Translation with BLEU-style Syntax Features and Bilingual Translation Features from Target Synt,<TITLE>Syntactic SMT with Bilingual Translation Features<TITLE>Syntactic SMT with Soft Translation Features<TITLE>Syntactic SMT with Bilingual Translation<TITLE>Syntactic SMT with Soft Features<TITLE>Syntactic SMT with Soft Features,<TITLE>Syntactic SMT: An Unconstrained Approach<TITLE>Syntactic SMT with Soft Features for Translation<TITLE>Syntactic SMT Using Soft Features<TITLE>Syntactic SMT with Soft Features<TITLE>Syntactic SMT,<TITLE> Syntactic Syntactic SMT with Soft Features<TITLE> Syntactic Syntactic SMT Using Soft Features<TITLE> Neural Syntactic SMT with Soft Features<TITLE> A Neural Model for Syntactic SMT<TITLE> Syntactic Syntactic SMT with Translation Rules,<TITLE>Syntactic Machine Translation with Soft Syntax Features<TITLE>Unconstrained Syntactic Machine Translation with Soft Features<TITLE>Soft Syntactic Features for Statistical Machine Translation<TITLE>Unconstrained Syntactic Machine Translation<TITLE>Soft Syntactic Machine Translation
33,Infusion of Labeled Data into Distant Supervision for Relation Extraction,"Distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models. However, in some cases a small amount of human labeled data is available. In this paper, we demonstrate how a state-of-theart multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve.",<TITLE>Relation-Level Distant Supervision with Relation Extraction Models<TITLE>Relation-Level Distant Supervision with Relation Extraction<TITLE>Multi-Instance Distant Supervision for Relation Extraction<TITLE>Relation Extraction from Distant Supervision Data<TITLE>Relation Extraction from Distant Supervision,<TITLE>Multi-Instance Distant Supervision for Relation Extraction with Application to Sentence-Level Relation Level Relation Classification<TITLE>Multi-Instance Distant Supervision for Relation Extraction with Application to Sentence-Level Relation Level Relation Ret<TITLE>Multi-Instance Distant Supervision for Relation Extraction with Application to Sentence-Level Relation Level Relation Learning<TITLE>Multi-Instance Distant Supervision for Relation Extraction with Application to Sentence-Level Relation Level Relation Recall<TITLE>Multi-Instance Distant Supervision for Relation Extraction with Application to Sentence-Level Relation Retrieval.,<TITLE>Learning Relation Extraction from Distant Supervision with Multi-Instance Multi-label Models<TITLE>Learning Relation Extraction from Distant Supervision with Multi-Instance Multi-Label Models<TITLE>Multi-instance Distant Supervision for Relation Extraction with Human-Labeled Data<TITLE>Multi-instance Distant Supervision for Relation Extraction with Human-Labeled Information<TITLE>Multi-instance Distant Supervision for Relation Extraction with Human Labeled Data,<TITLE>Multi-Instance Multi-Label Models for Distant Supervision<TITLE>A Multi-Instance Multi-Label Model for Distant Supervision<TITLE>A Multi-Instance Multi-Label Model for Relation Extraction<TITLE>Multi-Instance Multi-Label Models for Relation Extraction<TITLE>Multi-instance Multi-Label Models for Relation Extraction,<TITLE> Improving Distant Supervision for Relation Extraction<TITLE> Modeling Relation Extraction with Unlabeled Data<TITLE> Improving Distant Supervision for Relation Extraction by Using Unlabeled Data<TITLE> Modeling Relation Extraction with Weakly Supervised Label Data<TITLE> Improving Distant Supervision for Relation Extraction with Data from a Database,<TITLE>Relation-Level Distant Supervision with Multi-Instance Multi-label Models<TITLE>Relation-Level Distant Supervision Using Multi-Instance Multi-label Models<TITLE>Relation-Level Distant Supervision with Multi-Instance Multi-Label Models<TITLE>Relation-Level Distant Supervision with Multi-Instance Multi-label Model<TITLE>Relation-Level Distant Supervision Using Multi-Instance Multi-Label Models
34,A Pitfall and Solution in Multi-Class Feature Selection for Text Classification ,"Information Gain is a well-known and empirically proven method for high-dimensional feature selection. We found that it and other existing methods failed to produce good results on an industrial text classification problem. On investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes. In this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task. Based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods. Empirical evaluation on 19 datasets shows substantial improvements.",<TITLE>Applying Round-robin Scheduling to Information Gain for Text Classification<TITLE>Applying Round-robin Scheduling to Information Gain in Text Classification<TITLE>Applying Round-Robbin Scheduling to Information Gain for Text Classification<TITLE>Information Gain and Round-robin Scheduling for Text Classification<TITLE>Information Gain and Round-Robbin Scheduling for Text Classification,<TITLE>Information Gain: High-dimensional feature selection for text classification using round-robin Scheduling and Pitfall Problem Solving Problems<TITLE>Information Gain: High-dimensional feature selection for text classification using round-robin Scheduling and Pitfall Problem Solving Methods<TITLE>Information Gain: High-dimensional feature selection for text classification using round-robin Scheduling and Pitfall Problem in the Information<TITLE>Information Gain: High-dimensional feature selection for text classification using round-robin Scheduling and Pitfall Problem in industrial text<TITLE>Information Gain: High-dimensional feature selection for text classification using round-robin Scheduling and Pitfall Problem Solving Solutions,<TITLE>Round-robin Scheduling for High-Dimensional Feature Selection in Text Classification<TITLE>Round-robin Scheduling for High-Dimensional Feature Selection<TITLE>Information Gain and Round-robin Scheduling for Text Classification<TITLE>Round-robin Scheduling for High-Dimensional Feature Selection<TITLE>Round-robin Scheduling for High-Dimensional Feature Selection,<TITLE>Round-robin Scheduling for High-dimensional Feature Selection<TITLE>Round-robin scheduling for high-dimensional Feature Selection<TITLE>Round-robin Feature Selection for Industrial Text Classification<TITLE>Round-robin scheduling for high-dimensional feature selection<TITLE>Round-robin Feature Selection with Information Gain,<TITLE> A Pitfall-Free Approach to Feature Selection for Text Classification<TITLE> A Pitfall-Free Approach to Feature Selection in Text Classification<TITLE> A Pitfall-Free Solution to Feature Selection in Text Classification<TITLE> Efficient Feature Selection for Text Classification<TITLE> Towards Better Feature Selection for Text Classification,<TITLE>Round-robin Scheduling for Information Gain-based Text Classification<TITLE>Information Gain for Text Classification: A Tale of Two Heads<TITLE>Information Gain for Text Classification: A Tale of Two Towers<TITLE>Round-robin Scheduling for Text Classification with Information Gain<TITLE>Round-Robin Scheduling for Text Classification
35,Neighborhood Mixture Model for Knowledge Base Completion,"Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransEâa well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.",<TITLE>Entity Embedding as a Mixture of Neighborhoods in Knowledge Base<TITLE>Entity Embedding as a Mixture of Neighborhood and Knowledge Bases<TITLE>Entity Embedding as a Mixture of Neighborhood and Knowledge Base<TITLE>Entity Embedding as a Mixture of Knowledge Bases<TITLE>Entity Embedding as a Mixture of Neighborhoods,<TITLE>Neighborhood Representations for Knowledge Base Completion: A Mixture of Its Neighborhood in the Knowledge Base and TransE Model<TITLE>Neighborhood Representations for Knowledge Base Completion: A Mixture of Its Neighborhood in the Knowledge Base and TransE Domain<TITLE>Neighborhood Representations for Knowledge Base Completion: A Mixture of Its Neighborhood in the Knowledge Base and TransE Entity<TITLE>Neighborhood Representations for Knowledge Base Completion: A Mixture of Its Neighborhood in the Knowledge Base and TransE-<TITLE>Neighborhood Representations for Knowledge Base Completion: A Mixture of Its Neighborhood in the Knowledge Base and TransE Rel,<TITLE>Neighborhood Information Improves TransE: A Knowledge Base Completion Model<TITLE>Neighborhood Information Improves TransE: A Knowledge Base Completion Model<TITLE>Neighborhood Information Improves TransE: A Knowledge Base Completion Model<TITLE>Neighborhood Information for Knowledge Base Completion<TITLE>Neighborhood Information for Knowledge Base Completion,<TITLE>TransE: Embedding Neighborhoods for Knowledge Base completion<TITLE>Embedding Neighborhoods for Knowledge Base Fertilization<TITLE>Embedding Neighborhoods for Knowledge Base Completion<TITLE>Embedding Neighborhoods for Knowledge Base Fertility<TITLE>A Neighborhood Representation for Knowledge Base Completion,<TITLE> Entity Representations for Knowledge Base Completion<TITLE> Entity Representation for Knowledge Base Completion<TITLE> Mixture of Neighborhood Representations for Knowledge Base Completion<TITLE> Entity Embedding Modeling for Knowledge Base Completion<TITLE> Entity Representation for Knowledge Base Completion with Neighborhood Information,<TITLE>A Novel Entity Representation as a mixture of its neighborhood in knowledge base<TITLE>A Novel Entity Representation as a mixture of its Neighborhood in Knowledge Base<TITLE>Entity Representation as a mixture of its neighborhood in knowledge base<TITLE>A Novel Entity Representation as a Neighborhood for TransE<TITLE>Entity Representation as a Neighborhood in Knowledge Base
36,TypeSQL: Knowledge-Based Type-Aware Neural Text-to-SQL Generation,"Interacting with relational databases through natural language helps users with any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper, we present a novel approach TypeSQL which formats the problem as a slot filling task in a more reasonable way. In addition, TypeSQL utilizes type information to better understand rare entities and numbers in the questions. We experiment this idea on the WikiSQL dataset and outperform the prior art by 6% in much shorter time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL can reach 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.",<TITLE>TypeSQL: A Slot Filling Approach to Interacting with Relational Databases<TITLE>TypeSQL: A Novel Slot Filling Approach to Interacting with Relational Data<TITLE>TypeSQL: A Slot Filling Approach to Interacting with Relational Data<TITLE>TypeSQL: A Slot Filling Approach to Interacting with Database Queries<TITLE>TypeSQL: Unsupervised Query Filling with Type Information,<TITLE>TypeSQL: Slot Filling for Relational Database Query Retrieval via Schema Type-SQL Formalization and Information<TITLE>TypeSQL: Slot Filling for Relational Database Query Retrieval via Schema Type-SQL Formalization and Sem<TITLE>TypeSQL: Slot Filling for Relational Database Query Retrieval via Schema Type-SQL Formalization of Information<TITLE>TypeSQL: Slot Filling for Relational Database Query Retrieval via Schema Type-SQL Formalization with Application<TITLE>TypeSQL: Slot Filling for Relational Database Query Retrieval via Schema Type-SQL Formalization and Content,<TITLE>TypeSQL: A Slot Filling Task for Natural Language SQL Queries<TITLE>TypeSQL: A Slot Filling Task for Natural Language SQL Questions<TITLE>TypeSQL: A Slot Filling Task for Natural Language Queries<TITLE>TypeSQL: A Slot Filling Task for Natural Language Queries<TITLE>TypeSQL: A Slot Filling Task for Natural Language Queries,<TITLE>TypeSQL: A Slot Filling Approach to Interacting with Relational Databases<TITLE>TypeSQL: A Slot Filling Approach for Interacting with Relational Databases<TITLE>TypeSQL: A Slot Filling Approach to Relational Databases via Natural Language<TITLE>TypeSQL: A Slot Filling Approach to Relational Databases with Natural Language<TITLE>TypeSQL: A Slot Filling Approach to Interacting with Relational Data,<TITLE> TypeSQL: A Slot Filling Approach for Relational Databases<TITLE> TypeSQL: A Slot Filling System for Relational Databases<TITLE> TypeSQL: Slot Filling for Relational Databases<TITLE> TypeSQL: A Slot Filling Approach for Relational Database Understanding<TITLE> TypeSQL: A Slot Filling Approach,<TITLE>Type Interact: Learning to Interact with Natural Language via Slot Filling and Type Information<TITLE>Type Interact: A Slot Filling Approach to Interacting with Semantic Database through Type<TITLE>Type Interact: A Slot Filling Approach to Interacting with Natural Language via Type<TITLE>Type Interact: A Slot Filling Approach to Interacting with Semantic Database<TITLE>Type Interact: A Slot Filling Approach to Interacting with Semantics
37,Transition-based Neural Constituent Parsing,"Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters.",<TITLE>Transition-Based Neural Constituent Parsing with Neural Network Structure<TITLE>Transition-based Neural Constituent Parsing with Structured Data<TITLE>Transition-Based Constituent Parsing with Neural Network Structure<TITLE>Transition-based Constituent Parsing with Neural Network Structure<TITLE>Transition-Based Neural Constituent Parsing,<TITLE>Transition-based Neural Constituent Parsing with Rich Syntactic Structures and Memory of Long Heterosyllabic<TITLE>Transition-based Neural Constituent Parsing with Rich Syntactic Structures and Memory of Long Heterosyntactic Information<TITLE>Transition-based Neural Constituent Parsing with Rich Syntactic Structures and Memory of Long Heteroscedastic Memories<TITLE>Transition-based Neural Constituent Parsing with Rich Syntactic Structures and Memory of Long Heterosyntactic Phen<TITLE>Transition-based Neural Constituent Parsing with Rich Syntactic Structures and Memory of Long Heteroscedasticity,<TITLE>Transition-based Neural Constituent Parsing<TITLE>Transition-based Neural Constituent Parsing<TITLE>Transition-based Neural Constituent Parsing<TITLE>Transition-based Neural Constituent Parsing<TITLE>Transition-Based Neural Constituent Parsing,<TITLE>Transition-based Constituent Parsing with Neural Networks<TITLE>Transition-Based Constituent Parsing with Neural Networks<TITLE>Transition-based Constituent Parsing with Neural Network Structure<TITLE>Transition-Based Constituent Parsing with Neural Network Structure<TITLE>Transition-Based Neural Constituent Parsing,<TITLE> Neural Constituent Parsing with Rich Syntactic Features<TITLE> Neural Transition-based Constituent Parsing<TITLE> Neural Transition-Based Constituent Parsing<TITLE> Neural Constituent Parsing with Rich Syntactic Information<TITLE> Neural Constituent Parsing with Rich Syntactic Structure,<TITLE>Transition-based Neural Constituent Parsing with Stack and Queue<TITLE>Transition-Based Neural Constituent Parsing with Stack and Queue<TITLE>Transition-based Neural Constituent Parsing<TITLE>Transition-Based Neural Constituent Parsing<TITLE>Neural Transition-based Constituent Parsing
38,cpSGD: Communication-efficient and differentially-private distributed SGD,"Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For d variables and n â¡ d clients, the proposed method uses O(log log(nd)) bits of communication per client per coordinate and ensures constant privacy. We also improve previous analysis of the Binomial mechanism showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest.",<TITLE>Communication Efficiency and Differential Privacy for Distributed Stochastic Gradient Descent<TITLE>Communication Efficient Distributed Stochastic Gradient Descent via Binomial Methods<TITLE>Communication Efficient Distributed Stochastic Gradient Descent for Mobile Devices<TITLE>Communication Efficient Distributed Gradient Descent via Binomial Methods<TITLE>Communication Efficient Distributed Stochastic Gradient Descent,<TITLE>Communication Efficiency and Differential Privacy in Distributed Stochastic Gradient Descent using O(log Log(nd))<TITLE>Communication Efficiency and Differential Privacy in Distributed Stochastic Gradient Descent using O(log Log(nd)<TITLE>Communication Efficiency and Differential Privacy in Distributed Stochastic Gradient Descent Using O(log Log(nd))<TITLE>Communication Efficiency and Differential Privacy in Distributed Stochastic Gradient Descent Using O(log Log(nd)<TITLE>Communication Efficiency and Differential Privacy in Distributed Stochastic Gradient Descent via O(log Log(nd)),<TITLE>Communication Efficient Distributed Stochastic Gradient Descent for Mobile Devices<TITLE>Communication Efficient Distributed Stochastic Gradient Descent with Constant Privacy<TITLE>Communication Efficient Distributed Gradient Descent with Privacy Guarantees<TITLE>Communication and Privacy in Distributed Gradient Descent<TITLE>Communication and Privacy in Distributed Gradient Descent,<TITLE>Communication Efficiency and Differential Privacy in Distributed Learning.<TITLE>Communication Efficiency and Differential Privacy in Distributed Learning<TITLE>Communication Efficiency and Differential Privacy for Distributed Learning<TITLE>Distributed Stochastic Gradient Descent<TITLE>Communication Efficiency and Differential Privacy,<TITLE> Communication Efficient Distributed Gradient Descent<TITLE> Communication Efficient Distributed Stochastic Gradient Descent<TITLE> Communication Efficient Subroutine for Distributed Gradient Descent<TITLE> Communication Efficient Subroutine for Distributed Learning<TITLE> Communication Efficient Distributed Gradient Descent with Privacy Guarantees,<TITLE>Communication Efficiency and Differential Privacy for Distributed Gradient Descent<TITLE>Communication Efficiency and Differential Privacy in Distributed Gradient Descent<TITLE>Communication and Differential Privacy for Distributed Gradient Descent<TITLE>Communication Efficiency and Differential Privacy for Distributed Learning<TITLE>Communication Efficiency and Differential Privacy in Distributed Learning
39,On Range of Skill,"At AAAIâ07, Zinkevich, Bowling and Burch introduced the Range of Skill measure of a two-player game and used it as a parameter in the analysis of the running time of an algorithm for finding approximate solutions to such games. They suggested that the Range of Skill of a typical natural game is a small number, but only gave heuristic arguments for this. In this paper, we provide the first methods for rigorously estimating the Range of Skill of a given game. We provide some general, asymptotic bounds that imply that the Range of Skill of a perfectly balanced game tree is almost exponential in its size (and doubly exponential in its depth). We also provide techniques that yield concrete bounds for unbalanced game trees and apply these to estimate the Range of Skill of Tic-Tac-Toe and Heads-Up Limit Texas Holdâem Poker. In particular, we show that the Range of Skill of Tic-Tac-Toe is more than 100,000.",<TITLE>Estimating the Range of Skill of a Natural Game Tree<TITLE>Estimating the Range of Skill of a Game Tree.<TITLE>Estimating the Range of Skill of a Game Tree<TITLE>Estimating the Range of Skill of a Natural Game<TITLE>Estimating the Range of Skill of Natural Games,<TITLE>Estimating the Range of Skill in a Competitive Two-Player Game: Some Asymptotic Bounds and General Asym<TITLE>Estimating the Range of Skill in a Competitive Two-Player Game: Some Asymptotic Bounds and Concrete B<TITLE>Estimating the Range of Skill in a Competitive Two-Player Game: Some Asymptotic Bounds and Constraints<TITLE>Estimating the Range of Skill in a Competitive Two-Player Game: Some Asymptotic Bounds and Methods for Un<TITLE>Estimating the Range of Skill in a Competitive Two-Player Game: Some Asymptotic Bounds and Methods for Better,<TITLE>Estimating the Range of Skill of a Natural Game<TITLE>Estimating the Range of Skill of a Natural Game<TITLE>Estimating the Range of Skill of a Game<TITLE>Estimating the Range of Skill of a Natural Game<TITLE>Estimating the Range of Skill of a Natural Game,<TITLE>Estimating the Range of Skill of a Natural Game<TITLE>Estimating the Range of Skill of a Game Tree<TITLE>Estimating the Range of Skill of a Game<TITLE>The Range of Skill of a Natural Game Tree<TITLE>The Range of Skill of a Natural Game,<TITLE> Estimating the Range of Skill of a Two-Player Game<TITLE> Estimating the Range of Skill of Natural Games<TITLE> The Range of Skill of Tic-Tac-Toe<TITLE> The Range of Skill of Tic-Toe<TITLE> Estimating the Range of Skill of a Two-Player Game with Applications to Natural Language Processing,<TITLE>On the Range of Skill of Natural Games<TITLE>On the Range of Skill in Natural Games<TITLE>The Range of Skill of a Game Tree<TITLE>The Range of Skill of Natural Games<TITLE>The Range of Skill of a Game
40,Stack-propagation: Improved Representation Learning for Syntax,"Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call âstack-propagationâ. We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.",<TITLE>Stack Propagation for Dependency Parsing and Tagging with POS Taggers<TITLE>Stack-propagation for Dependency Parsing and Tagging with POS Tags<TITLE>Stack-propagation for Dependency Parsing and Tagging<TITLE>Learning Stack Propagation for Dependency Parsing and Tagging<TITLE>Stack Propagation for Dependency Parsing and Tagging,<TITLE>Learning Stack-propagation for Dependency Parsing and Tagging with POS Taggers from Hand-tuned Templates<TITLE>Learning Stack-propagation for Dependency Parsing and Tagging with POS Taggers from Hand-Tuned Templates<TITLE>Learning Stack-propagation for Dependency Parsing and Tagging with POS Tags from Hand-tuned Templates using<TITLE>Learning Stack-propagation for Dependency Parsing and Tagging with POS Tags from Hand-tuned Templates.<TITLE>Learning Stack-propagation for Dependency Parsing and Tagging with POS Taggers from Hand-tuned Template Data,<TITLE>Part-of-Speech Models for Dependency Parsing and Tagging with Stack Propagation<TITLE>Part-of-Speech Models for Dependency Parsing and Tagging via Stack Propagation<TITLE>Part-of-Speech Tagging for Dependency Parsing and Tagging<TITLE>Part-of-Speech Models for Dependency Parsing and Tagging<TITLE>Part-of-Speech Tagging for Dependency Parsing and Tagging,<TITLE>Learning Part-of-Speech Representations from Hand-Tuned Templates<TITLE>Learning Part-of-Speech Representations from Hand-tuned Templates<TITLE>Learning Part-of-Speech Information from Hand-Tuned Templates<TITLE>Learning Part-of-Speech Information from Hand-tuned Templates<TITLE>Learning Part-of-Speech Representations with Hidden Layers,<TITLE> Stack-propagation: Learning Syntax Models with Rich Representations<TITLE> Stack-propagation: Learning Structured Representations for Syntax Models<TITLE> Stack-propagation: Learning Structured Representations of Syntax<TITLE> Stack-propagation: Learning Structured Representations of Syntax Models<TITLE> Stack-propagation: Learning Structured Representations for Parsing,<TITLE>Stack-Propagation: Learning POS-based Representations for Dependency Parsing<TITLE>Stack-Propagation for Universal Dependency Parsing and Tagging with POS Tags<TITLE>Stack-Propagation for Universal Dependency Parsing and Tagging<TITLE>Stack-Propagation for Universal Dependency Parsing and Tagging.<TITLE>Stack-Propagation for Universal Dependency Parsing
41,Dynamic Feature Selection for Dependency Parsing,"Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.",<TITLE>Fast Dynamic Feature Selection for Graph-based Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsing with Imitation Learning<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing<TITLE>Fast Dynamic Feature Selection for Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsing,<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing with Imitation Learning and Sequential Decision-Making Techniques on 7 Languages<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing with Imitation Learning and Sequential Decision-Making Techniques on Multilingual<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing with Imitation Learning and Sequential Decision-Making Techniques on the Web<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing with Imitation Learning and Sequential Decision-Making Techniques on Lingu<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing with Imitation Learning and Sequential Decision-Making Techniques on Larger,<TITLE>Dynamic Feature Selection for Dependency Parsing with Imitation Learning<TITLE>Dynamic Feature Selection for Dependency Parsing with Imitation Learning<TITLE>Dynamic Feature Selection for Fast Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsing,<TITLE>Dynamic Feature Selection for Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsing.<TITLE>Dynamic Parameter Selection for Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsers<TITLE>Dynamic Dependency Parsing,<TITLE> Dynamic Feature Selection for Graph-based Dependency Parsing<TITLE> A Fast Dynamic Feature Selection Framework for Graph-based Dependency Parsing<TITLE> Fast Feature Selection for Graph-based Dependency Parsing<TITLE> Dynamic Feature Selection for Dependency Parsing<TITLE> Fast Dynamic Feature Selection for Graph-based Dependency Parsing,<TITLE>Dynamic Feature Selection for Graph-based Dependency Parsing<TITLE>Dynamic Feature Selection for Dependency Parsing with Imitation Learning<TITLE>Dynamic Feature Selection for Dependency Parsing Using Imitation Learning<TITLE>Dynamic Feature Selection for Dependency Parsing using Imitation Learning<TITLE>Fast Feature Selection for Dependency Parsing with Imitation Learning
42,Joint Inference for Fine-grained Opinion Extraction,"This paper addresses the task of finegrained opinion extraction â the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.",<TITLE>Joint Inference for Fine-Grained Opinion Extraction<TITLE>A Joint Inference Model for Fine-Grained Opinion Extraction<TITLE>Joint Inference Model for Fine-Grained Opinion Extraction<TITLE>A Joint Inference Model for Fine-grained Opinion Extraction<TITLE>Joint Inference for Fine-grained Opinion Extraction,<TITLE>Joint Inference for Fine-grained Opinion Extraction: Identifying Opinion Entities and Relations in a Pipelined<TITLE>Joint Inference for Fine-grained Opinion Extraction: Identifying Opinions and their Targets in a Pip<TITLE>Joint Inference for Fine-grained Opinion Extraction: Identifying Opinions and their Targets in the Presence<TITLE>Joint Inference for Fine-grained Opinion Extraction: Identifying Opinions and their Targets in the Qu<TITLE>Joint Inference for Fine-grained Opinion Extraction: Identifying Opinion Entities and Relations in a Pipelines with,<TITLE>Joint Inference for Fine-Grained Opinion Extraction<TITLE>Joint Inference for Fine-grained Opinion Extraction<TITLE>Joint Inference for Fine-Grained Opinion Extraction<TITLE>Joint Inference for Fine-Grained Opinion Extraction<TITLE>Joint Inference for Fine-Grained Opinion Extraction,<TITLE>Joint Inference for Finegrained Opinion Extraction<TITLE>Joint Inference for Fine-Grained Opinion Extraction<TITLE>Joint Inference for Feingrained Opinion Extraction<TITLE>Joint Inference for Finegrained Opinion Extraction.<TITLE>Joint Inference for Fine Grained Opinion Extraction,<TITLE> Joint Inference for Fine-Grained Opinion Extraction<TITLE> Joint Inference for Fine-grained Opinion Extraction<TITLE> Joint Inference for Opinion Extraction<TITLE> Joint Inference for Fine-Grained Opinions and Opinions<TITLE> Joint Inference for Fine-Grained Opinion Extraction via Joint Inference,<TITLE>Fine-Grained Opinion Extraction with Joint Inference<TITLE>Joint Inference for Fine-Grained Opinion Extraction<TITLE>Fine-Grained Opinion Extraction via Joint Inference<TITLE>Joint Inference for Fine-grained Opinion Extraction<TITLE>Fine-grained Opinion Extraction via Joint Inference
43,Is Twitter A Better Corpus for Measuring Sentiment Similarity?,"Extensive experiments have validated the effectiveness of the corpus-based method for classifying the wordâs sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of peopleâs sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.",<TITLE>Evaluating the Performance of Different Corpora in Twitter Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Metric Classification<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora for Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Metrics,<TITLE>Evaluating the Performance of Twitter Sentiment Similarity Measurement for Word Polarity Classification: An Empirical Study on<TITLE>Evaluating the Performance of Twitter Sentiment Similarity Measurement for Word Polarity Classification: An Empirical Study of<TITLE>Evaluating the Performance of Twitter Sentiment Similarity Measurement for Word Polarity Classification: An Empirical Study with<TITLE>Evaluating the Performance of Twitter Sentiment Similarity Measurement for Word Polarity Classification: An Empirical Study Using<TITLE>Evaluating the Performance of Twitter Sentiment Similarity Measurement for Word Polarity Classification: An Empirical Study.,<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement for Word Polarity Classification<TITLE>Evaluating Different Corpora for Sentiment Similarity Measurement in Word Polarity Classification<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement,<TITLE>Evaluating the Performance of Twitter Corpora in Sentiment Similarity Classification<TITLE>Evaluating the Performance of Twitter Corpora in Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Classification<TITLE>Evaluating the Performance of Different Corpora in Sentiment Similarity Measurement<TITLE>Evaluating the Performance of Different Corpora in Sentiment Polarity Classification,<TITLE> Comparison of Corpora for Word Polarity Classification<TITLE> Comparison of Corpora for Polarity Classification<TITLE> A Comparison of Corpora for Polarity Classification<TITLE> A Comparison of Corpora for Sentiment Similarity Measures<TITLE> Comparison of Corpora for Sentiment Similarity Measures,<TITLE>An Empirical Study on the Performance of Twitter Data for Word Polarity Classification<TITLE>An Empirical Study on the Performance of Twitter Data for Sentiment Similarity Measurement<TITLE>An Empirical Study on the Performance of Twitter Data in Sentiment Similarity Measurement<TITLE>An Empirical Comparison of Twitter Data for Word Polarity Classification<TITLE>Comparing Twitter Corpora for Word Polarity Classification
44,Homophonic Pun Generation with Lexically Constrained Rewriting,"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.",<TITLE>Punning: A Computational Approach to Pun Modeling<TITLE>Punning: A Computational Approach to Pun Model Generation<TITLE>Generating Pun Sentences for Homophones<TITLE>Generating Pun Sentences with Explicit Sentences<TITLE>Punning with Explicit Sentences,<TITLE>Punning with Explicit Positive and Negative Constraints: An Error Analysis of Pun Generation for Novelty-Based Dialogues<TITLE>Punning with Explicit Positive and Negative Constraints: An Error Analysis of Pun Generation for Novelty-Based Conversational Writing<TITLE>Punning with Explicit Positive and Negative Constraints: An Error Analysis of Pun Generation for Novelty-Based Conversations in<TITLE>Punning with Explicit Positive and Negative Constraints: An Error Analysis of Pun Generation for Novelty-Based Conversations.<TITLE>Punning with Explicit Positive and Negative Constraints: An Error Analysis of Pun Generation for Novelty-based Conversations in,<TITLE>Generating Pun Sentences from Homophones with Explicit Constraints<TITLE>Generating Pun Sentences for Homophones with Explicit Constraints<TITLE>Generating Pun Sentences from Homophones with Constraint Words<TITLE>Punning with Positive and Negative Constraints<TITLE>Generating Pun Sentences from Homophones,<TITLE>Generating a Pun Sentence given a pair of homophones<TITLE>Generating a Pun Sentence for Literary Writing<TITLE>Generating a Pun Sentence for Punning<TITLE>Generating a Pun Sentence in Punning<TITLE>Generating Pun Sentences for Punning,<TITLE> Generating Pun Sentences with Explicit and Negative Constraints<TITLE> Generating Pun Sentences with Explicit Constraints<TITLE> Generating Pun Sentences from Homophones<TITLE> Generating Pun Sentences with Constraints<TITLE> Generating Pun Sentences Using Constraints,<TITLE>Generating Pun Sentences with Semantic Incongruity<TITLE>Generating Pun Sentences with Semantic Inconsistencies<TITLE>Generating Pun Sentences with Semantic Inconsistency<TITLE>Generating Pun Sentences with Constraints<TITLE>Generating Pun Sentences with Constraint Words
45,Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge,"In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness.",<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Sources<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Links<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Information<TITLE>Crosslingual Semantic Relatedness Using Wikipedia<TITLE>Crosslingual Semantic Relatedness via Wikipedia,<TITLE>Cross-lingual Semantic Relatedness via Web Embedding of Wikipedia Links to Multiple Languages using Interlanguage Knowledge Extraction<TITLE>Cross-lingual Semantic Relatedness via Web Embedding of Wikipedia Links to Multiple Languages Using Interlanguage Knowledge Extraction<TITLE>Cross-lingual Semantic Relatedness via Web Embedding of Wikipedia Links to Multiple Languages Using Interlanguage Knowledge from Wikipedia<TITLE>Cross-lingual Semantic Relatedness Using Wikipedia Information Extracted from Wikipedia for Cross-Lingual Similarity in Multiple<TITLE>Cross-lingual Semantic Relatedness Using Wikipedia Information Extracted from Wikipedia for Cross-Lingual Similarity in Multi,<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Links<TITLE>Crosslingual Semantic Relatedness Using Wikipedia<TITLE>Crosslingual Semantic Relatedness using Wikipedia Links<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Links<TITLE>Crosslingual Semantic Relatedness Using Wikipedia,<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Information<TITLE>Crosslingual Semantic Relatedness Using Wikipedia<TITLE>Crosslingual Semantic Relatedness using Wikipedia<TITLE>Crosslingual Semantic Relatedness<TITLE>Crosslingual semantic relatedness,<TITLE> Cross-lingual Semantic Relatedness Using Wikipedia<TITLE> Crosslingual Semantic Relatedness via Information Extraction<TITLE> Cross-Lingual Semantic Relatedness Using Wikipedia<TITLE> Cross-Lingual Semantic Relatedness<TITLE> Cross-lingual Semantic Relatedness,<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Links<TITLE>Crosslingual Semantic Relatedness using Wikipedia Links<TITLE>Cross-lingual Semantic Relatedness Using Wikipedia<TITLE>Crosslingual Semantic Relatedness Using Wikipedia Information<TITLE>Crosslingual Semantic Relatedness Using Wikipedia
46,Learning What to Talk about in Descriptive Games,"Text generation requires a planning module to select an object of discourse and its properties. This is specially hard in descriptive games, where a computer agent tries to describe some aspects of a game world. We propose to formalize this problem as a Markov Decision Process, in which an optimal message policy can be defined and learned through simulation. Furthermore, we propose back-off policies as a novel and effective technique to fight state dimensionality explosion in this framework.",<TITLE>Text Generation as a Markov Decision Process<TITLE>Markov Decision Processes for Text Generation<TITLE>Text Generation as Markov Decision Processes<TITLE>Text Generation with Markov Decision Processes<TITLE>Text Generation as Markov Decision Process,<TITLE>Markov Decision Processes for Text Generation in Descriptive Games: An Optimal Message Policy can be Defined and Simulation<TITLE>Markov Decision Processes for Text Generation in Descriptive Games: An Optimal Message Policy can be Defined and Learned<TITLE>Markov Decision Processes for Text Generation in Descriptive Games: An Optimal Message Policy can be Defined and Sim<TITLE>Markov Decision Processes for Text Generation in Descriptive Games: An Optimal Message Policy can be Defined and Optim<TITLE>Markov Decision Processes for Text Generation in Descriptive Games: An Optimal Message Policy can be Defined and Learning,<TITLE>Markov Decision Processes for Text Generation in Descriptive Games<TITLE>Markov Decision Processes for Text Generation and Descriptive Games<TITLE>Text Generation as Markov Decision Process with Back-Off Policies<TITLE>Markov Decision Processes for Text Generation and Descriptive Games<TITLE>Text Generation as Markov Decision Process with Back-Off Policies,<TITLE>A Markov Decision Process for Text Generation Using Planning<TITLE>Markov Decision Processes for Text Generation<TITLE>Markov Decision Processes in Text Generation<TITLE>A Markov Decision Process for Text Generation<TITLE>Markov Decision Process for Text Generation,<TITLE> A Markov Decision Process for Discourse Generation<TITLE> Markov Decision Processes for Discourse Generation<TITLE> A Markov Decision Process for Discourse Planning<TITLE> Planning for Discourse Generation<TITLE> Markov Decision Processes for Discourse Planning,<TITLE>Learning Optimal Message Policies for Text Generation in Games<TITLE>Learning Optimal Message Policy for Text Generation in Games<TITLE>Markov Decision Processes for Text Generation in Game Simulation<TITLE>Markov Decision Processes for Text Generation in Games.<TITLE>Markov Decision Processes for Text Generation in Games
47,Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation,"Sequence-to-sequence models for open-domain dialogue generation tend to favor generic, uninformative responses. Past work has focused on word frequency-based approaches to improving specificity, such as penalizing responses with only common words. In this work, we examine whether specificity is solely a frequency-related notion and find that more linguistically-driven specificity measures are better suited to improving response informativeness. However, we find that forcing a sequence-to-sequence model to be more specific can expose a host of other problems in the responses, including flawed discourse and implausible semantics. We rerank our model's outputs using externally-trained classifiers targeting each of these identified factors. Experiments show that our final model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses.",<TITLE>Linguistically-Driven Sequence-to-Sequence Model for Improved Response Informativeness and Reasonableness<TITLE>Linguistically-Driven Sequence-to-Sequence Models for Improved Response Informativeness and Reasonableness<TITLE>Linguistically-Driven Sequence-to-Sequence Modeling for Response Informativeness and Reasonableness<TITLE>Linguistically-Driven Sequence-to-Sequence Model for Response Informativeness and Reasonableness<TITLE>Linguistically-Driven Sequence-to-Sequence Modeling for Improved Response Informativeness,<TITLE>Linguistically-Driven Response Informativeness in Open-Domain Dialogue Generation Models with Frequency-Related Intrinsic<TITLE>Linguistically-Driven Response Informativeness in Open-Domain Dialogue Generation Models with Frequency-Related Intrinsism<TITLE>Linguistically-Driven Response Informativeness in Open-Domain Dialogue Generation Models with Frequency-Related Intra- and<TITLE>Linguistically-Driven Response Informativeness in Open-Domain Dialogue Generation Models with Frequency-Related Intrinsics<TITLE>Linguistically-Driven Response Informativeness in Open-Domain Dialogue Generation Models with Frequency-Related Intrinsically,<TITLE>Linguistically Motivated Informativeness and Plausibility Reranking for Sequence-to-Sequence Dialogue Generation<TITLE>Linguistically Motivated Informativeness and Plausibility Reranking for Sequence-to-Sequence Generation<TITLE>Linguistically Motivated Informativeness and Plausibility Reranking for Sequence-to-Sequence Dialogue Generation<TITLE>Linguistically Motivated Informativeness and Plausibility Reranking for Sequence-to-Sequence Generation<TITLE>Linguistically Motivated Informativeness and Plausibility Reranking for Dialogue Generation,<TITLE>Reranking Sequence-to-Sequence Models for Dialog Generation<TITLE>More Specificity in Sequence-to-Sequence Dialogue Generation<TITLE>More Linguistic Specificity in Open-Domain Dialogue Generation<TITLE>More Linguistic Specificity for Open-Domain Dialogue Generation<TITLE>More Specificity in Open-Domain Dialogue Generation,<TITLE> Linguistically-Driven Specificity for Open-Domain Dialogue Generation<TITLE> Improving Informative Dialogue Generation with Specificity Measures<TITLE> Exploiting Linguistic Knowledge for Improving Dialogue Generation<TITLE> Improving Informative Dialogue Generation with Linguistic-Driven Specificities<TITLE> Improving Informative Dialogue Generation with Linguistic-Driven Diversity Measures,<TITLE>Reranking Sequence-to-Sequence Dialogue Generation with Linguistic Information<TITLE>Reranking Sequence-to-Sequence Models for Response Generation<TITLE>Reranking Sequence-to-Sequence Dialogue Generation Models<TITLE>Reranking Sequence-to-Sequence Dialogue Generation<TITLE>Reranking Sequence-to-Sequence Dialogue Models
48,A Benchmark for Cooperative Learning Agents,"Introduction Cooperative multi-agent systems are of current interest due to their relevance to both robotics and networking. Researchers often create machine learning environments to explore these domains; however, the lack of reuse of previous environments prevents comparisons between the works of research groups. Further, while a growing understanding of the relationships between problem domains and machine learning techniques has emerged over time, there have been few attempts to systematically measure the performance of machine learning techniques as domain characteristics vary. Our primary goal is to create TASER, the Team Agent Simulator for Efficient Researchâa non-trivial simulation system aimed at the efficient comparison of machine learning algorithms given a wide variety of conditions, concentrating on cooperative tasks. Our secondary goal is the adoption of TASER by other researchers.",<TITLE>TASER: A Team Agent Simulator for Efficient Research<TITLE>TASER: Team Agent Simulator for Efficient Research<TITLE>TASER: The Team Agent Simulator for Efficient Research<TITLE>TASER: Team Agent Simulator for Efficient Research.<TITLE>TasER: Team Agent Simulator for Efficient Research,<TITLE>TASER: Team Agent Simulator for Efficient Research in Cooperative Multi-Agent Systems with Application to Collaborative Task Labeling<TITLE>TASER: Team Agent Simulator for Efficient Research in Cooperative Multi-Agent Systems with Application to Collaborative Multi-agent<TITLE>TASER: Team Agent Simulator for Efficient Research in Cooperative Multi-Agent Systems with Application to Collaborative Tasks.<TITLE>TASER: Team Agent Simulator for Efficient Research in Cooperative Multi-Agent Systems with Application to Collaborative Task Labelling<TITLE>TASER: Team Agent Simulator for Efficient Research in Cooperative Multi-Agent Systems with Application to Collaborative Multi-Ag,<TITLE>TASER: The Team Agent Simulator for Efficient Research<TITLE>TASER: The Team Agent Simulator for Efficient Research.<TITLE>TASER: Team Agent Simulator for Efficient Research.<TITLE>TASER: The Team Agent Simulator for Efficient Research<TITLE>TASER: Team Agent Simulator for Efficient Research,<TITLE>TASER: A Team Agent Simulator for Efficient Research<TITLE>TASER: The Team Agent Simulator for Efficient Research<TITLE>TASER: Team Agent Simulator for Efficient Research<TITLE>TASER: Team Agent Simulator for Efficient Research.<TITLE>Team Agent Simulator for Efficient Research,<TITLE> TASER: The Team Agent Simulator for Efficient Exploration<TITLE> TASER: The Team Agent Simulator for Efficient Exploration in Cooperative Multi-Agent Systems<TITLE> TASER: The Team Agent Simulator for Efficient Computation<TITLE> TASER: A Team Agent Simulator for Efficient Exploration<TITLE> TASER: The Team Agent Simulator,<TITLE>TASER: The Team Agent Simulator for Efficient Multi-Agent Learning<TITLE>TASER: The Team Agent Simulator for Efficient Multi-Agent Systems<TITLE>TASER: The Team Agent Simulator for Efficient Research<TITLE>TASER: A Non-trivial Simulator for Efficient Research<TITLE>TASER: A Team Agent Simulator for Efficient Research
49,Minimum Robust Multi-Submodular Cover for Fairness,"In this paper, we study a novel problem, Minimum Robust Multi-Submodular Cover for Fairness (MINRF), as follows: given a ground set V ; m monotone submodular functions f1, ..., fm; m thresholds T1, ..., Tm and a non-negative integer r, MINRF asks for the smallest set S such that for all i â [m], min|X|â¤r fi(S \X) â¥ Ti. We prove that MINRF is inapproximable within (1â ) lnm; and no algorithm, taking fewer than exponential number of queries in term of r, is able to output a feasible set to MINRF with high certainty. Three bicriteria approximation algorithms with performance guarantees are proposed: one for r = 0, one for r = 1, and one for general r. We further investigate our algorithmsâ performance in two applications of MINRF, Information Propagation for Multiple Groups and Movie Recommendation for Multiple Users. Our algorithms have shown to outperform baseline heuristics in both solution quality and the number of queries in most cases.",<TITLE>Minimum Robust Multi-Submodular Cover for Fairness<TITLE>Minimally Robust Multi-Submodular Cover for Fairness<TITLE>Minimum Robust Multi-Submodular Cover for Fairness.<TITLE>Maximum Robust Multi-Submodular Cover for Fairness<TITLE>Minimization of Submodular Cover for Fairness,<TITLE>Minimum Robust Multi-Submodular Cover for Fairness: Information Propagation and Bicriteria Approximation<TITLE>Minimum Robust Multi-Submodular Cover for Fairness: Information Propagation and Bicriteria Algorithms<TITLE>Minimally Robust Multi-Submodular Cover for Fairness: Information Propagation and Bicriteria App<TITLE>Minimally Robust Multi-Submodular Cover for Fairness: Information Propagation and Bicriteria Al<TITLE>Minimum Robust Multi-Submodular Cover for Fairness: Information Propagation and Bicriteria Approxim,<TITLE>Minimum Robust Multi-Submodular Cover for Fairness<TITLE>Minimum Robust Multi-Submodular Cover for Fairness.<TITLE>Minimum Robust Multi-Submodular Cover for Fairness<TITLE>Minimum Robust Multi-Submodular Cover for Fairness.<TITLE>Minimum Robust Multi-Submodular Cover for Fairness,<TITLE>MINRF: Minimum Robust Multi-Submodular Cover for Fairness<TITLE>Minimum Robust Multi-Submodular Cover for Fairness (MINRF)<TITLE>Minimum Robust Multi-Submodular Cover for Fairness<TITLE>Minimum Robust Multi-Submodular Cover for Fairness: A Case Study<TITLE>Minimally Robust Multi-Submodular Cover for Fairness,<TITLE> Minimum Robust Multi-Submodular Cover for Fairness<TITLE> Minimum Robust Multi-Submodular Cover<TITLE> MinRobust Multi-Submodular Cover for Fairness<TITLE> Minimum Robust Multi-Submodularity Cover for Fairness<TITLE> Minimum Robust Multi-Submodular Cover Minimization,<TITLE>Minimum Robust Multi-Submodular Cover for Fairness<TITLE>Minimum Robust Multi-Submodular Cover for Fairness.<TITLE>Minimum Robust Multi-Submodular Cover for Fairness Problem<TITLE>Minimum Robust Multi-Submodular Cover For Fairness<TITLE>Minimum Robust Multi-Modular Cover for Fairness
