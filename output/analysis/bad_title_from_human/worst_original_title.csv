,title,abstract,bart_base,bart_cnn,bart_xsum,t5_small,gpt2,pegasus_xsum
6,Inherent Disagreements in Human Textual Inferences,"We analyze human's disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation ""noise"", but rather persist as we collect more ratings and as we vary the amount of context provided to raters. We further show that the type of uncertainty captured by current state-of-the-art models for natural language inference is not reflective of the type of uncertainty present in human disagreements. We discuss implications of our results in relation to the recognizing textual entailment (RTE)/natural language inference (NLI) task. We argue for a refined evaluation objective that requires models to explicitly capture the full distribution of plausible human judgments.",<TITLE>A Refined Evaluation of Human's Discussions about Natural Language Inference<TITLE>A Refined Evaluation of Human's Discourse about Natural Language Inference<TITLE>The Uncertainty of Natural Language Inference: A Refined Evaluation<TITLE>Understanding Human's Discussions about Natural Language Inference<TITLE>The Uncertainty of Natural Language Inference,<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE/natural language<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based inference<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based Natural<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based natural<TITLE>An Analysis of Human's Disagreement about the validity of natural language inferring textual entailments in RTE-based In,<TITLE>Exploring Human Disagreement about the Validity of Natural Language Inference Tasks<TITLE>Human Disagreement about Natural Language Inference: A Case for Refined Evaluation Objective<TITLE>Exploring Human Disagreement about the Validity of Natural Language Inference<TITLE>Exploring Human Disagreement about Natural Language Inference<TITLE>Human Disagreement about Natural Language Inference,<TITLE>Understanding the Validity of Natural Language Inferences<TITLE>Uncertainty in Natural Language Inferences<TITLE>Uncertainty in Natural Language Inference<TITLE>Understanding the Validity of Natural Language Inference<TITLE>Analyzing Natural Language Inferences,<TITLE> Noise vs. Natural Language Inference: An Empirical Study<TITLE> Noise vs. Confident Natural Language Inference<TITLE> Noise vs. Natural Language Inference: An Empirical Analysis<TITLE> Noise vs. Natural Language Inference<TITLE> Are Natural Language Inference Systems Inconsistent?,<TITLE>Analyzing Human Disagreements about the validity of Natural Language Inference<TITLE>Analyzing Human's Disagreements about Natural Language Inference Models<TITLE>Analyzing Human's Disagreements about Natural Language Inference<TITLE>Analyzing Human Disagreements about Natural Language Inference Models<TITLE>Analyzing Human Disagreements about Natural Language Inference
33,Kernels for Multi--task Learning,"This paper provides a foundation for multi–task learning using reproducing kernel Hilbert spaces of vector–valued functions. In this setting, the kernel is a matrix–valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix– valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi–task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation.",<TITLE>Linear Multi-Task Learning with Matrix-Valued Kernels<TITLE>Linear Multi-task Learning with Matrix-Valued Kernels<TITLE>Linear Multi-Task Learning with Matrix-Valued Kernel Functions<TITLE>On the Representation of Matrix-Valued Kernels<TITLE>Kernel Representations of Vector-Valued Functions,<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernel Hilbert Spaces of Vector-valued Functions.<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions using Hilbert<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions using Reprodu<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions via Represent<TITLE>Multi-Task Learning with Minimal Norm Interpolation and Matrix-Valued Kernels of Vector-valued Functions via Hilbert,<TITLE>Multi-Task Learning with Reproducing Kernels<TITLE>Multi-Task Learning with Reproducing Kernel Hilbert Spaces<TITLE>Multi-task Learning with Reproducing Kernels<TITLE>Multi-Task Learning with Reproducing Kernels<TITLE>Multi-Task Learning with Reproducing Kernels,<TITLE>Multi-Task Learning using Replicating Kernel Hilbert Spaces of Vector–Valuated Functions<TITLE>Multi-Task Learning with Replicating Kernel Hilbert Spaces of Vector–Valuated Functions<TITLE>Multi-Task Learning using Replicating Kernel Hilbert Spaces of Vector–Valid Functions<TITLE>Multi-Task Learning with Replicating Kernels of Vector–Valued Functions<TITLE>Multi-Task Learning using Replicating Kernels of Vector–Valued Functions,<TITLE> Multi-Task Learning with Reproducing Kernel Hilbert Spaces<TITLE> Multitask Learning with Reproducing Kernel Hilbert Spaces<TITLE> Multi–Task Learning with Reproducing Kernel Hilbert Spaces<TITLE> Multi-Task Learning Using Reproducing Kernel Hilbert Spaces<TITLE> Multi–Task Learning Using Reproducing Kernel Hilbert Spaces,<TITLE>Multi-Task Learning with Matrix-Valued Kernels<TITLE>Multi-Task Learning with Matrix-valued Kernels<TITLE>On the Representation of Matrix-Valued Kernels<TITLE>On the Representation of Matrix-valued Kernels<TITLE>Learning Linear Multi-Task Kernels
39,Latent-Variable Synchronous CFGs for Hierarchical Translation,"Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs. In this paper, we extend these techniques to learn latent refinements of single-category synchronous grammars, so as to improve translation performance. We compare two estimators for this latent-variable model: one based on EM and the other is a spectral algorithm based on the method of moments. We evaluate their performance on a Chinese–English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches, but in particular the momentsbased estimator is both faster and performs better than EM.",<TITLE>Latent Refinement of Single-Category Synchronous Grammars for Machine Translation<TITLE>Latent refinements of single-category synchronous grammars for improving translation accuracy<TITLE>Latent refinements of single-category synchronous grammars for improving translation performance<TITLE>Latent Refinement of Synchronous Grammars for Statistical Machine Translation<TITLE>Latent Refinement of Synchronous Grammars for Machine Translation,<TITLE>Learning Latent Refinements of Single Category Synchronous Grammars for Statistical Machine Translation with Moments-Based PCFG<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars for Statistical Machine Translation with Moments-Based PC<TITLE>Learning Latent Refinements of Single Category Synchronous Grammars for Statistical Machine Translation with Moments-based PCFG<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars to Improve Statistical Machine Translation Performance with Moments-<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars for Statistical Machine Translation with Moments-Based Rates,<TITLE>Learning Latent Refinements of Synchronous Grammars for Statistical Machine Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Improved Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Machine Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Improved Translation<TITLE>Learning Latent Refinements of Synchronous Grammars for Statistical Machine Translation,<TITLE>Learning Latent Raffination of Single-Category Synchronous Grammars<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars<TITLE>Learning Latent Raffinations of Single-Category Synchronous Grammars<TITLE>Latent Raffination of Single-Category Synchronous Grammars<TITLE>Latent Refinement of Single-Category Synchronous Grammars,<TITLE> Latent Refinement of Synchronous Grammars for Machine Translation<TITLE> Learning Latent Refinement of Synchronous Grammars for Machine Translation<TITLE> Latent Refinement of Synchronous Grammars<TITLE> Learning Latent Refinement of Synchronous Grammars<TITLE> Learning Latent Refinements of Synchronous Grammars,<TITLE>Learning Latent Refinements of Synchronous Grammars for Statistical Machine Translation<TITLE>Learning Latent Refinements of Single-Category Synchronous Grammars<TITLE>Learning Latent Refinements for Single-Category Synchronous Grammars<TITLE>Learning Latent Refinements of Synchronous Grammars for Machine Translation<TITLE>Learning Latent Refinements for Synchronous Grammars
69,Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization,"Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation. We present HYPERBAND, a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound. HYPERBAND is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations. We compare HYPERBAND with popular Bayesian Optimization methods on several hyperparameter optimization problems. We observe that HYPERBAND can provide more than an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems.",<TITLE>HyPERBAND: Adaptive Resource Allocation for Dynamic Hyperparameter Optimization<TITLE>HyPERBAND: Adaptive Resource Allocation for Neural Hyperparameter Optimization<TITLE>HyPERBAND: Adaptive Resource Allocation for Hyperparameter Optimization.<TITLE>HyPERBAND: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>HyPERBAND: Adaptive Resource Allocation for Neural Network and Kernel Learning,<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stopp<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stopping<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation with Early-Stoppping Methods.<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stop<TITLE>HyPERBAND: Faster Hyperparameter Optimization by Adaptive Resource Allocation via Stochastic Early-Stopped,<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization.<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>Adaptive Resource Allocation for Hyperparameter Optimization.<TITLE>HyperBand: Adaptive Resource Allocation for Hyperparameter Optimization,<TITLE>HYPERBAND: Accelerating Random Search via Bayesian Optimization<TITLE>HYPERBAND: Accelerating Random Search through Bayesian Optimization<TITLE>HYPERBAND: A Bayesian Optimization Approach to Random Search<TITLE>HYPERBAND: A Bayesian Optimization Approach<TITLE>Bayesian Optimization for Hyperparameter Optimization,<TITLE> HyPERBAND: Adaptive Random Search for Hyperparameter Optimization<TITLE> HyPERBAND: Adaptive Resource Allocation for Hyperparameter Optimization<TITLE> HyPERBAND: Adaptive Random Search via Hyperparameter Optimization<TITLE> HyPERBAND: Adaptive Random Search via Adaptive Resource Allocation<TITLE> HyPERBAND: Adaptive Random Search Through Adaptive Resource Allocation,<TITLE>Adaptive Resource Allocation for Hyperparameter Optimization<TITLE>Adaptive Resource Allocation for Hyperparameter Learning<TITLE>Adaptive Resource Allocation for Hyperparameters<TITLE>Adaptive Resource Allocation for Hyperparameter optimization<TITLE>Fast Bayesian Hyperparameter Optimization
82,Using Search-Logs to Improve Query Tagging,"Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.",<TITLE>Part-of-Speech Tagging for Syntactic Search Queries<TITLE>Part-of-Speech Tagging for Syntactic Search Query Analysis<TITLE>Part-of-Speech Tagging for Syntactic Query Analysis<TITLE>Part-of-Speech Tagging for Search Queries<TITLE>Part-of-Speech Tagging for Search Query Analysis,<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Information Retrieval Queries<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Run-Time Information Retri<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Run-time Information Retri<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Information Retrieval to Qu<TITLE>Syntactic Query Analysis with Part-of-Speech Tagging and Efficient Training of Information Retrieval to Query,<TITLE>Part-of-Speech Tagging for Efficient Syntactic Query Analysis<TITLE>Part-of-Speech Transfer for Efficient Syntactic Query Analysis<TITLE>Part-of-Speech Tagging for Efficient Syntactic Query Analysis<TITLE>Part-of-Speech Tagging for Efficient Query Analysis<TITLE>Part-of-Speech Tagging for Efficient Syntactic Query Analysis,<TITLE>Syntactic Analysis of Search queries with Part-of-Speech Tags<TITLE>Syntactic Query Analysis with Part-of-Speech Tags<TITLE>Using Part-of-Speech Tags for Syntactic Search Analysis<TITLE>Syntactic Query Analysis with Part-of-Speech Tags.<TITLE>Syntactic Query Analysis with Part-of-speech Tags,<TITLE> Part-of-Speech Tagging for Search Queries<TITLE> Part-of-Speech Tagging for Query Analysis<TITLE> Part-of-Speech Tagging for Syntactic Query Analysis<TITLE> Part-of-Speech Tagging for Search Query Analysis<TITLE> Part-of-Speech Tagging for Search Queries via Transfer,"<TITLE>Training Query Analysis Models with Part-of-Speech Tagging Data<TITLE>A Simple, Efficient Approach to Part-of-Speech Search Comprehension<TITLE>A Simple, Efficient Approach to Part-of-Speech Search Segmentation<TITLE>Training Query Analysis Models with Part-of-Speech Tag Transfer<TITLE>Training Query Analysis Models with Part-of-Speech Tagging"
