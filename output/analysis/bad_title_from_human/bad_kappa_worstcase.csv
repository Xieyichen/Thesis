,title,abstract,bart_base,bart_cnn,bart_xsum,t5_small,gpt2,pegasus_xsum
1,Biomedical Entity Representations with Synonym Marginalization,"Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.",<TITLE>Modeling Biomedical Named Entity Normalization with Incomplete Synonyms<TITLE>Modeling Biomedical Named Entity Normalization from Incomplete Synonyms<TITLE>Modeling Biomedical Entity Normalization with Synonyms of Entities<TITLE>Modeling Biomedical Named Entity Normalization with Synonym Selection<TITLE>Modeling Biomedical Named Entity Normalization with Synonyms,<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood Maximization<TITLE>Learning Representations of Biomedical Named Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood Maxim<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood for Text<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximizing Marginal Likelihood for<TITLE>Learning Representations of Biomedical Entities from Incomplete Synonym-Based Candidate Selection and Maximum Marginal Likelihood for Normal,<TITLE>Biomedical Entity Normalization based on the Synonym Selection of Entities<TITLE>Biomedical Entity Normalization Based on the Synonym Selection of Entities<TITLE>Biomedical Entity Normalization Based on the Synonym of Entities<TITLE>Biomedical Entity Normalization Using Model-Based Synonym Selection<TITLE>Biomedical Entity Normalization Using Model-Based Candidate Selection,<TITLE>Learning Representations of Biomedical Entities based on Synonyms<TITLE>Learning Representations of Biomedical Entity based on Synonyms<TITLE>Learning Representations of Biomedical Entities Based on Synonyms<TITLE>Learning Biomedical Entity Representations based on Synonyms<TITLE>Learning Representations of Biomedical Entities from Synonyms,<TITLE> Learning Representations of Biomedical Entities Based on Synonyms<TITLE> Learning Representations of Biomedical Entities Based on Synonym Selection<TITLE> Learning Representations of Biomedical Entities with Synonyms<TITLE> Learning Representations of Biomedical Entities by Maximizing Marginal Likelihood<TITLE> Learning Representations of Biomedical Entities by Maximizing Marginal Probability,<TITLE>Learning Representations of Biomedical Named Entities from Incomplete Synonyms<TITLE>Learning Representations of Biomedical Named Expressions from Incomplete Synonyms<TITLE>Learning Representations of Biomedical Named Entities from Incomplete Synonym<TITLE>Learning Biomedical Entity Normalization from Incomplete Synonyms.<TITLE>Learning Biomedical Entity Normalization from Incomplete Synonyms
2,Shaping Visual Representations with Language for Few-Shot Classification,"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.",<TITLE>Language-shaped Learning for Few-Shot Visual Classification<TITLE>Language-shaped Learning for Few-shot Visual Classification<TITLE>Language-Shape Learning for Few-Shot Visual Classification<TITLE>Language-Shape Learning for Few-shot Visual Classification<TITLE>Language-shaped Learning for Few-Shot Image Classification,<TITLE>Language-Shared Few-shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Abstract<TITLE>Language-Shared Few-Shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Abstract<TITLE>Language-Shared Few-shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Text<TITLE>Language-Shared Few-Shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Text<TITLE>Language-Shared Few-shot Visual Classification with End-to-End Modeling of Natural Language Descriptors and Image,<TITLE>Language-Shaped Learning for Few-Shot Visual Classification<TITLE>Language-Shaped Learning for Few-shot Visual Classification<TITLE>Language-Shaped Learning for Few-Shot Classification<TITLE>Language-Shaped Learning for Few-Shot Visual Classification<TITLE>Language-Shaped Learning for Few-Shot Visual Classification,<TITLE>Language-shaped Learning for Few-Shot Visual Classification<TITLE>Language-shaped Learning for Few-shot Visual Classification<TITLE>Language-shaped Learning for Visual Classification<TITLE>Language-shaped Learning to Predict Language<TITLE>Language-shaped Learning,<TITLE> Language-Structure Learning for Few-Shot Visual Classification<TITLE> Language-Structure Learning for Few-shot Visual Classification<TITLE> Language-Oriented Few-Shot Visual Classification<TITLE> Language-Sensitive Few-Shot Visual Classification<TITLE> Language-Structure Learning for Visual Classification,<TITLE>Language-Shaped Learning for Few-Shot Visual Classification<TITLE>Language-Shaped Learning for Few-shot Visual Classification<TITLE>Language-shaped Learning for Few-Shot Visual Classification<TITLE>Language-shaped Learning for Few-shot Visual Classification<TITLE>Language-Shaped Learning for Few-Shot Visual Recognition
4,Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing,"A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times.",<TITLE>Estimating High Utility Measures for Psycholinguistic Modeling<TITLE>Estimating High Utility Measures for Psycholinguistic Models<TITLE>Estimating High Utility Measures from Incremental Parser Output<TITLE>Estimating High Utility Measures from Incremental Parsing Output<TITLE>Estimating High Utility Measures from Incremental Parsers,<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and Entropy Measures from a Single Incremental<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and Entropy Measures for Predicting Human Reading<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser using a Lexicalized PCFG and Ent<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and a Lexicalized PCFG-Based<TITLE>Incremental Measures of Psycholinguistic Modeling Using an Incremental Parser and a Lexicalized PCFG-based,<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling and Reading Times<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling and Prediction<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling<TITLE>Incremental Measures of High Utility in Psycholinguistic Modeling<TITLE>Incremental Measures of High Utility for Psycholinguistic Modeling,<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures from a Single Incremental Parser<TITLE>Calculing Separate Lexical and Syntactic Surprisal Measures from a Single Incremental Parser<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures from a Single Incentive Parser<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures from a Single Incentives Parser<TITLE>Calculating Separate Lexical and Syntactic Surprisal Measures using a Lexicalized PCFG,<TITLE> Incremental Parsing with Lexicalized PCFGs<TITLE> Incremental Parsing Using Lexicalized PCFGs<TITLE> Incremental Parsing with Lexicalized PCFG Measures<TITLE> Incremental Incremental Parsing with Lexicalized PCFG<TITLE> Incremental Incremental Parsing Using Lexicalized PCFG,<TITLE>Lexical and Syntactic Surprisal Measures from an Incremental PCFG<TITLE>Lexical and Syntactic Surprisal Measures for Incremental PCFGs<TITLE>Lexical and Syntactic Surprisal Measures of Incremental PCFG Output<TITLE>Lexical and Syntactic Surprisal Measures for Incremental PCFG Output<TITLE>Lexical and Syntactic Surprisal Measures for Incremental PCFG
5,Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction,"We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.",<TITLE>Learning Probabilistic Grammar Priors with Shared Logistic Normal Distributions<TITLE>Learning Probabilistic Grammars with Shared Logistic Normal Distributions<TITLE>Learning Probabilistic Grammars with Shared Logistic Normal Distributions.<TITLE>Learning Probabilistic Grammars using Shared Logistic Normal Distributions.<TITLE>Learning Probabilistic Grammars using Shared Logistic Normal Distributions,<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Probabilistic Priors over Probabil<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Variational EM Algorithms over Prob<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Probabilistic Priors over Belief Networks<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Probabilistic Priors over Belief Prop<TITLE>Unsupervised Dependency Grammar Induction with Shared Logistic Normal Distributions and Variational EM Algorithms over Pri,<TITLE>A Shared Logistic Normal Distribution for Learning Probabilistic Grammar<TITLE>A Shared Logistic Normal Distribution for Grammar Induction<TITLE>A Shared Logistic Normal Distribution for Grammar Induction<TITLE>A Shared Logistic Normal Distribution for Grammar Learning<TITLE>A Shared Logistic Normal Distribution for Grammar Induction,<TITLE>Partitioned Logistic Normal Distribution for Probabilistic Grammars<TITLE>Partitioned Logistic Normal Distribution of Probabilistic Grammars<TITLE>A Variational EM algorithm for learning Probabilistic Grammars<TITLE>Partitioned Logistic Normal Distribution for Probabilistic Grammar Knowledge<TITLE>Partitioned Logistic Normal Distribution for Probabilistic Grammar,<TITLE> Learning Probabilistic Grammars with Shared Logistic Normal Distributions<TITLE> Unsupervised Probabilistic Grammar Induction and Bilingual Learning<TITLE> Unsupervised Probabilistic Grammar Induction<TITLE> Learning Probabilistic Grammars with Shared Logistic Regular Distributions<TITLE> Unsupervised Dependency Grammar Induction and Bilingual Learning,<TITLE>A Shared Logistic Normal Distribution for Learning Probabilistic Grammars<TITLE>A Family of Priors over Probabilistic Grammar Weights<TITLE>A Shared Logistic Normal Distribution for Probabilistic Grammar Induction<TITLE>A Shared Logistic Normal Distribution for Probabilistic Grammars<TITLE>A Shared Logistic Normal Distribution for Unsupervised Grammar Induction
9,CancerEmo: A Dataset for Fine-Grained Emotion Detection,"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients. However, progress on this task has been hampered by the absence of large labeled datasets. To this end, we introduce CancerEmo, an emotion dataset created from an online health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best BERT model achieves an average F1 of 71%, which we improve further using domain-specific pre-training.",<TITLE>CancerEmo: A Dataset for Fine-Grained Emotions<TITLE>CancerEmo: A Dataset of Fine-Grained Emotions<TITLE>CancerEmo: An Emotion Dataset for Health and Safety<TITLE>CancerEmo: An Emotion Dataset for Health Domain<TITLE>CancerEmo: A Dataset for Emotion Detection,<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in Online Health Community<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in an Online Health<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in Online Health Communities<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in a Health Community<TITLE>C CancerEmo: A Dataset for Fine-Grained Emotion Detection and Domain Adaptation in Health Community Communities,<TITLE>CancerEmo: A Large-Scale Dataset for Emotion Detection in the Health Domain<TITLE>CancerEmo: A Dataset for Fine-Grained Emotion Detection in Health Domain<TITLE>CancerEmo: A Large-Scale Dataset for Emotion Detection in the Health Domain<TITLE>CancerEmo: A Dataset for Fine-Grained Emotion Detection in the Health Domain<TITLE>CancerEmo: A Dataset for Fine-grained Emotion Detection in the Health Domain,<TITLE>CancerEmo: An Emotion Dataset for Emotion Detection<TITLE>CancerEmo: A Dataset of Emotions from an Online Health Community<TITLE>CancerEmo: A Dataset for Emotion Detection in Health<TITLE>CancerEmo: An Emotion Dataset created from an Online Health Community<TITLE>CancerEmo: An Emotion Dataset created from an online health community,<TITLE> CancerEmo: A Fine-Grained Emotion Dataset for Healthcare<TITLE> CancerEmo: A Fine-Grained Emotion Dataset for Health Care<TITLE> CancerEmo: A Fine-Grained Emotion Dataset<TITLE> CancerEmo: A Fine-Grained Emotion Dataset for Medical Care<TITLE> CancerEmo: A Corpus-based Emotion Dataset,<TITLE>CancerEmo: A Fine-Grained Dataset for Emotion Detection in Health<TITLE>CancerEmo: A Dataset for Fine-grained Emotion Detection in Health Domains<TITLE>CancerEmo: A Dataset for Fine-grained Emotion Detection in Health<TITLE>CancerEmo: A Dataset for Fine-Grained Emotions in Health<TITLE>CancerEmo: A Health Dataset for Fine-grained Emotion Detection
10,Generalized Character-Level Spelling Error Correction,"We present a generalized discriminative model for spelling error correction which targets character-level transformations. While operating at the character level, the model makes use of wordlevel and contextual information. In contrast to previous work, the proposed approach learns to correct a variety of error types without guidance of manuallyselected constraints or language-specific features. We apply the model to correct errors in Egyptian Arabic dialect text, achieving 65% reduction in word error rate over the input baseline, and improving over the earlier state-of-the-art system.",<TITLE>A Discriminative Model for Spelling Error Correction in Arabic Dialect Text<TITLE>A Discriminative Model for Spelling Error Correction in Arabic Dialects<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>A Discriminative Model for Improved Spelling Error Correction<TITLE>Learning to Correct Spelling Errors in Arabic Dialect Text,<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: An Application<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: Learning Character<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: The Case<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectual Speech: Learning to<TITLE>A Generalized Discriminative Model for Spelling Error Correction in Arabic-Arabic Dialectal Speech: Learning Character,<TITLE>A Generalized Discriminative Model for Spelling Error Correction with Character-Level Information<TITLE>A Generalized Discriminative Model for Spelling Error Correction with Character-Level Transformations<TITLE>A Generalized Discriminative Model for Spelling Error Correction with Character-Level Transformation<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>A Generalized Discriminative Model for Correcting Spelling Errors,<TITLE>A Generalized Discriminative Model for Structural Error Correction<TITLE>A Generalized Discriminative Model for Correcting Errors in Arabic<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>A Discriminative Model for Correcting Errors in Arabic<TITLE>A Discriminative Model for Correcting Errors,<TITLE> Generalized Discriminative Model for Spelling Error Correction<TITLE> A Generalized Discriminative Model for Spelling Error Correction<TITLE> Generalized Discriminative Models for Spelling Error Correction<TITLE> Generalized Discriminative Modeling for Spelling Error Correction<TITLE> Generalized Discriminative Spelling Error Correction,<TITLE>Generalized Discriminative Model for Character-Level Spelling Error Correction<TITLE>A Generalized Discriminative Model for Character-Level Spelling Correction<TITLE>A Generalized Discriminative Model for Spelling Error Correction<TITLE>Generalized Discriminative Model for Spelling Error Correction<TITLE>Character-Level Spelling Error Correction
13,A Geometric Method to Construct Minimal Peer Prediction Mechanisms,"Minimal peer prediction mechanisms truthfully elicit private information (e.g., opinions or experiences) from rational agents without the requirement that ground truth is eventually revealed. In this paper, we use a geometric perspective to prove that minimal peer prediction mechanisms are equivalent to power diagrams, a type of weighted Voronoi diagram. Using this characterization and results from computational geometry, we show that many of the mechanisms in the literature are unique up to affine transformations, and introduce a general method to construct new truthful mechanisms.",<TITLE>Minimal Peer Prediction Mechanisms are Equivalent to Power Diagrams<TITLE>A Geometric Perspective on Minimal Peer Prediction via Power Diagrams<TITLE>A Geometric Perspective on Minimal Peer Prediction Mechanisms.<TITLE>A Geometric Perspective on Minimal Peer Prediction Mechanisms<TITLE>A Geometric View of Minimal Peer Prediction Mechanisms,<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited from Rational Agents: Structure and Affine Transformations<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited from Rational Agents: An Affine-Based Approach<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited From Rational Agents: An Affine-Based Approach<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited from Rational Agents: An Affine-Based Perspective<TITLE>Geometric Analysis of Minimal Peer Prediction Mechanisms Truthfully Elicited From Rational Agents: An Affine-Based Perspective,<TITLE>Minimal Peer Prediction Mechanisms are Weighted Voronoi Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Weighted Power Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Power Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Power Diagrams<TITLE>Minimal Peer Prediction Mechanisms are Power Diagrams,<TITLE>A Geometric Perspective for Minimal Peer Prediction Mechanisms<TITLE>A Geometric Perspective for Mindful Peer Prediction Mechanisms<TITLE>Geometric Perspectives for Minimal Peer Prediction Mechanisms<TITLE>A Geometric Perspective on Minimal Peer Prediction Mechanisms<TITLE>Geometry for Minimal Peer Prediction Mechanisms,<TITLE> Truthful Minimal Peer Prediction Mechanisms<TITLE> A Geometric Perspective on Minimal Peer Prediction Mechanisms<TITLE> A Geometric Perspective to Minimal Peer Prediction Mechanisms<TITLE> Efficient Minimal Peer Prediction Mechanisms<TITLE> Truthful Peer Prediction Mechanisms,<TITLE>Minimal Peer Prediction Mechanisms Are Like Power Diagram<TITLE>Truthful Minimal Peer Prediction via Geometric Power Diagram<TITLE>Minimal Peer Prediction Mechanisms as Power Diagram Models<TITLE>Minimal Peer Prediction Mechanisms as Power Diagram<TITLE>Truthful Minimal Peer Prediction Mechanisms
16,Faster and Simpler Algorithm for Optimal Strategies of Blotto Game,"In the Colonel Blotto game, which was initially introduced by Borel in 1921, two colonels simultaneously distribute their troops across different battlefields. The winner of each battlefield is determined independently by a winner-take-all rule. The ultimate payoff of each colonel is the number of battlefields he wins. This game is commonly used for analyzing a wide range of applications such as the U.S presidential election, innovative technology competitions, advertisements, etc. There have been persistent efforts for finding the optimal strategies for the Colonel Blotto game. After almost a century Ahmadinejad, Dehghani, Hajiaghayi, Lucier, Mahini, and Seddighin provided a poly-time algorithm for finding the",<TITLE>Finding Optimal Strategies for the Colonel Blotto Game<TITLE>The Colonel Blotto Game: Finding Optimal Strategies<TITLE>Finding Optimal Strategies for the Colonel Blotto Games<TITLE>Optimal Strategies for the Colonel Blotto Game<TITLE>The Colonel Blotto Game,<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Different Batt battlefield<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Multi-Field Games<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Multi-Field Differences<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Different Bounded Games<TITLE>The Colonel Blotto Game: Finding the optimal strategies for the Colonel-to-Colonot Strategy Across Different Battches,<TITLE>A Polynomial-Time Algorithm for the Colonel Blotto Game.<TITLE>A Polynomial-Time Algorithm for the Colonel Blotto Game<TITLE>A Poly-Time Algorithm for the Colonel Blotto Game.<TITLE>A Poly-Time Algorithm for the Colonel Blotto Game<TITLE>Finding Optimal Strategies for the Colonel Blotto Game,<TITLE>The Colonel Blotto Game: A Poly-Time Optimization Tool<TITLE>The Colonel Blotto Game: A Poly-Time Approach<TITLE>The Collectible Blotto Game<TITLE>The Collective Blotto Game<TITLE>The Colonel Blotto Game,<TITLE> Colonel Blotto: A Poly-Time Algorithm for Finding the Optimal Strategies for the Colored General<TITLE> Colonel Blotto: A Poly-Time Algorithm for Finding the Optimal Strategies for the Colored One<TITLE> Colonel Blotto: A Poly-Time Algorithm for Finding Generalized Strategies<TITLE> The Colonel Blotto Game<TITLE> Colonel Blotto: A Game-Theoretic Approach,<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies for the Colonel Blotto Game<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies in the Colonel Blotto Game<TITLE>The Colonel Blotto Game: A Poly-Time Algorithm for Finding the Optimal Strategies<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies of the Colonel Blotto Game<TITLE>A Poly-Time Algorithm for Finding the Optimal Strategies for the Colonel Blotto Games
18,#SupportTheCause: Identifying Motivations to Participate in Online Health Campaigns,"We consider the task of automatically identifying participants’ motivations in the public health campaign Movember and investigate the impact of the different motivations on the amount of campaign donations raised. Our classification scheme is based on the Social Identity Model of Collective Action (van Zomeren et al., 2008). We find that automatic classification based on Movember profiles is fairly accurate, while automatic classification based on tweets is challenging. Using our classifier, we find a strong relation between types of motivations and donations. Our study is a first step towards scaling-up collective action research methods.",<TITLE>Automatic Identification of Participants' Motives in the Public Health Campaign Movember<TITLE>Automatic Classification of Participants' Motives in the Public Health Campaign Movember<TITLE>Automatic Identification of Participants' Motives in the Movember Campaign<TITLE>Identifying Participants' Motives in the Public Health Campaign Movember<TITLE>Automatic Identification of Participants' Motives in Movember,<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Collective Action Research and Social Identity Model of Collective Action<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Collective Action Research and Social Identity Model of Movember<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Social Identity Model of Collective Action Profiles and Twe<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Social Identity Model of Collective Action Profiles and Automatic<TITLE>Automatic Classification of Participants' Motivations in Movember Campaigns Using Social Identity Model of Collective Action Profiles and Multi,<TITLE>Automatic classification of Movember participants' motivations and donations<TITLE>Automatic Classification of Movember Participants' Motives<TITLE>Automatic classification of Movember motivations and donations<TITLE>Automatic classification of motivations in Movember<TITLE>Automatic classification of Movember participants' motivations,<TITLE>Identifying Motivations in the Public Health Campaign Movember<TITLE>Identifying Motivations in Public Health Campaign Movember<TITLE>Automatic Identification of Motivations in Public Health Campaign Movember<TITLE>Identifying Motivations for Public Health Campaign Movember<TITLE>Automatic Classification for Public Health Campaign Movember,<TITLE> Automatic Identification of Participants' Motivations in the Campaign<TITLE> Automatic Identification of Participants' Motivations in Campaigns<TITLE> Automatic Identification of Political Motivations in Campaigns<TITLE> Automatic Characterization of Political Campaign Contributions<TITLE> Automatic Identification of Political Campaign Contributions,<TITLE>Identifying Participants' Motivatedness in the Movember Campaign<TITLE>Identifying Participants' Motivatedness in the Movember campaign<TITLE>Identifying Participants'  motivations in the Movember campaign<TITLE>Identifying Participants'  motivations for the Movember campaign<TITLE>Automatic Classification of Movember Participants'  motivations
19,Learning Invariant Representations Of Planar Curves,"We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Euclidean and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm.",<TITLE>Learning invariant geometric functions of planar curves for Euclidean and Similarity transformations<TITLE>Learning invariant geometric functions of planar curves with convolutional neural networks.<TITLE>Learning invariant geometric functions of planar curves with convolutional neural networks<TITLE>Learning Geometric Functions of Planar Curves with Convolutional Neural Networks<TITLE>Learning invariant geometric functions of planar curves,<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning with Convolutional Neural<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning: A Case Study on<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning: A Convex<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning: A Case Study in<TITLE>Metric Learning of Invariant Geometric Functions for Euclidean and Similarity Metric Learning with Convolutional Networks,<TITLE>Metric Learning for the Euclidean and Similarity Group of Transformations<TITLE>Metric Metric Learning for the Euclidean and Similarity Group<TITLE>Metric Learning for the Euclidean and Similarity Transformations<TITLE>Metric Learning for the Euclidean and Similarity Transformations<TITLE>Metric Learning of Similarity Metric Functions,<TITLE>Invariant Geometry of Planar Curves for Euclidean and Similarity<TITLE>Multi-Scale Simultaneous Learning for Planar Curves<TITLE>Multi-Scale Simultaneous Learning of Planar Curves<TITLE>Invariant Geometric Functions of Planar Curves<TITLE>Invariant Geometry of Planar Curves,<TITLE> A Metric Learning Framework for Planar Curves<TITLE> Semi-Supervised Learning of Invariant Geometric Functions<TITLE> Semi-Supervised Learning of Equivariant Geometric Functions<TITLE> Semi-supervised Learning of Equivariant Geometric Functions<TITLE> Semi-Supervised Learning of Equivariant Geometry,<TITLE>Similarity Metric Learning with Convolutional Neural Networks.<TITLE>Similarity Metric Learning via Convolutional Neural Networks.<TITLE>Similarity Metric Learning with Convolutional Neural Networks<TITLE>Similarity Metric Learning via Convolutional Neural Networks<TITLE>Metric Learning of Invariant Geometric Functions
20,Semantic parsing of speech using grammars learned with weak supervision,"Semantic grammars can be applied both as a language model for a speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations. Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases. Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision. We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors. Further, we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies, yielding an improvement in parsing performance over applying unweighted grammars.",<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognizer Grammars<TITLE>Induction of Semantic Speech Recognition Grammars under Weak Supervision<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision<TITLE>Weakly Supervised Induction of Semantic Speech Recognizers,<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing of Speech<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing in N<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing with N<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing of Sp<TITLE>Inducing Semantic Speech Recognition Grammars under Weak Supervision for Improved Parsing and Semantic Parsing in a,<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammars<TITLE>Weakly Supervised Semantic Speech Recognition and Parsing,<TITLE>Learning Semantic Speech Recognition Grammars under weak supervision<TITLE>Induction of Semantic Speech Recognition Grammars<TITLE>Semantic Speech Recognition Grammars under weak supervision<TITLE>Learning Semantic Speech Recognition Grammars<TITLE>Semantic Speech Recognition Grammars,<TITLE> Improving Semantic Speech Recognition Grammars with Weak Supervision<TITLE> Improving Semantic Speech Recognition Grammar Induction<TITLE> Improving Semantic Speech Recognition Grammars via Weak Supervision<TITLE> Improving Semantic Speech Recognition Grammars using Weak Supervision<TITLE> Inducing Semantic Speech Recognition Grammars,<TITLE>Weakly Supervised Induction of Semantic Speech Recognition Grammar<TITLE>Weakly-Supervised Induction of Semantic Speech Recognition Grammar<TITLE>Induction of Semantic Speech Recognition Grammars under Weak Supervision<TITLE>Weakly Supervised Semantic Speech Recognition Grammar Induction<TITLE>Induction of Semantic Speech Recognition Grammars
23,How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?,"This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer.",<TITLE>Hidden layers of deep rectifier networks are linearly separable<TITLE>The Universal Classification Power of Deep Rectifier Neural Networks<TITLE>The Universal Classification Power of Deep Rectifier Networks<TITLE>Distance preserving Deep rectifier networks<TITLE>Distance preserving Deep Rectifier Networks,<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via linear<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace using linear<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via is<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via non<TITLE>Distance preserving deep rectifier neural networks with hidden layers and maximum margin in the output layer of linear separable subspace via error,<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets into linearly separable units<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets into linearly separable ones<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets to linearly separable ones<TITLE>Hidden layers of deep rectifier neural networks can transform pattern sets to linearly separable patterns<TITLE>Hidden layers of deep rectifier neural networks are capable of linearly separating pattern sets.,<TITLE>Distance Preserving Deep Korrelier Neural Networks<TITLE>Distance preserving Deep Korrelier Neural Networks<TITLE>Distance preserving deep rectifier neural networks with hidden layers<TITLE>Distance preserving deep rectifier neural networks<TITLE>Distance preserving deep rectifier networks,<TITLE> Distance preserving deep rectifier networks<TITLE> On the Generalization Power of Distance Preserving Deep Rectifier Networks<TITLE> On the Generalization Power of Deep Rectifier Networks<TITLE> On the Generalization Power of Deep Separable Pattern Sets<TITLE> On the Generalization Power of Distance Preserving Deep RNNs,<TITLE>Universal Distance Preserving of Deep Neural Networks<TITLE>Universal Distance Preserving in Deep Neural Networks<TITLE>Universal Classification Power of Distance Preserving Networks<TITLE>Distance Preserving in Deep Neural Networks<TITLE>Distance Preserving of Deep Neural Networks
25,Structured Learning for Cell Tracking,We study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology. Reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research. Existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search. We propose a structured learning approach that allows to learn optimum parameters automatically from a training set. This allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences.,<TITLE>A Structured Learning Approach for Reliable Cell Tracking in Microscapes<TITLE>A Structured Learning Approach to Reliable Cell Tracking in Microscapes<TITLE>Structured Learning for Reliable Cell Tracking in Microscopic Images<TITLE>Structured Learning for Reliable Cell Tracking in Microscapes<TITLE>A Structured Learning Approach to Cell Tracking,<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes: A Structured Learning Approach<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes: A Structured Approach.<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes using Structured Random Fields.<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes: a Structured Approach.<TITLE>Structured Learning for Reliable Cell Tracking in Microscaling Time-Lapse Microscapes using Structured Random Fields and,<TITLE>Structured Learning for Cell Tracking in Time-Lapse Microscopic Images<TITLE>Structured Learning for Cell Tracking in Time-lapse Microscopic Images<TITLE>Structured Learning for Cell Tracking in Time-Lapse Microscopic Images<TITLE>Structured Learning for Cell Tracking in Time-lapse Microscopic Images<TITLE>Structured Learning for Large-Scale Cell Tracking,<TITLE>Learning to Track a Large Quantity of Homogeneous Objects<TITLE>Learning to Track a Large Quantum of Homogeneous Objects<TITLE>Learning to Track Homogeneous Objects in Cell Culture and Development Biology<TITLE>Learning to Track Homogeneous Objects in Cell Culture and Evolution Biology<TITLE>Learning to Track Homogeneous Objects via Structured Learning,<TITLE> Structured Learning for Cell Tracking in Time-Lapse Microarray Sequences<TITLE> Learning to Track Homogeneous Objects in Time-Lapse Microarray Sequences<TITLE> Learning to Track Homogeneous Objects with Structured Learning<TITLE> Learning to Track Homogeneous Objects in Microarray Sequences<TITLE> Structured Learning for Modeling Homogeneous Object Tracking,<TITLE>Learning to Track Cells in Time-lapse Microarrays<TITLE>Learning to Track Cells in Microscopic Time-lapse Images<TITLE>Learning to Track Cells in Microarray Time-lapses<TITLE>Learning to Track Cells in Microscopic Time-lapses<TITLE>Learning to Track Cells in Time-lapse Microarray Images
26,Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural Machine Translation,"We show that the state-of-the-art Transformer MT model is not biased towards monotonic reordering (unlike previous recurrent neural network models), but that nevertheless, long-distance dependencies remain a challenge for the model. Since most dependencies are short-distance, common evaluation metrics will be little influenced by how well systems perform on them. We therefore propose an automatic approach for extracting challenge sets rich with long-distance dependencies, and argue that evaluation using this methodology provides a complementary perspective on system performance. To support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.",<TITLE>Towards Automatic Evaluation of Machine Translation Systems<TITLE>Compiling Challenge Sets for Machine Translation Evaluation<TITLE>Compiling Challenge Sets for Neural Machine Translation<TITLE>Automatic Evaluation of Machine Translation Challenge Sets<TITLE>Compiling Challenge Sets for Machine Translation,<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-Art Trans<TITLE>Evaluating Machine Translation with Long-distance Dependencies: A Scalable Approach to the State-of-the-Art<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-art Trans<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-Art Transformers<TITLE>Evaluating Machine Translation with Long-distance Dependencies: An Automatic Approach to the State-of-the-Art MT,<TITLE>Automatic Evaluation of Transformer MT on the Long-Tail of Syntactic Phenomena<TITLE>Evaluating Transformer MT on the Long Tail of Syntactic Phenomena Using Challenge Sets<TITLE>Evaluating Transformer MT on the Long Tail of Syntactic Phenomena with Challenge Sets<TITLE>Automatic Evaluation of Transformer MT on the Long-tail of Syntactic Phenomena<TITLE>Automatic Evaluation of Transformer MT on Long-Tail Syntactic Phenomena,<TITLE>Extraction of Challenge Sets with Long-Distance Dependencies<TITLE>Extraction of Long-Distance Dependencies for Transformer MT<TITLE>Evaluating Long-Distance Dependencies for Transformer MT<TITLE>Automatic Extraction of Challenge Sets for Transformer MT<TITLE>Extraction of Challenge Sets for Transformer MT,<TITLE> Extracting Challenge Sets Rich with Long-Distance Dependencies<TITLE> Extracting Challenges Rich with Long-Distance Dependencies<TITLE> Extracting Challenges for Neural Machine Translation<TITLE> Extracting Challenge Sets Rich with Short-Distance Dependencies<TITLE> Extracting Challenge Sets from Recurrent Neural Networks,<TITLE>Automatic Evaluation of Long-Tailed Syntactic Challenges for Transformer MT<TITLE>Automatic Evaluation of Transformer MT with Long-distance Dependencies<TITLE>Automatic Evaluation of Long-distance Dependencies in Transformer MT<TITLE>Automatic Evaluation of Long-distance Dependencies for Transformer MT<TITLE>Automatic Evaluation of Transformer MT
27,Humor as Circuits in Semantic Networks,"This work presents a first step to a general implementation of the Semantic-Script Theory of Humor (SSTH). Of the scarce amount of research in computational humor, no research had focused on humor generation beyond simple puns and punning riddles. We propose an algorithm for mining simple humorous scripts from a semantic network (ConceptNet) by specifically searching for dual scripts that jointly maximize overlap and incongruity metrics in line with Raskin’s Semantic-Script Theory of Humor. Initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles. We evaluate the said metrics through a user-assessed quality of the generated two-liners.",<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor<TITLE>Mining Simple Humor with Semantic-Script Theory of Humor<TITLE>Mining Simple Humor from Semantic-Script Theory of Humor<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor.<TITLE>Mining Simple Humor with Semantic-Script Theory of Humor.,<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor with ConceptNet and Raskin’s Sem<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor via ConceptNet and Raskin’s Sem<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor via ConceptNet and Raskin's Semantic Network<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor from ConceptNet: A Relaxed Two-Line Approach<TITLE>Mining Simple Humor for Semantic-Script Theory of Humor with ConceptNet and Raskin's Semantic Network,<TITLE>Mining Semantic Scripts for Humor Generation<TITLE>Mining Semantic Scripts for Humor Generation<TITLE>Mining Semantic Scripts for Comedy Generation<TITLE>Mining Semantic Scripts for Humor Generation<TITLE>Mining Semantic Scripts for Humor Generation,<TITLE>Semantic-Script Theory of Humor: Exploiting Simple Humor from ConceptNet<TITLE>A Semantic-Script Theory of Humor Approach to Humor Generating<TITLE>A Semantic-Script Theory of Humor Approach to Humor Generation<TITLE>A Semantic-Script Theory of Humor Approach<TITLE>Semantic-Script Theory of Humor,<TITLE> Computational Linguistic Humor Mining<TITLE> Computational Linguistic Humor Mining from ConceptNet<TITLE> Semantic-Script Mining for Humor Generation<TITLE> Computational Linguistic Humor Mining Using ConceptNet<TITLE> Semantic-Script Mining for Generating Humor,<TITLE>Generating Two-liners from a Semantic Network<TITLE>Generating Two-liners from Semantic Networks<TITLE>Generating Two-liners from Semantic Content<TITLE>Mining Humor from a Semantic Network<TITLE>Generating Humor from Semantic Content
29,A Discriminative Training Procedure for Continuous Translation Models,"Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inference is to use them in an N -best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines.",<TITLE>Neural Rescoring for Continuous-Space Translation<TITLE>Rescoring Neural Networks for Continuous-Space Translation<TITLE>Rescoring Neural Network for Continuous-Space Translation<TITLE>Neural Rescoring for Continuous-space Translation<TITLE>Rescoring Neural Machine Translation Models,<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Resoring Algorithm and an Objective Function<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Resoring Algorithm and an N-<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Rescoring Algorithm and an Objective<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Rescoring Algorithm and a Joint<TITLE>Rescoring Neural Networks for Continuous-Space Translation Modeling: An N-best Rescoring Algorithm and an N,<TITLE>Rescoring with Joint Neural Networks for Continuous-Space Translation Models<TITLE>Rescoring in Continuous-Space Translation Models with Joint Inference<TITLE>Rescoring in Continuous-Space Translation Models with Joint Neural Networks<TITLE>Rescoring with Joint Neural Networks for Continuous-Space Translation<TITLE>Rescoring with Neural Networks for Continuous-Space Translation Models,<TITLE>Continuous-Space Translation Models in Inference<TITLE>Continuous-Space Translation Models with Objective Function<TITLE>Continuous-Space Translation Models for Inference<TITLE>Continuous-Sspace Translation Models<TITLE>Continuous-Space Translation Models,<TITLE> Jointly Training Neural Networks for Continuous-Space Translation<TITLE> Training Neural Networks for Continuous-Space Translation<TITLE> Joint Training of Neural Networks for Continuous-Space Translation<TITLE> Joint Training of Neural Network Models for Continuous-Space Translation<TITLE> Joint Training of Continuous-Space Translation Models,<TITLE>Jointly Training Neural Networks for Rescoring in Continuous-Space Translation<TITLE>Jointly Training Neural Networks for Improved Rescoring of Machine Translation Models<TITLE>Joint Training of Continuous-Space Translation Models for Improved Rescoring<TITLE>Joint Training of Continuous-Space Neural Networks for Improved Rescoring<TITLE>Jointly Training Neural Networks for Improved Rescoring
30,Interpreting and Boosting Dropout from a Game-Theoretic View,"This paper aims to understand and improve the utility of the dropout operation from the perspective of game-theoretic interactions. We prove that dropout can suppress the strength of interactions between input variables of deep neural networks (DNNs). The theoretic proof is also verified by various experiments. Furthermore, we find that such interactions were strongly related to the over-fitting problem in deep learning. Thus, the utility of dropout can be regarded as decreasing interactions to alleviate the significance of over-fitting. Based on this understanding, we propose an interaction loss to further improve the utility of dropout. Experimental results have shown that the interaction loss can effectively improve the utility of dropout and boost the performance of DNNs.",<TITLE>The Utility of Dropout in Deep Neural Networks via Interaction Loss<TITLE>Improving the Utility of Dropout in Deep Neural Networks<TITLE>Improved the Utility of Dropout in Deep Neural Networks<TITLE>The Utility of Dropout in Deep Neural Networks<TITLE>The Utility of Dropout for Deep Neural Networks,<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for Over-fitting DNN<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for DNNs using Drop<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for DNNs Using Drop<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for Over-fitting Games in<TITLE>Understanding and Improving the Utility of Deep Neural Networks with Game-Theoretic Interaction Loss for DNNs using an,<TITLE>The Utility of Dropout in Deep Neural Networks via Game-Theoretic Interactions<TITLE>The Utility of Dropout in Deep Neural Networks via Game-theoretic Interactions<TITLE>The Utility of Dropout in Deep Learning from the Perspective of Game Theory<TITLE>The Utility of Dropout in Deep Neural Networks via Interaction Loss<TITLE>The Utility of Dropout and Interaction Loss in Deep Neural Networks,<TITLE>Learning the Utility of Dropout from Game-Theoretic Interactions<TITLE>Game-Theoretic Interaction Loss for Deep Neural Networks<TITLE>Learning the Utility of Dropout from Game-Theoretic interactions<TITLE>Game-Theoretic Interaction Loss in Deep Learning<TITLE>Game-Theoretic Interaction Loss for Deep Learning,<TITLE> On the Utility of Dropout in Deep Learning<TITLE> On the Utility of Dropout for Deep Neural Networks<TITLE> On the Utility of Dropout in Deep Neural Networks<TITLE> Improving the Utility of Dropout in Deep Learning<TITLE> Improving the Utility of Dropout for Deep Learning,<TITLE>Understanding and Improving the Utility of Dropout in Deep Neural Networks<TITLE>Understanding and Improving the Utility of Deep Neural Networks via Interaction Loss<TITLE>Understanding and Improving the Utility of Deep Neural Network via Interaction Loss<TITLE>Understanding and Improving the Utility of Dropout in Deep Learning<TITLE>Understanding and Improving the Utility of Deep Neural Network dropout
31,LexSemTm: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning,"There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and optimises it for application to the entire vocabulary of a given language. The optimised method is then used to produce LEXSEMTM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polysemous, English simplex lemmas, which is released as a public resource to the community. Finally, the quality of this data is investigated, and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR, and at least on par with SEMCOR-based distributions otherwise.",<TITLE>LEXSEMTM: An Unsupervised Learning of Sense Distributions<TITLE>LEXSEMTM: An Unsupervised Method for Sense Distribution Learning<TITLE>LEXSEMTM: An Unsupervised Method for Learning Sense Distribution<TITLE>LEXSEMTM: A Large Scale Corpus for Sense Distribution Learning<TITLE>Unsupervised Sense Distribution Learning with LEXSEMTM,<TITLE>Optimising Sense Distributions Using LEXSEMTM: An Empirical Study of the State-of-the<TITLE>Optimising Sense Distributions Using LEXSEMTM: An Empirical Study on the State-of-the<TITLE>Optimising Sense Distributions with LEXSEMTM: An Empirical Study of the State-of-the<TITLE>Optimising Sense Distributions with LEXSEMTM: An Empirical Study on the State-of-the<TITLE>Optimising Sense Distributions Using LEXSEMTM: An Empirical Analysis of the State-of-the,<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning of Sense Distributions for English<TITLE>LEXSEMTM: A Large-Scale Semantic Dataset for Sense Distribution Learning<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning of Sense Distributions for English<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning Method for Sense Distributions<TITLE>LEXSEMTM: A Large-Scale Unsupervised Learning Method for Sense Distributions,<TITLE>LEXSEMTM: An Unsupervised Method for Learning Sense Distributions<TITLE>LEXSEMTM: An Unsupervised Method for Learning Sentiment Distribution<TITLE>LEXSEMTM: Unsupervised Learning of Sentiment Distributions<TITLE>LEXSEMTM: Unsupervised Learning of Sense Distributions<TITLE>Unsupervised Learning of Sense Distributions,<TITLE> Unsupervised Learning of Sense Distributions<TITLE> Unsupervised Sense Distribution Learning<TITLE> Unsupervised Learning of Word Sense Distributions<TITLE> Unsupervised Sense Distribution Learning Using Semantic Datasets<TITLE> Unsupervised Learning of Sense Distributions Using WordNet,<TITLE>LEXSEMTM: A Sense Distribution Model for the Entire Vocabulary of a Language<TITLE>LEXSEMTM: A Sense Distribution Dataset for the Entire Vocabulary of a Language<TITLE>LEXSEMTM: A Sense Distribution Dataset for Unsupervised Learning<TITLE>LEXSEMTM: A Sense Distribution Dataset of Unsupervised Learning<TITLE>LEXSEMTM: A Sense Distribution Dataset for English
35,Cross-domain Semantic Parsing via Paraphrasing,"Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.",<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing with Syntactic Paraphrase<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation Problem<TITLE>Cross-domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing with Domain Adaptation,<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation with Paraphrasing and Pre-Trained Word Embeddings<TITLE>Cross-Domain Semantic Parsing as a Domain Adaptation Problem with Paraphrasing and Pre-Trained Word Emb<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation in Natural Language using Paraphrasing and Pre-Trained Word<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation with Paraphrasing and Pre-Trained Word Embedding<TITLE>Cross-Domain Semantic Parsing as a Domain Adaptation Problem with Paraphrasing and Pre-Trained Word Represent,<TITLE>Cross-Domain Semantic Parsing with Paraphrasing<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation,<TITLE>Cross-Domain Semantic Parsing as a Domain adaptation problem<TITLE>Cross-Domain Semantic Parsing as a Domain adaptation Problem<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation Problem<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation<TITLE>Cross-Domain Semantic Parsing for Domain Adaptation,<TITLE> Cross-Domain Semantic Parsing as a Domain Adaptation Problem<TITLE> Domain Adaptation for Cross-Domain Semantic Parsing<TITLE> Cross-Domain Semantic Parsing with Neural Networks<TITLE> Cross-Domain Semantic Parsing as a Domain Adaptation Task<TITLE> Cross-Domain Semantic Parsing with Adaptive Neural Networks,<TITLE>Cross-Domain Semantic Parsing with Pre-trained Word Embedding<TITLE>Cross-Domain Semantic Parsing as a Domain Adaptation Problem<TITLE>Cross-Domain Semantic Parsing via Paraphrasing<TITLE>Cross-Domain Semantic Parsing with Paraphrasing<TITLE>Cross-Domain Semantic Parsing as Domain Adaptation
40,Synchronous Tree Adjoining Machine Translation,"Tree Adjoining Grammars have well-known advantages, but are typically considered too difficult for practical systems. We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable. Using adjoining to model optionality allows general translation patterns to be learned without the clutter of endless variations of optional material. The appropriate modifiers can later be spliced in as needed. In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data. We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding. Finally, we show that adjoining results in an end-to-end improvement of +0.8 BLEU over a baseline statistical syntax-based MTmodel on a large-scale Arabic/EnglishMT task.",<TITLE>Acquiring Synchronous Tree Adjoining Grammars with Application to Machine Translation<TITLE>Acquiring Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Acquiring Synchronous Tree Adjoining Grammars for Machine Translation<TITLE>Acquisition of Synchronous Tree Adjoining Grammars<TITLE>Acquiring Synchronous Tree Adjoining Grammars,<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-to-English<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-English Machine Translation<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-to-Tree<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding and Decoding in Machine<TITLE>Learning Tree Adjoining Grammars from Aligned Tree/String Training Data with Application to Decoding Arabic-to-Translation,<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical MT<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation,<TITLE>A Synchronous Tree Adjoining Grammar with Adjoining<TITLE>Learning Synchronous Tree Adjoining Grammars and associated Probabilities<TITLE>Learning Synchronous Tree Adjoining Grammars<TITLE>A Synchronous Tree Adjoining Grammar<TITLE>Tree Adjoining Grammars,<TITLE> Improving Syntax-Based Machine Translation with Aligned Tree Adjoining Grammars<TITLE> Improving Syntax-Based Statistical Machine Translation with Aligned Tree Adjoining Grammar<TITLE> Improving Syntax-Based Machine Translation with Aligned Tree Adjoining Grammar<TITLE> Improving Syntax-Based Statistical Machine Translation with Alignments<TITLE> Improving Syntax-Based Statistical Machine Translation by Splitting Alignments,<TITLE>Learning Synchronous Tree Adjoining Grammars for Statistical Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars with Model optionality<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation<TITLE>Learning Synchronous Tree Adjoining Grammars for Machine Translation Models<TITLE>Synchronous Tree Adjoining Grammars for Statistical Machine Translation
43,Robust Incremental Neural Semantic Graph Parsing,"Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.",<TITLE>Neural Transition-based Semantic Graph Parsing for Minimal Recursion Semantics<TITLE>Neural Transition-based Semantic Graph Parsing for Minimal Recursion Semantics.<TITLE>Neural Encoder-Decoder Transition-based Semantic Graph Parsing<TITLE>Semantic Graph Parsing with Stack-based Embedding Features<TITLE>Semantic Graph Parsing for Minimal Recursion Semantics,<TITLE>Neural Encoder-Decoder Transition-based Parsing of Minimal Recursion Semantic Parsers for Natural Language Processing<TITLE>Neural Encoder-Decoder Transition-based Parsing of Minimal Recursion Semantic Representations for Natural Language Processing<TITLE>Neural Encoder-Decoder Transition-based Parsing for Minimal Recursion Semantic Parsing with Stack Embedd<TITLE>Neural Encoder-Decoder Transition-based Parsing for Minimal Recursion Semantic Parsing of Sentences to<TITLE>Neural Encoder-Decoder Transition-based Parsing for Minimal Recursion Semantic Parsing with Stack-based,<TITLE>A Neural Transition-based Parser for Minimal Recursion Semantics<TITLE>A Neural Transition-Based Parser for Minimal Recursion Semantics<TITLE>A Neural Transition-based Parser for Minimal-Recursion Semantics<TITLE>A Neural Transition-based Parser for Minimal Recursion Semantics<TITLE>A Neural Transition-Based Parser for Minimal Recursion Semantics,<TITLE>Full-coverage Semantic Graph Parsing for Minimal Recursion Semantics<TITLE>Minimal Recursion Semantics with Neural Encoder-Decoder Transitions<TITLE>Minimal Recursion Semantics with Neural Encoder-Decoder Transition<TITLE>Minimal Recursion Semantics with Neural Graph Parsing<TITLE>Graph Parsing for Minimal Recursion Semantics,<TITLE> Neural Decoding for Minimal Recursion Semantics<TITLE> Minimal Recursion Semantic Graph Parsing<TITLE> Minimal Recursion Semantic Parsing<TITLE> Minimal Recursion Semantic Parsing with Neural Encoders<TITLE> Minimal Recursion Semantic Graph Parsing with Neural Encoders,<TITLE>Neural-Decoder Parsing for Minimal Recursion Representations<TITLE>Neural-Decoder Parsing for Minimal Recursions<TITLE>Neural-Decoder Parsing of Minimal Recursions<TITLE>Neural-Decoder Parsing for Minimal Recursion Representation<TITLE>Minimal Recursion Parsing with Neural Networks
45,Bregman Alternating Direction Method of Multipliers,"We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.",<TITLE>Generalization Error Bounds on the Input-Output Mutual Information<TITLE>Lower Bounds on Generalization Error of Learning Algorithms<TITLE>Generalization Error Bounds on Input-Output Mutual Information<TITLE>Generalization Error Bounds for Learning with Mutual Information<TITLE>Lower Bounds on Generalization Error in Learning Problems,<TITLE>Generalization Error Bounds on the Generalization Error of Learning Algorithms via Mutual Information Control and Random Noise Regularization.<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Control and Random Noise Regularization.<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Regularization and Random Noise Control.<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Regularization and Random Noise in Rein<TITLE>Generalization Error Bounds on the Generalization Error of a Learning Algorithm via Mutual Information Regularization and Random Noise in E,<TITLE>Generalization Error Bounds for Learning Algorithms with Relative Entropy and Random Noise<TITLE>On the Generalization Error of Learning Algorithms with Relative Entropy and Random Noise<TITLE>Generalization Error Bounds for Learning Algorithms with Relative Entropy and Random Noise<TITLE>On the Generalization Error of Learning Algorithms<TITLE>Generalization Error Bounds for Learning Algorithms,<TITLE>Generalization of a Learning Algorithm<TITLE>Generalization of Learning Algorithms<TITLE>Generalization in Learning with Mutual Information<TITLE>Generalization in Learning Problems<TITLE>Generalization in Learning,<TITLE> Bounds on Generalization Error of Learning Algorithms<TITLE> Information-Theoretic Bounds on Generalization Error<TITLE> Bounds on Generalization Error in Learning Problems<TITLE> Information-Theoretic Generalization Error Bounds<TITLE> Bounds on Generalization Error in Learning,<TITLE>Upper Bounds on the Generalization Error<TITLE>On the Generalization Error of Learning Algorithms<TITLE>On the Generalization Error of ERM<TITLE>Upper Bounds on the ERM Error<TITLE>Upper bounds on the ERM error
51,Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages,"Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and evaluate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.",<TITLE>Semi-Supervised Learning of Part-of-Speech Tagging Techniques for Low-Resource Languages<TITLE>Semi-Supervised Learning of Part-of-Speech Tagging for Low-Resource Languages<TITLE>Semi-Supervised Learning of Part-of-Speech Tagging for Low-resource Languages<TITLE>Semi-supervised Learning of Part-of-Speech Tagging for Low-Resource Languages<TITLE>Semi-supervised Learning of Part-of-Speech Tagging for Low-resource Languages,<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Words into Corpora<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Extracted Annotation of Word Types in Low-<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Word Types in Low<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Word Types in the<TITLE>Semi-supervised Part-of-Speech Tagging by Timing-Annotated Annotation of Word Type Information.,<TITLE>How Much Data Do You Need to Train a Semi-Supervised Part-of-Speech Tagger?<TITLE>How Much Data Do You Need for Semi-Supervised Part-of-Speech Tagging?<TITLE>How much data is necessary to train a semi-supervised part-of-speech tagger?<TITLE>How Much Data Do You Need to Train a Semi-Supervised POS-Tagger?<TITLE>How much data is necessary to train a semi-supervised POS-tagger?,<TITLE>Part-of-Speech Tagging with Timed Annotations<TITLE>Part-of-Speech Tagging with Timed Annotation<TITLE>Semi-Supervised Learning for Low-Resource Language Processing<TITLE>Semi-Supervised Learning for Low-Resource Languages<TITLE>Part-of-Speech Tagging for Language Processing,<TITLE> Semi-Supervised Learning of Part-of-Speech Tagging<TITLE> Semi-Supervised Learning for Low-Resource Languages<TITLE> Semi-Supervised Learning of Low-Resource Languages<TITLE> Semi-Supervised Learning of Part-of-Speech Tagging Using Data from Low-Resource Languages<TITLE> Semi-Supervised Learning of Part-of-Speech Tagging Using Finite-State Morphology,<TITLE>How Much Data Does a Semi-Supervised Part-of-Speech Tagging System Need?<TITLE>How Much Data Does a Semi-Supervised Part-of-Speech Tagging System Use?<TITLE>How Much Data Do We Need to Train a Part-of-Speech Tagging System?<TITLE>How Much Data Should We Use for Part-of-Speech Tagging?<TITLE>How Much Data Does a Semi-Supervised POS-Tagger Need?
54,An Empirical Study of Automatic Chinese Word Segmentation for Spoken Language Understanding and Named Entity Recognition,"Word segmentation is usually recognized as the first step for many Chinese natural language processing tasks, yet its impact on these subsequent tasks is relatively under-studied. For example, how to solve the mismatch problem when applying an existing word segmenter to new data? Does a better word segmenter yield a better subsequent NLP task performance? In this work, we conduct an initial attempt to answer these questions on two related subsequent tasks: semantic slot filling in spoken language understanding and named entity recognition. We propose three techniques to solve the mismatch problem: using word segmentation outputs as additional features, adaptation with partial-learning and taking advantage of n-best word segmentation list. Experimental results demonstrate the effectiveness of these techniques for both tasks and we achieve an error reduction of about 11% for spoken language understanding and 24% for named entity recognition over the baseline systems.",<TITLE>How to Solve the Mismatch Problem in Chinese Word Segmentation<TITLE>Using Word Segmentation Output to Solve the Mismatch Problem<TITLE>How to Solve the Mismatch Problem in Chinese NLP<TITLE>Using Word Segmentation Output for Chinese NLP Tasks<TITLE>Improving Word Segmentation for Chinese NLP Tasks,<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Spoken Language Understanding and NLP Tasks<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese NLP Tasks: An Initial Investigation<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese NLP Tasks: An Application to<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese Natural Language Processing Tasks: An Initial<TITLE>Adaptive Word Segmentation with Partial-Learning and Named Entity Recognition for Chinese NLP Tasks: An Application of,<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition Tasks<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem of Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem in Word Segmentation Tasks for Chinese NLP,<TITLE>Resolving the Mismatch Problem with Semantic Slot Filling in Word Segmentation<TITLE>Resolving the Mismatch Problem with Neural Word Segmentation Outputs<TITLE>Resolving the Mismatch Problem with Word Segmentation Outputs<TITLE>Resolving the Mismatch Problem with Semantic Slot Filling<TITLE>Resolving the Mismatch Problem with Segmentation Outputs,<TITLE> Semantic Slot Filling in Spoken Language Understanding and Named Entity Recognition<TITLE> Improving Semantic Slot Filling by Adapting Word Segmentation Outputs<TITLE> Semantic Slot Filling in Spoken Language Understanding Using Word Segmentation<TITLE> Semantic Slot Filling in Spoken Language Understanding<TITLE> Semantic Slot Filling in Spoken Language Understanding and NER,<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Solving the Mismatch Problem in Word Segmentation for Spoken Language Understanding and named entity recognition<TITLE>Solving the Mismatch in Word Segmentation for Spoken Language Understanding and Named Entity Recognition<TITLE>Improving Word Segmentation for Spoken Language Understanding and Named Entity Recognition in Chinese<TITLE>Improving Word Segmentation for Spoken Language Understanding and Named Entity Recognition
56,A Framework for Entailed Relation Recognition,"We define the problem of recognizing entailed relations – given an open set of relations, find all occurrences of the relations of interest in a given document set – and pose it as a challenge to scalable information extraction and retrieval. Existing approaches to relation recognition do not address well problems with an open set of relations and a need for high recall: supervised methods are not easily scaled, while unsupervised and semi-supervised methods address a limited aspect of the problem, as they are restricted to frequent, explicit, highly localized patterns. We argue that textual entailment (TE) is necessary to solve such problems, propose a scalable TE architecture, and provide preliminary results on an Entailed Relation Recognition task.",<TITLE>Relation Recognition with Textual Entailment: A Scalable Approach<TITLE>Recognizing Entailed Relations with Textual Entailment<TITLE>Recognizing Entailed Relations using Textual Entailment<TITLE>Textual Entailment for Relation Recognition<TITLE>Relation Recognition with Textual Entailment,<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Methods. A Scalable Architecture<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Learning. A Scalable Architecture<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Methods. A Scalable Information<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Methods. A Scalable Approach<TITLE>Recognizing Entailed Relation Recognition with Textual Entailment and Semi-Supervised Learning. A Scalable Approach,<TITLE>Textual Entailment for Relation Recognition: A Scalable Approach<TITLE>Textual Entailment for Relation Recognition<TITLE>Textual Entailment for Relation Recognition<TITLE>Textual Entailment for Relation Recognition<TITLE>Textual Entailment for Relation Recognition,<TITLE>Adaptive Textual Entailment for Entailed Relation Recognition<TITLE>Resolving Entailed Relation Recognition with Textual Entailment<TITLE>Adapting Textual Entailment for Entailed Relation Recognition<TITLE>Resolving Entailed Relation Recognition with Textual Entailments<TITLE>Adaptive Textual Entailment for Entailed Relations,<TITLE> Towards Scalable Information Extraction and Retrieval<TITLE> An Entailed Relation Recognition Architecture for Textual Entailment<TITLE> Towards Scalable Information Extraction and Retrieval from Texts<TITLE> An Entailed Relation Recognition Architecture<TITLE> Towards Scalable Information Extraction and Retrieval from Textual Relations,<TITLE>Textual Entailment for Open-Set Relation Recognition<TITLE>Entailed Relation Recognition with Textual Entailment<TITLE>Entailed Relation Recognition via Textual Entailment<TITLE>Textual Entailment for Open Relation Recognition<TITLE>Textual Entailment for Relation Recognition
57,Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation,"Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. One of the big challenges with this task is the lack of sufficient annotated data. While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction. Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences. Experimental results confirm that our method alleviates the data scarcity problem significantly. It also effectively boosts the performances of several current models for aspect term extraction.",<TITLE>Conditional Augmentation of Aspect Term Extraction as a Conditional Generation Task<TITLE>Conditional Augmentation of Aspect Term Extraction via Sequence toSequence Method<TITLE>Conditional Augmentation of Aspect Term Extraction as Conditional Generation.<TITLE>Conditional Augmentation of Aspect Term Extraction as Conditional Generation<TITLE>Conditional Augmentation of Aspect Term Extraction from Texts,<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentiment Analysis<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentence Generation<TITLE>Conditional Augmentation of Aspect Term Extraction with Masked Sequence-to-Sequence Method for Sentiment Analysis and<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentiment Generation<TITLE>Conditional Augmentation of Aspect Term Extraction: A Masked Sequence-to-Sequence Method for Sentiment Target,<TITLE>Conditional Augmentation of Aspect Term Extraction via Conditional Generation<TITLE>Conditional Augmentation of Aspect Term Extraction with Conditional Generation<TITLE>Conditional Augmentation for Aspect Term Extraction with Conditional Generation<TITLE>Conditional Augmentation of Aspect Term Extraction via Conditional Generation<TITLE>Conditional Augmentation for Aspect Term Extraction,<TITLE>Aspect term Extraction with Conditional Error<TITLE>Aspect term Extraction with Conditional Boosting<TITLE>Conditional Extraction of Aspect term Extraction<TITLE>Aspect term Extraction with Conditional augmentation<TITLE>Conditional Extraction of Aspect Terminology,<TITLE> Conditional Augmentation for Aspect Term Extraction<TITLE> Conditional Augmentation of Aspect Term Extraction<TITLE> Data Augmentation for Aspect Term Extraction<TITLE> Augmenting Aspect Term Extraction with Conditional Generation<TITLE> Modeling Aspect Term Extraction as Conditional Generation,<TITLE>Conditional Aspect Term Extraction via Masked Sequence-to-Sequence Generation<TITLE>Masked Sequence-to-Sequence Aspect Term Extraction<TITLE>Conditional Data Augmentation for Aspect Term Extraction<TITLE>Conditional Augmentation of Aspect Term Extraction<TITLE>Aspect Term Extraction as Conditional Generation
58,Dependency Parsing of Japanese Spoken Monologue Based on Clause Boundaries,"Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences.",<TITLE>Dependency Parsing of Japanese Monologues Based on Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologue Sentences using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues Using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues with Sentence Segmentation,<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependency Parser<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependencies over Language<TITLE>Dependency Parsing of Japanese Monologues Using Sentence Segmentation and Stochastic Dependency Parser for<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependency Parsings<TITLE>Dependency Parsing of Japanese Monologue Sentences Using Sentence Segmentation and Stochastic Dependency Parsers,<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologues<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologue Sentences<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologues<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologue Sentences<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Monologues,<TITLE>Dependency Parsing of Japanese Monologues with Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues using Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues via Sentence Segmentation<TITLE>Dependency Parsing of Japanese Monologues<TITLE>Dependency Parsing for Japanese Monologues,<TITLE> Dependency Parsing of Japanese Monologues Using Sentence Segmentation<TITLE> Dependency Parsing of Japanese Monologues with Sentence Segmentation<TITLE> Japanese Dependency Parsing with Sentence Segmentation<TITLE> Dependency Parsing of Japanese Monologues<TITLE> Dependency Parsing of Japanese Monologues Using Parse Segmentation,<TITLE>Dependency Parsing of Japanese Spoken Monologues Based on Sentence Segmentation<TITLE>Dependency Parsing of Japanese Spoken Monologues based on Sentence Segmentation<TITLE>Dependency Parsing of Japanese Spoken Monologues Using Sentence Segmentation<TITLE>Sentence Segmentation for Dependency Parsing of Japanese Spoken Monologues<TITLE>Dependency Parsing of Spoken Monologues Based on Sentence Segmentation
59,Named Entity Disambiguation for Noisy Text,"We address the task of Named Entity Disambiguation (NED) for noisy text. We present WikilinksNED, a large-scale NED dataset of text fragments from the web, which is significantly noisier and more challenging than existing news-based datasets. To capture the limited and noisy local context surrounding each mention, we design a neural model and train it with a novel method for sampling informative negative examples. We also describe a new way of initializing word and entity embeddings that significantly improves performance. Our model significantly outperforms existing state-of-the-art methods on WikilinksNED while achieving comparable performance on a smaller newswire dataset.",<TITLE>A Large-Scale Named Entity Disambiguation Dataset for Noisy Text<TITLE>A Large-Scale Named Entity Disambiguation Dataset of Noisy Text<TITLE>A Large-Scale Named Entity Disambiguation Dataset from the Web<TITLE>Unsupervised Named Entity Disambiguation for Noisy Text<TITLE>Neural Named Entity Disambiguation for Noisy Text,<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-Based Neural Model for Extracting Informative Examples<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-based Neural Model for Extracting Informative Examples<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-based Approach to Extracting Information from the Web<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-Based Approach to Extracting Informative Examples from<TITLE>Named Entity Disambiguation for Noisy Text: A Novelty-based Approach to Extracting Informative Examples from,<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Noisy Text<TITLE>WikilinksNED: Noisy Named Entity Disambiguation with Local Context<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Local Context<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Local Context<TITLE>WikilinksNED: Noisy Named Entity Disambiguation for Local Context,<TITLE>Named Entity Disambiguation for Noise-Free Text<TITLE>Named Entity Disambiguation for Noise-based Text<TITLE>Named Entity Disambiguation for Neural Texts<TITLE>Named Entity Disambiguation for Neural Text<TITLE>Named Entity Disambiguation for Noiseful Text,<TITLE> Unsupervised Named Entity Disambiguation for Noisy Text<TITLE> Unsupervised Named Entity Disambiguation Using Word and Entity Embeddings<TITLE> Unsupervised Named Entity Disambiguation for Unseen Text<TITLE> Unsupervised Named Entity Disambiguation Using Word Embeddings<TITLE> Unsupervised Named Entity Disambiguation for Text,<TITLE>WikilinksNED: A Large-Scale Named Entity Disambiguation Dataset<TITLE>Named Entity Disambiguation for Noisy Text: WikilinksNED<TITLE>WikilinksNED: Named Entity Disambiguation for Noisy Text<TITLE>Named Entity Disambiguation for Noisy Text: WikiLinksNED<TITLE>Named Entity Disambiguation for Noisy Text
63,Sentence Compression with Joint Structural Inference,"Sentence compression techniques often assemble output sentences using fragments of lexical sequences such as ngrams or units of syntactic structure such as edges from a dependency tree representation. We present a novel approach for discriminative sentence compression that unifies these notions and jointly produces sequential and syntactic representations for output text, leveraging a compact integer linear programming formulation to maintain structural integrity. Our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-theart approaches. Experiments on corpora featuring human-generated compressions demonstrate a 13-15% relative gain in 4gram accuracy over a well-studied language model-based compression system.",<TITLE>Discriminative Sentence Compression with Compact Integer Programming Formulations<TITLE>Discriminative Sentence Compression with Dependency Tree Representations<TITLE>Discriminative Sentence Compression with Compact Integer Programming Formulas<TITLE>Discriminative Sentence Compression with Compact Integer Programming Formulation<TITLE>Discriminative Sentence Compression with Compact Integer Linear Programming,<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression with Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression using Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression Using Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression via Heterogeneous<TITLE>Jointly Generating Sequential and Syntactic Representations for Discriminative Sentence Compression with a Compact Integer,<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representations<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Structures<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representation<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representations<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representation,<TITLE>Discriminative Sentence Compression with Sequential and Syntactic Representations<TITLE>Discriminative Sentence Compression with Integer Linear Programming for Text<TITLE>Discriminative Sentence Compression with a Compact Integer Linear Program<TITLE>Discriminative Sentence Compression with Integer Linear Programming<TITLE>Discriminative Sentence Compression with Integer Linear programming,<TITLE> Discriminative Sentence Compression with Compact Integer Linear Programming<TITLE> Discriminative Sentence Compression<TITLE> Discriminative Sentence Compression with Structural Integrity<TITLE> Discriminative Sentence Compression with Heterogeneous Structures<TITLE> Discriminative Sentence Compression with Structured Input,<TITLE>Joint Sequential and Syntactic Representations for Discriminative Sentence Compression<TITLE>Jointly Assembling Lexical and Syntactic Structures for Sentence Compression<TITLE>Discriminative Sentence Compression with Joint Syntactic and Sequential Representations<TITLE>Joint Sequential and Syntactic Structures for Discriminative Sentence Compression<TITLE>Joint Sequential and Syntactic Representations for Sentence Compression
67,Sequence Generation with Optimal-Transport-Enhanced Reinforcement Learning,"Reinforcement learning (RL) has been widely used to aid training in language generation. This is achieved by enhancing standard maximum likelihood objectives with userspecified reward functions that encourage global semantic consistency. We propose a principled approach to address the difficulties associated with RL-based solutions, namely, highvariance gradients, uninformative rewards and brittle training. By leveraging the optimal transport distance, we introduce a regularizer that significantly alleviates the above issues. Our formulation emphasizes the preservation of semantic features, enabling end-to-end training instead of ad-hoc fine-tuning, and when combined with RL, it controls the exploration space for more efficient model updates. To validate the effectiveness of the proposed solution, we perform a comprehensive evaluation covering a wide variety of NLP tasks: machine translation, abstractive text summarization and image caption, with consistent improvements over competing",<TITLE>End-to-End Reinforcement Learning with Global Semantic Consistency<TITLE>Continuous Reinforcement Learning with Global Semantic Consistency<TITLE>End-to-End Reinforcement Learning for Neural Language Generation<TITLE>End-to-End Reinforcement Learning for Language Generation<TITLE>End-to-End Reinforcement Learning for NLP,<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Tasks: A Principled Approach<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Models: An Application to Textual<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Models: A Principled Approach.<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Models: A Principled Approach to<TITLE>Optimal Transport-based Reinforcement Learning for Global Semantic Consistency in NLP Tasks: A Principled Perspective,<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Consistency<TITLE>End-to-End Reinforcement Learning for Language Generation with Userspecified Reward Functions<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Regularization<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Consistency<TITLE>End-to-End Reinforcement Learning for Language Generation with Global Semantic Regularization,<TITLE>A Regularizer for Reinforcement Learning in Language Generation<TITLE>Improved Reinforcement Learning for Language Generation<TITLE>Improved Reinforcement Learning in Language Generation<TITLE>A Regularizer for Reinforcement Learning<TITLE>Reinforcement Learning for Language Generation,<TITLE> Efficient Reinforcement Learning for Language Generation<TITLE> Highvariance Gradients for Reinforcement Learning<TITLE> Efficient Reinforcement Learning for Natural Language Generation<TITLE> Learning to Reinforce Language Generation<TITLE> Efficient Reinforcement Learning for Neural Language Generation,<TITLE>Reinforcement Learning for Natural Language Processing with Optimal Transport<TITLE>Reinforcement Learning with Optimal Transport for Natural Language Processing<TITLE>Reinforcement Learning with Optimal Transport for Neural Language Generation<TITLE>Reinforcement Learning with Optimal Transport for Natural Language Generation<TITLE>Reinforcement Learning for Language Generation with Optimal Transport
73,Generating Natural Language Summaries from Multiple On-Line Sources,"We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.",<TITLE>Summarization of Current Events in the Form of Briefings<TITLE>SummONS: Summarizing Current Events with Background Information<TITLE>SummONS: Summarizing Current Events with Historical Information<TITLE>Summarization of Current Events in the Form of Briefing<TITLE>Summarization of Current Events with Background Information,<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background and Multi-Document Summaries<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background and Multi-Document Summarization<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background Descriptions of Entities and Cont<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background Descriptions of Entities and Conf<TITLE>Summarization of Current Events: Summarizing News About Current Events by Combining Background Descriptions of Entities and General,<TITLE>Summons: A System for Historical Summarization of News Briefings<TITLE>Summons: A System for Historical Summarization of News and Current Events<TITLE>Summons: A System for Historical Summarization of News Briefing<TITLE>Summons: A System for Historical Summarization of News Briefings<TITLE>Summons: A System for Historical Summarization of News and Events,<TITLE>SUMMONS: A System for Summarization of News about Current Events<TITLE>SUMMONS: Synchronous Summarization of News about Current Events<TITLE>SUMMONS: A System for Summarization of Information about Current Events<TITLE>SUMMONS: Synchronous Summarization of Information about Events<TITLE>SUMMONS: A System for Summarization of Information about Events,<TITLE> Summarization of News About Current Events<TITLE> Summarization of News About Current Events with Background Information<TITLE> Summarization of News with Background Information<TITLE> Summarization of News about Current Events with Background Information<TITLE> Summarization of News About Current Events Using Briefing,<TITLE>SUMMONS: A Methodology for Semantic Summarization of News about Events<TITLE>SUMMONS: A Methodology for Summarizing News about Events in History<TITLE>SUMMONS: A Methodology for Summarization of News about Events<TITLE>SUMMONS: A Methodology for News Summarization of Historical Documents<TITLE>SUMMONS: A Methodology for Semantic Summarization of Historical Documents
74,Positional Encoding to Control Output Sequence Length,"Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider an additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) so that a neural encoder-decoder model preserves the length constraint. Unlike previous studies that learn length embeddings, the proposed method can generate a text of any length even if the target length is unseen in training data. The experimental results show that the proposed method is able not only to control generation length but also improve ROUGE scores.",<TITLE>A Simple but Effective Extension of a sinusoidal positional encoding for Abstractive Summarization<TITLE>A Simple but Effective Extension of a Sinusoidal positional encoding for Abstractive Summarization<TITLE>A Simple but Effective Extension of a sinusoidal positional encoding model for abstractive summarization<TITLE>A Simple but Effective Extension of a sinusoidal positional encoding for abstractive summarization<TITLE>Learning length embeddings for abstractive summarization with sinusoidal positional encoding.,<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal Position<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal Pos<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization with Length Constraints.<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal POS<TITLE>A Simple and Effective Extension of Neural Encoder-Decoder Models for Abstractive Summarization Using a Sinusoidal positional,<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Encoder-Decoder Models<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Encoder-Decoder Model<TITLE>Learning to Generate Text of Any Length with a Sinusoidal Positioning Encoder-Decoder<TITLE>Learning to Generate Text of Any Length with a Sinusoidal Positioning Encoding<TITLE>Learning to Generate Text of Any Length with a Sinusoidal Positioning Encoding,<TITLE>Sinusoidal Positional Encoding for Neural Language Generation<TITLE>Sinusoidal Positional Encoding for Neural Language Generating<TITLE>A Sinusoidal Positional Encoding for Neural Language Generation<TITLE>Sinusoidal Positional Encoding for Neural Language Generation<TITLE>A Sinusoidal Positional Encoding for Natural Language Generation,<TITLE> A Sinusoidal Neural Encoder-Decoder Model for Abstractive Summarization<TITLE> Learning Length Constraints for Abstractive Summarization<TITLE> A Sinusoidal Perceptron for Abstractive Summarization<TITLE> Simplifying Abstractive Summarization with Sinusoidal Parameter Encoders<TITLE> Simplifying Abstractive Summarization with Sinusoidal Position Representations,<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Decoding<TITLE>Learning Length Embeddings for Abstractive Summarization with Neural Networks<TITLE>Learning Length Embeddings for Abstractive Summarization<TITLE>Learning Length Embeddings for Neural Stream Summarization<TITLE>Learning Length Embeddings for Neural Summarization
75,Pre-training of Recurrent Neural Networks via Linear Autoencoders,"We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the inputto-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.",<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-Training for Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoders<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoders.<TITLE>Pre-Training for Recurrent Neural Networks with Linear Autoencoders,<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and a<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and an<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and Applications<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and Application<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks: A Polyphonic Music Algorithm and Hidden,<TITLE>Pre-training of Linear Autoencoder Networks for Recurrent Neural Networks<TITLE>Pre-training of Linear Autoencoders for Recurrent Neural Networks<TITLE>Pre-training of Linear Autoencoders for Recurrent Neural Networks<TITLE>Pre-training of Linear Autoencoder Networks for Sequences<TITLE>Pre-training of Linear Autoencoders for Recurrent Neural Networks,<TITLE>Pre-training for Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-training for Recurrent Neural Networks using Linear Autoencoder Networks<TITLE>Pre-training Recurrent Neural Networks with Linear Autoencoder Networks<TITLE>Pre-Training for Recurrent Neural Networks<TITLE>Pre-training for Recurrent Neural Networks,<TITLE> Training Recurrent Neural Networks with Linear Autoencoders<TITLE> Recurrent Neural Networks with Linear Autoencoders<TITLE> Recurrent Neural Networks for Polyphonic Music<TITLE> Training Recurrent Neural Networks with Linear Autoencoder Networks<TITLE> Pre-Training Recurrent Neural Networks for Polyphonic Music,<TITLE>Pre-training of Linear Autoencoder Networks for Sequences<TITLE>Pre-training of Linear Autoencoder Networks for Sequence Learning<TITLE>Pre-training Linear Autoencoders for Recurrent Neural Networks<TITLE>Pre-training Linear Autoencoder Networks for Sequences<TITLE>Pre-training Linear Autoencoder Networks for Sequence Learning
76,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the ""language agnostic"" status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",<TITLE>Language Agnostic: A quantitative study of the impact of language technologies on NLP<TITLE>Language Agnostic: A quantitative study of the impact of linguistic technologies on NLP<TITLE>Language Agnostic: A quantitative study of the impact of languages on NLP conferences<TITLE>Language Agnostic: A quantitative study of the impact of languages on NLP<TITLE>Language Agnostic: A Case Study in the ACL,"<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts.<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts (<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts over<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts with<TITLE>The Language Agnostic Community: The Case of Multilingualism, Diversity, and Representation in NLP Conference Transcripts -",<TITLE>The State of the Art of Multilingualism in NLP: A Quantitative Analysis<TITLE>The State of the Art of Multilingualism in NLP: A Quantitative Investigation<TITLE>The State of the Art of Multilingualism in NLP: A quantitative analysis<TITLE>The State of the Art of Multilingualism in the World of NLP<TITLE>The State of the Art of Multilingualism in NLP,"<TITLE>Understanding the Relation between Languages, Resources, and Their Representation in NLP Conférences<TITLE>Understanding the Relation between Languages, Resources, and Their Representation in NLP Conferences<TITLE>Understanding the Relation between Languages, Resources, and Their Representations in NLP Conferences<TITLE>The Relation between Languages, Resources, and Their Representation in NLP Conferences<TITLE>Understanding the Relation between Languages and Resource Representations in NLP Conferences",<TITLE> The Impact of Language Technology on Multilingualism and Linguistic Diversity<TITLE> The Impact of Language Technology on Multilingualism and Diversity<TITLE> The Role of Language Technology in Multilingualism and Diversity<TITLE> The Impact of Language Technology on Multilingualism and Linguistics<TITLE> The Impact of Language Technology on Multilingualism and Diversity in Conference,"<TITLE>""No Language is Left Behind"": A Case Study on the Representation of Languages in NLP Conferences<TITLE>""No Language is Left Behind"": An ACL study of the status of languages in NLP conferences<TITLE>""No Language is Left Behind"": A Case Study on the Status of Languages in NLP Conferences<TITLE>""No Language is Left Behind"": A Case Study on the Representation of Languages in NLP conferences<TITLE>""No Language is Left Behind"": A Case Study on the Status of Languages in NLP Conference"
80,Multilingual Document Clustering: An Heuristic Approach Based on Cognate Named Entities,"This paper presents an approach for Multilingual Document Clustering in comparable corpora. The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora. One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources. However, it depends on the possibility of identifying cognate named entities between the languages used in the corpus. An additional advantage of the approach is that it does not need any information about the right number of clusters; the algorithm calculates it. We have tested this approach with a comparable corpus of news written in English and Spanish. In addition, we have compared the results with a system which translates selected document features. The obtained results are encouraging.",<TITLE>Multilingual Document Clustering in Comparable Corpora Using Heuristic Data<TITLE>Multilingual Document Clustering in Comparable Corpora Using Heuristic Information<TITLE>Multilingual Document Clustering in Comparable Corpora using Heuristic Data<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora.,<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Information<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristics<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Networks<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Dynamics<TITLE>Multilingual Document Clustering in Comparable Corpora: Identifying Cognate Named Entities by Means of Heuristic Complex,<TITLE>Multilingual Document Clustering in Comparable Corpora using Cognate Named Entities<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparable Corpora,<TITLE>Multilingual Document Clustering in Comparable Corpora<TITLE>Multilingual Document Clustering in Comparative Corpora<TITLE>Multilingual Document Clustering for Comparable Corpora<TITLE>Multilingual Document Clustering in comparable corpora<TITLE>Multilingual Document Clustering,<TITLE> Multilingual Document Clustering Using Comparable Corpora<TITLE> Multilingual Document Clustering Based on Cognate Name Identification<TITLE> Multilingual Document Clustering Based on Cognate Identification<TITLE> Multilingual Document Clustering Using Bilingual Information<TITLE> Multilingual Document Clustering Based on Cognate Identification,<TITLE>Comparing Document Clustering in Comparable Corpora Using Heuristics<TITLE>An Algorithm for Document Clustering in Comparable Corpora<TITLE>Comparing Document Clustering in Comparable Corpora<TITLE>Comparative Document Clustering in Comparable Corpora<TITLE>Document Clustering in Comparable Corpora
87,Joint Emotion Analysis via Multi-task Gaussian Processes,"We propose a model for jointly predicting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches.",<TITLE>Joint Predicting of Multiple Emotions in Natural Language Sentences<TITLE>Joint Predicting Multiple Emotions in Natural Language Sentences<TITLE>Joint Predicting of Emotions in Natural Language Sentences<TITLE>Joint Predicting of Multiple Emotions in News Headlines<TITLE>Joint Predicting Multiple Emotions in News Headlines,<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Gaussian Processes and Low-Rank Coregional<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Low-Rank Coregionalisation and Gaussian Process<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Low-Rank Coregionalisation and Vector-valued<TITLE>Jointly Predicting Multiple Emotions in Sentences using Gaussian Processes and Low-Rank Coregionalisation.<TITLE>Jointly Predicting Multiple Emotions in Natural Language Sentences using Low-Rank Coregionalisation and Vector-Val,<TITLE>A Low-Rank Coregionalisation Model for Joint Prediction of Multiple Emotions in Natural Language<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences with Coregionalisation<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences<TITLE>Joint Prediction of Multiple Emotions in Natural Language Sentences,<TITLE>A Low-rank Coregionalisation Approach to Predicting Multiple Emotions in Natural Language Sentences<TITLE>A Low-rank Coregionalisation Approach for Predicting Multiple Emotions in Natural Language Sentences<TITLE>A Low-Rank Coregionalisation Approach for Predicting Multiple Emotions in Natural Language Sentences<TITLE>A Low-rank Coregionalisation Approach to Predicting Multiple Emotions<TITLE>A Low-rank Coregionalisation Approach for Predicting Multiple Emotions,<TITLE> Joint Predicting Multiple Emotions in Natural Language Sentences<TITLE> Joint Predicting Multiple Emotions in Natural Language<TITLE> Jointly Predicting Multiple Emotions in Natural Language<TITLE> Joint Predicting Multiple Emotions in Natural Language Text<TITLE> Jointly Predicting Multiple Emotions in Natural Language Text,<TITLE>A Low-Rank Co-Regionisation Approach for Joint Emotion Prediction<TITLE>A Low-Rank Co-Regionisation Approach to Joint Emotion Prediction<TITLE>A Low-Rank Co-regionalisation Approach to Joint Emotion Prediction<TITLE>A Low-Rank Co-Region Model for Joint Emotion Prediction<TITLE>Learning to Predict Multiple Emotions in Natural Language Sentences
89,Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent,"Contrary to most natural language processing research, which makes use of static datasets, humans learn language interactively, grounded in an environment. In this work we propose an interactive learning procedure called Mechanical Turker Descent (MTD) and use it to train agents to execute natural language commands grounded in a fantasy text adventure game. In MTD, Turkers compete to train better agents in the short term, and collaborate by sharing their agents’ skills in the long term. This results in a gamified, engaging experience for the Turkers and a better quality teaching signal for the agents compared to static datasets, as the Turkers naturally adapt the training data to the agent’s abilities.",<TITLE>Turker Descent: Interactively Learning to execute Natural Language commands in a Fantasy Game<TITLE>Turker Descent: Interactive Learning of Natural Language commands grounded in a Fantasy Game<TITLE>Turker Descent: Interactive Learning of Natural Language commands grounded in an Environment<TITLE>Turker Descent: An Interactive Learning Procedure for Natural Language Processing<TITLE>Turker Descent: Interactive Training of Natural Language Agents,<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Dataset and Skill Sharing<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Datasets and Skill Sharing<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game Using Static Datasets and Util<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Dataset and Util<TITLE>Turker Descent: Interactive Learning of Natural Language Agents in a Fantasy Text Adventure Game using Static Dataset and Teaching Skills,<TITLE>Turker Descent: Interactive Learning for Natural Language Processing with Mechanical Turkers<TITLE>Interactive Learning for Natural Language Processing with Mechanical Turker Descent<TITLE>Interactive Learning of Natural Language Agents with Mechanical Turker Descent<TITLE>Turker Descent: Interactive Learning for Natural Language Processing<TITLE>Turker Descent: Interactive Learning with Mechanical Turkers,<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Language Commands<TITLE>Mechanical Turker Descent: An Interactive Learning Approach to Natural Language<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Languages<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Language<TITLE>Mechanical Turker Descent,<TITLE> Turker Descent: Interactive Learning for Natural Language Utterance Games<TITLE> Turker Descent: Interactive Learning of Natural Language Utterance<TITLE> Learning to execute natural language commands grounded in a fantasy text game<TITLE> Turker Descent: Interactive Learning for Natural Language Utterance<TITLE> Turker Descent: Interactive Learning of Natural Language Utterance Games,<TITLE>Mechanical Turker Descent: Learning Language Interactively from Text Adventure Games<TITLE>Mechanical Turker Descent: Interactive Learning of Natural Language from Text Games<TITLE>Mechanical Turker Descent: Learning Language Interactively from Text Games<TITLE>Mechanical Turker Descent: Learning Language Interactively from Text<TITLE>Interactive Learning of Natural Language with Mechanical Turker Descent
93,Improved Estimation of Entropy for Evaluation of Word Sense Induction,"Information-theoretic measures are among the most standard techniques for evaluation of clustering methods including word sense induction (WSI) systems. Such measures rely on sample-based estimates of the entropy. However, the standard maximum likelihood estimates of the entropy are heavily biased with the bias dependent on, among other things, the number of clusters and the sample size. This makes the measures unreliable and unfair when the number of clusters produced by different systems vary and the sample size is not exceedingly large. This corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems.",<TITLE>Evaluating Word Sense Induction Systems Using Entropy Estimation<TITLE>Information-Theoretic Evaluation of Word Sense Induction Systems<TITLE>Information-Theoretic Measures of Word Sense Induction Systems<TITLE>Information-theoretic Measures of Word Sense Induction Systems<TITLE>Information-Theoretic Measures of Word Sense Induction,<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Clustering Schemes<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Schem<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Systems.<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Systems with<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation Measures and the Sample Size of Multiple Clustering Scores,<TITLE>Evaluation of Word Sense Induction Systems Based on Sample-Based Entropy Estimation<TITLE>Evaluation of Word Sense Induction Systems: More Accurate Estimation of Entropy<TITLE>Evaluation of Word Sense Induction Systems: More Accurate Entropy Estimators<TITLE>Evaluation of Word Sense Induction Systems via Entropy Estimation<TITLE>Evaluation of Word Sense Induction Systems via Entropy Estimates,<TITLE>Information-Theoretic Measures for Word Sense Induction Evaluation<TITLE>Information-Theoretic Measures for Word Sentence Induction<TITLE>Information-Theoretic Measures for Word Sense Induction<TITLE>Evaluating Word Sentence Induction Systems<TITLE>Evaluating Word Sense Induction Systems,<TITLE> Evaluating Clustering Methods Using Word Sense Induction<TITLE> Evaluating Clustering Methods using Word Sense Induction<TITLE> Efficient Evaluation of Clustering Methods<TITLE> Evaluating Clustering Methods<TITLE> Estimating the Entropy of Clustering Methods,<TITLE>On the Accuracy of Entropy Estimation for Word Sense Induction<TITLE>On the Accuracy of Entropy Measures for Word Sense Induction<TITLE>Information-theoretic measures of entropy for word sense induction<TITLE>Evaluation of Word Sense Induction Using Entropy Estimation<TITLE>Information-theoretic measures for word sense induction
94,Learning with Annotation Noise,"It is usually assumed that the kind of noise existing in annotated data is random classification noise. Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases. Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time.",<TITLE>Hard Training Cases in Random Classification Noise Generation<TITLE>Hard Training Cases for Random Classification Noise Detection<TITLE>Hard Training Cases for Random Classification Noise<TITLE>Hard Training Cases of Random Classification Noise<TITLE>Hard Training Cases in Random Classification Noise,<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Hardness<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Error Classification<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Hard Cases<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Hard Examples<TITLE>Annotating Hard Cases with Noisy Annotations: Unsupervised Learning of Annotation Models under Uncertainty and Error Detection,<TITLE>Automatic Generation of Hard Cases with Noisy Annotated Data<TITLE>Automatic Generation of Hard Cases with Unwanted Attention Slips<TITLE>Automatic Generation of Hard Cases with Bias-Based Annotation<TITLE>Automatic Generation of Hard Cases with Unwanted Annotation Errors<TITLE>Automatic Generation of Hard Cases with Noisy Annotations,<TITLE>Hard Training Cases with Unreliable Annotation Generation<TITLE>Unreliable Annotation Generation for Random Classification Noise<TITLE>Hard Training Cases with Unreliable Annotations<TITLE>Hard Training Cases with Unreliable Annotation<TITLE>A Random Classification Noise Model for Hard Training,<TITLE> Random Attention Slippage in Annotation<TITLE> Random Attention Slips in Annotation<TITLE> Random Attention Slippage and Hard Cases<TITLE> Random Attention Slips in Annotation Models<TITLE> Random Attention Slippage in Annotations,<TITLE>Training hard cases with unreliable annotations could result in incorrect prediction on uncontroversial cases<TITLE>Training hard-to-describe cases with unreliable annotations is not always random<TITLE>Training hard-to-decide cases with unreliable annotations is not always random<TITLE>Training hard-to-decidency cases with unreliable annotations<TITLE>Training hard cases with unreliable annotations could result in incorrect prediction
95,Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network,"This paper proposes a method for intrasentential subject zero anaphora resolution in Japanese. Our proposed method utilizes a Multi-column Convolutional Neural Network (MCNN) for predicting zero anaphoric relations. Motivated by Centering Theory and other previous works, we exploit as clues both the surface word sequence and the dependency tree of a target sentence in our MCNN. Even though the F-score of our method was lower than that of the state-of-the-art method, which achieved relatively high recall and low precision, our method achieved much higher precision (>0.8) in a wide range of recall levels. We believe such high precision is crucial for real-world NLP applications and thus our method is preferable to the state-of-the-art method.",<TITLE>A Multi-column Convolutional Neural Network for Zero Anaphora Resolution in Japanese<TITLE>A Multi-column Convolutional Neural Network for Zero Anaphora Resolution in Japanese.<TITLE>Japanese Zero Anaphora Resolution Using Multi-Column Convolutional Neural Networks<TITLE>Japanese Zero Anaphora Resolution Using Multi-column Convolutional Neural Networks<TITLE>A Multi-column Convolutional Neural Network for Zero Anaphora Resolution,<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using a Multi-Matrix<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using Clues from Cent<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using MCNNs and<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using Clues from Sent<TITLE>A Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution using Clues both the,<TITLE>Japanese Zero Anaphora Resolution with Multi-Column Convolutional Neural Networks<TITLE>Japanese Zero Anaphora Resolution Using Multi-Column Convolutional Neural Networks<TITLE>Japanese Zero Anaphora Resolution Using Multi-Column Convolutional Neural Network<TITLE>Japanese Zero Anaphora Resolution with Multi-Column Convolutional Neural Network<TITLE>Japanese Zero Anaphora Resolution with Multi-Column Convolutional Neural Network,<TITLE>Multi-Column Convolutional Neural Networks for Intrentential Subject Zero Anaphora Resolution<TITLE>A Multi-Column Convolutional Neural Network for Intrentential Subject Zero Anaphora Resolution<TITLE>Multi-Column Convolutional Neural Network for Intrasentential Subject Zero Anaphora Resolution<TITLE>Multi-Column Convolutional Neural Network for Intrentential Subject Zero Anaphora Resolution<TITLE>Intrasentential Subject Zero Anaphora Resolution in Japanese,<TITLE> Intrasentential Subject Zero Anaphora Resolution in Japanese<TITLE> Intrasentential Subject Zero Anaphora Resolution with Multi-Column Convolutional Neural Networks<TITLE> Intrasentential Subject Zero Anaphora Resolution Using Multi-Column Convolutional Neural Networks<TITLE> Multi-column Convolutional Neural Network for Japanese Subject Zero Anaphora Resolution<TITLE> Intrasentential Subject Zero Anaphora Resolution Using Multi-column Convolutional Neural Networks,<TITLE>Intra-sentential Subject Zero Anaphora Resolution with Multi- Convolutional Neural Network<TITLE>Intra-sentential Subject Zero Anaphora Resolution with Multi- Convolutional Neural Networks<TITLE>Intra-Sentential Zero Anaphora Resolution with Multi- Convolutional Neural Network.<TITLE>Intra-sentential Zero Anaphora Resolution with Multi- Convolutional Neural Network.<TITLE>Intra-Sentential Zero Anaphora Resolution with Multi-Convolutional Neural Network.
96,Constructing Semantic Space Models from Parsed Corpora,Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations.,<TITLE>Semantic Spaces for Lexical Semantic Parsing<TITLE>Constructing Semantic Spaces for Lexical Semantics<TITLE>Constructing Semantic Spaces for Lexical Meaning<TITLE>Semantic Spaces for Lexical Semantics<TITLE>Semantic Spaces with Syntactic Relations,<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: A Case Study in Vector-Based Learning of Lexical Meaning<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: A Case Study in Vector-based Learning of Lexical Meaning<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: An Application to Modelling Lexical Meaning in Vector-based<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: A Case Study in Vector-based Models for Lexical Meaning<TITLE>Building Semantic Spaces with Syntactic and Semantic Priming: An Application to Modelling Lexical Meaning in Vector-Based,<TITLE>Semi-supervised Learning of Semantic Spaces with Syntactic Relations<TITLE>Semi-supervised Learning of Lexical Spaces with Syntactic Relations<TITLE>Semi-supervised Learning of Semantic Spaces using Lexical Relations<TITLE>Semi-supervised Learning of Semantic Spaces from Lexical Data<TITLE>Semi-supervised Learning of Lexical Space Models,<TITLE>Semantic Spaces for Syntactic Relation Models<TITLE>Semantic Spaces for Syntactic Relation Modeling<TITLE>Building Semantic Spaces with Syntactic Relations<TITLE>Constructing Semantic Spaces for Lexical Meaning<TITLE>Semantic Spaces for Syntactic Relations,<TITLE> Semantic Spaces for Lexical Relations<TITLE> Semantic Spaces for Word Co-occurrence Models<TITLE> Semantic Spaces for Lexical Roles<TITLE> Semantic Spaces for Lexical Semantic Representation<TITLE> Semantic Spaces for Lexical Semantics,<TITLE>A Formalisation of Vector-based Models of Semantic Spaces<TITLE>Constructing Semantic Spaces for Lexical Relations Using Syntactic Information<TITLE>Constructing Semantic Spaces for Lexical Relations with Syntactic Information<TITLE>A Formalisation of Vector-based Models of Semantic Properties<TITLE>Constructing Semantic Spaces for Lexical Relations
97,Experimental Evaluation of LTAG-Based Features for Semantic Role Labeling,"This paper proposes the use of Lexicalized Tree-Adjoining Grammar (LTAG) formalism as an important additional source of features for the Semantic Role Labeling (SRL) task. Using a set of one-vs-all Support Vector Machines (SVMs), we evaluate these LTAG-based features. Our experiments show that LTAG-based features can improve SRL accuracy significantly. When compared with the best known set of features that are used in state of the art SRL systems we obtain an improvement in F-score from 82.34% to 85.25%.",<TITLE>Lexicalized Tree-Adjoining Grammar as an Application to Semantic Role Labeling<TITLE>Using Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE>Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE>Using LTAG-based Features for Semantic Role Labeling<TITLE>Using LTAG-Based Features for Semantic Role Labeling,<TITLE>Semantic Role Labeling with LTAG-based Features and Lexicalized Tree-Adjoining Grammar (LTAG)<TITLE>Semantic Role Labeling with LTAG-based Features and Lexicalized Tree-Adjoining Grammar (LTAGs<TITLE>Semantic Role Labeling with LTAG-based Features and Lexicalized Tree-Adjoining Grammar (LTAG):<TITLE>Semantic Role Labeling with LTAG-based Features from Tree-Adjoining Grammars and Support Vector Machines: A<TITLE>Semantic Role Labeling with LTAG-based Features from Tree-Adjoining Grammars and Support Vector Machines using L,<TITLE>Using LTAG-based Features for Semantic Role Labeling<TITLE>Semantic Role Labeling with LTAG-based Features<TITLE>Semantic Role Labeling with LTAG-Based Features<TITLE>Semantic Role Labeling with LTAG-based Features<TITLE>Semantic Role Labeling with LTAG-based Features,<TITLE>Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE>Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling.<TITLE>Lexicalized Tree Adjoining Grammar for Semantic Role Labeling<TITLE>Lexicalized Tree-Adjoining Grammar<TITLE>Lexicalized Tree Adjoining Grammar,<TITLE> Lexicalized Tree-Adjoining Grammar Features for Semantic Role Labeling<TITLE> Lexicalized Tree-Adjoining Grammar for Semantic Role Labeling<TITLE> Lexicalized Tree-Adjoining Grammar Features Improve Semantic Role Labeling<TITLE> The Use of Lexicalized Tree-Adjoining Grammar<TITLE> Lexicalized Tree-Adjoining Grammar,<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalism for Improved SRL<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalisms for SRL<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalism for SRL Tasks<TITLE>Using Lexicalized Tree-Adjoining Grammar Formalism for SRL<TITLE>Using Lexicalized Tree-Adjoining Grammar to Improve SRL Accuracy
