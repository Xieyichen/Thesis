{"0":{"0":"Leveraging user-provided translation to constrain NMT has practical significance. Existing methods can be classified into two main categories, namely the use of placeholder tags for lexicon words and the use of hard constraints during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the MNT model or decoding algorithm, allowing the model to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.","1":"This paper presents three hybrid models that directly combine latent Dirichlet allocation and word embedding for distinguishing between speakers with and without Alzheimer's disease from transcripts of picture descriptions. Two of our models get F-scores over the current state-of-the-art using automatic methods on the DementiaBank dataset.","2":"Binarization of grammars is crucial for improving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation.","3":"Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don't achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems.","4":"We show that it is feasible to perform entity linking by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-text links in Wikipedia, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, it can retrieve candidates extremely fast, and generalizes well to a new dataset derived from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised negative mining algorithm for this task.","5":"Are word-level affect lexicons useful in detecting emotions at sentence level? Some prior research finds no gain over and above what is obtained with ngram features\u2014arguably the most widely used features in text classification. Here, we experiment with two very different emotion lexicons and show that even in supervised settings, an affect lexicon can provide significant gains. We further show that while ngram features tend to be accurate, they are often unsuitable for use in new domains. On the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain.","6":"Relation extraction suffers from a performance loss when a model is applied to out-of-domain data. This has fostered the development of domain adaptation techniques for relation extraction. This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.","7":"Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.","8":"In multi-task learning (MTL), multiple tasks are learnt jointly. A major assumption for this paradigm is that all those tasks are indeed related so that the joint training is appropriate and beneficial. In this paper, we study the problem of multi-task learning of shared feature representations among tasks, while simultaneously determining \u201cwith whom\u201d each task should share. We formulate the problem as a mixed integer programming and provide an alternating minimization technique to solve the optimization problem of jointly identifying grouping structures and parameters. The algorithm monotonically decreases the objective function and converges to a local optimum. Compared to the standard MTL paradigm where all tasks are in a single group, our algorithm improves its performance with statistical significance for three out of the four datasets we have studied. We also demonstrate its advantage over other task grouping techniques investigated in literature.","9":"We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.","10":"We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled","11":"This paper provides a binary, token-based classification of German particle verbs (PVs) into literal vs. non-literal usage. A random forest improving standard features (e.g., bagof-words; affective ratings) with PV-specific information and abstraction over common nouns significantly outperforms the majority baseline. In addition, PV-specific classification experiments demonstrate the role of shared particle semantics and semantically related base verbs in PV meaning shifts.","12":"We demonstrate that current state-of-the-art approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences, and propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models.","13":"Existing timeline generation systems for complex events consider only information from traditional media, ignoring the rich social context provided by user-generated content that reveals representative public interests or insightful opinions. We instead aim to generate socially-informed timelines that contain both news article summaries and selected user comments. We present an optimization framework designed to balance topical cohesion between the article and comment summaries along with their informativeness and coverage of the event. Automatic evaluations on real-world datasets that cover four complex events show that our system produces more informative timelines than state-of-theart systems. In human evaluation, the associated comment summaries are furthermore rated more insightful than editor\u2019s picks and comments ranked highly by users.","14":"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.","15":"Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.","16":"In this paper, we present a Chinese event extraction system. We point out a language specific issue in Chinese trigger labeling, and then commit to discussing the contributions of lexical, syntactic and semantic features applied in trigger labeling and argument labeling. As a result, we achieved competitive performance, specifically, F-measure of 59.9 in trigger labeling and F-measure of 43.8 in argument labeling.","17":"We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. We compare our approach against multiple baselines using both automatic metrics and human evaluation. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.","18":"Although users\u2019 preference is semantically reflected in the free-form review texts, this wealth of information was not fully exploited for learning recommender models. Specifically, almost all existing recommendation algorithms only exploit rating scores in order to find users\u2019 preference, but ignore the review texts accompanied with rating information. In this paper, we propose a novel matrix factorization model (called TopicMF) which simultaneously considers the ratings and accompanied review texts. Experimental results on 22 real-world datasets show the superiority of our model over the state-of-the-art models, demonstrating its effectiveness for recommendation tasks.","19":"We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players\u2019 goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent\u2019s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players\u2019 goals, in both cooperative and competitive settings.","20":"Previous work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and relation), but did not achieve satisfactory performance. In this paper, we propose that transition-based model is more appropriate for parsing the naked discourse tree (i.e., identifying span and nuclearity) due to data sparsity. At the same time, we argue that relation labeling can benefit from naked tree structure and should be treated elaborately with consideration of three kinds of relations including within-sentence, across-sentence and across-paragraph relations. Thus, we design a pipelined two-stage parsing method for generating an RST tree from text. Experimental results show that our method achieves state-of-the-art performance, especially on span and nuclearity identification.","21":"The weak equivalence of Combinatory Categorial Grammar (CCG) and Tree-Adjoining Grammar (TAG) is a central result of the literature on mildly context-sensitive grammar formalisms. However, the categorial formalism for which this equivalence has been established differs significantly from the versions of CCG that are in use today. In particular, it allows restriction of combinatory rules on a per grammar basis, whereas modern CCG assumes a universal set of rules, isolating all cross-linguistic variation in the lexicon. In this article we investigate the formal significance of this difference. Our main result is that lexicalized versions of the classical CCG formalism are strictly less powerful than TAG.","22":"This paper introduces a model of multipleinstance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from usercontributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multipleinstance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE.","23":"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling-a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.","24":"In this paper, we target at improving the performance of multi-label emotion classification with the help of sentiment classification. Specifically, we propose a new transfer learning architecture to divide the sentence representation into two different feature spaces, which are expected to respectively capture the general sentiment words and the other important emotion-specific words via a dual attention mechanism. Experimental results on two benchmark datasets demonstrate the effectiveness of our proposed method.","25":"In this paper we study the identification and verification of simple claims about statistical properties, e.g. claims about the population or the inflation rate of a country. We show that this problem is similar to extracting numerical information from text and following recent work, instead of annotating data for each property of interest in order to learn supervised models, we develop a distantly supervised baseline approach using a knowledge base and raw text. In experiments on 16 statistical properties about countries from Freebase we show that our approach identifies simple statistical claims about properties with 60% precision, while it is able to verify these claims without requiring any explicit supervision for either tasks. Furthermore, we evaluate our approach as a statistical property extractor and we show it achieves 0.11 mean absolute percentage error.","26":"Even for common NLP tasks, sufficient supervision is not available in many languages - morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones.","27":"Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.","28":"We prove bounds on the generalization error of convolutional networks. The bounds are in terms of the training loss, the number of parameters, the Lipschitz constant of the loss and the distance from the weights to the initial weights. They are independent of the number of pixels in the input, and the height and width of hidden feature maps. We present experiments using CIFAR-10 with varying hyperparameters of a deep convolutional network, comparing our bounds with practical generalization gaps.","29":"The goal of inverse reinforcement learning is to find a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.","30":"We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.","31":"Linking pronominal expressions to the correct references requires, in many cases, better analysis of the contextual information and external knowledge. In this paper, we propose a two-layer model for pronoun coreference resolution that leverages both context and external knowledge, where a knowledge attention mechanism is designed to ensure the model leveraging the appropriate source of external knowledge based on different context. Experimental results demonstrate the validity and effectiveness of our model, where it outperforms state-of-the-art models by a large margin.","32":"In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.","33":"Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present DRaiL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.","34":"Abstract Meaning Representation (AMR) research has mostly focused on English. We show that it is possible to use AMR annotations for English as a semantic representation for sentences written in other languages. We exploit an AMR parser for English and parallel corpora to learn AMR parsers for Italian, Spanish, German and Chinese. Qualitative analysis show that the new parsers overcome structural differences between the languages. We further propose a method to evaluate the parsers that does not require gold standard data in the target languages. This method highly correlates with the gold standard evaluation, obtaining a Pearson correlation coefficient of 0.95.","35":"In this paper, we propose a novel pretraining-based encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the encoder of our model, we encode the input sequence into context representations using BERT. For the decoder, there are two stages in our model, in the first stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the refined word for each masked position. To the best of our knowledge, our approach is the first method which applies the BERT into text generation tasks. As the first step in this direction, we evaluate our proposed method on the text summarization task. Experimental results show that our model achieves new state-of-the-art on both CNN\/Daily Mail and New York Times datasets.","36":"Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. We show that integrating topic information into bidirectional long short-term memory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in two- and three-label cross-topic settings. We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.","37":"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https:\/\/github.com\/princeton-nlp\/calm-textgame.","38":"Spectral clustering is one of the most popular clustering approaches. Despite its good performance, it is limited in its applicability to large-scale problems due to its high computational complexity. Recently, many approaches have been proposed to accelerate the spectral clustering. Unfortunately, these methods usually sacrifice quite a lot information of the original data, thus result in a degradation of performance. In this paper, we propose a novel approach, called Landmark-based Spectral Clustering (LSC), for large scale clustering problems. Specifically, we select p ( n) representative data points as the landmarks and represent the original data points as the linear combinations of these landmarks. The spectral embedding of the data can then be efficiently computed with the landmark-based representation. The proposed algorithm scales linearly with the problem size. Extensive experiments show the effectiveness and efficiency of our approach comparing to the state-of-the-art methods.","39":"We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.","40":"Most of the current automated essay scoring (AES) systems are trained using manually graded essays from a specific prompt. These systems experience a drop in accuracy when used to grade an essay from a different prompt. Obtaining a large number of manually graded essays each time a new prompt is introduced is costly and not viable. We propose domain adaptation as a solution to adapt an AES system from an initial prompt to a new prompt. We also propose a novel domain adaptation technique that uses Bayesian linear ridge regression. We evaluate our domain adaptation technique on the publicly available Automated Student Assessment Prize (ASAP) dataset and show that our proposed technique is a competitive default domain adaptation algorithm for the AES task.","41":"Evidence-based medicine is an approach whereby clinical decisions are supported by the best available findings gained from scientific research. This requires efficient access to such evidence. To this end, abstracts in evidence-based medicine can be labeled using a set of predefined medical categories, the socalled PICO criteria. This paper presents an approach to automatically annotate sentences in medical abstracts with these labels. Since both structural and sequential information are important for this classification task, we use kLog, a new language for statistical relational learning with kernels. Our results show a clear improvement with respect to state-of-the-art systems.","42":"We present KB-UNIFY, a novel approach for integrating the output of different Open Information Extraction systems into a single unified and fully disambiguated knowledge repository. KB-UNIFY consists of three main steps: (1) disambiguation of relation argument pairs via a sensebased vector representation and a large unified sense inventory; (2) ranking of semantic relations according to their degree of specificity; (3) cross-resource relation alignment and merging based on the semantic similarity of domains and ranges. We tested KB-UNIFY on a set of four heterogeneous knowledge bases, obtaining high-quality results. We discuss and provide evaluations at each stage, and release output and evaluation data for the use and scrutiny of the community1.","43":"We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.","44":"Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. Accordingly, automation strategies, such as natural language generation, are beginning to be investigated. Still, they suffer from the lack of sufficient amount of quality data and tend to produce generic\/repetitive responses. Being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively, employing large scale unsupervised language models such as GPT-2 for the generation of silver data, and the best annotation strategies\/neural architectures that can be used for data filtering before expert validation\/post-editing.","45":"Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.","46":"While unsupervised anaphoric zero pronoun (AZP) resolvers have recently been shown to rival their supervised counterparts in performance, it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features. To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings. Our approach achieves stateof-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus.","47":"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.","48":"We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish.","49":"We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al."},"1":{"0":"Code-Switching for Enhancing NMT with Pre-Specified Translation","1":"Augmenting word2vec with latent Dirichlet allocation within a clinical application","2":"General binarization for parsing and translation","3":"A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction","4":"Learning Dense Representations for Entity Retrieval","5":"Portable Features for Classifying Emotional Text","6":"Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction","7":"Sequence Level Training with Recurrent Neural Networks","8":"Learning with Whom to Share in Multi-task Feature Learning.","9":"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification","10":"Non-Projective Dependency Parsing in Expected Linear Time","11":"Distinguishing Literal and Non-Literal Usage of German Particle Verbs","12":"Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input","13":"Socially-Informed Timeline Generation for Complex Events","14":"Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing","15":"Experimental Support for a Categorical Compositional Distributional Model of Meaning","16":"Language Specific Issue and Feature Exploration in Chinese Event Extraction","17":"Learning to Update Natural Language Comments Based on Code Changes","18":"TopicMF: Simultaneously Exploiting Ratings and Reviews for Recommendation","19":"Modeling Others using Oneself in Multi-Agent Reinforcement Learning","20":"A Two-Stage Parsing Method for Text-Level Discourse Analysis","21":"Lexicalization and Generative Power in CCG","22":"Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis","23":"Enabling Language Models to Fill in the Blanks","24":"Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network","25":"Identification and Verification of Simple Claims about Statistical Properties","26":"Cross-lingual Character-Level Neural Morphological Tagging","27":"Disconnected Recurrent Neural Networks for Text Categorization","28":"Generalization bounds for deep convolutional neural networks","29":"Feature Construction for Inverse Reinforcement Learning","30":"A Neural Network Approach to Context-Sensitive Generation of Conversational Responses","31":"Incorporating Context and External Knowledge for Pronoun Coreference Resolution","32":"Learning Chinese Word Representations From Glyphs Of Characters","33":"Modeling Content and Context with Deep Relational Learning","34":"Cross-Lingual Abstract Meaning Representation Parsing","35":"Pretraining-Based Natural Language Generation for Text Summarization","36":"Cross-topic Argument Mining from Heterogeneous Sources","37":"Keep CALM and Explore: Language Models for Action Generation in Text-based Games","38":"Large Scale Spectral Clustering with Landmark-Based Representation","39":"Variational Auto-encoded Deep Gaussian Processes","40":"Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression","41":"A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories","42":"Knowledge Base Unification via Sense Embeddings and Disambiguation","43":"Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages","44":"Generating Counter Narratives against Online Hate Speech: Data and Strategies","45":"Neural Models for Documents with Metadata","46":"Chinese Zero Pronoun Resolution with Deep Neural Networks","47":"Friendly Topic Assistant for Transformer Based Abstractive Summarization","48":"Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines","49":"Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition"},"2":{"0":{"bart_xsum":"Improving Neural Machine Translation with Source-side Data Augmentation","bart_cnn":"Data Augmentation for Neural Machine Translation with Hard Lexicon Constraints and Source-side Target Words Using Code-Switched","t5_small":"Leveraging User-provided Translation to Constrained Neural Words","pegasus_xsum":"Data Augmentation for Constraint-Free Translation of Lexicon Words","gpt2":"Data Augmentation for Constrained NMT"},"1":{"bart_base":"Latent Dirichlet Allocation and Word Embedding for Detecting Alzheimer's Disease from Transcripts of Descriptions","gpt2":"Combining Latent Dirichlet Allocation and Word Embedding for Detecting Alzheimer's","pegasus_xsum":"Latent Dirichlet Allocation and Word Embedding for Alzheimer's Disease Recognition","bart_xsum":"Hybrid Word Embeddings and Latent Dirichlet Allocation for Alzheimer's Disease Detection","t5_small":"Combining Latent Dirichlet allocation and word embedding for distinguishing speakers with and without Alzheimer's disease"},"2":{"bart_xsum":"A Versatile Algorithm for Binarizing Tree-to-String Transducers","bart_cnn":"Binarization of Grammars for Statistical Machine Translation using Tree-to-String Transducers and its Application to Binar","pegasus_xsum":"A Versatile Binarization Algorithm for Parsing and Machine Translation","bart_base":"Binarization of Grammars for Syntax-Based Machine Translation","t5_small":"Binarization of Grammatical Formalisms"},"3":{"bart_cnn":"Entity-Pair Level Denoise for Distant-Supervised Relation Extraction with Soft Label Combination and Joint Score Function","bart_base":"Entity-Pair Level Denoise for Distant-Supervised Relation Extraction","pegasus_xsum":"Entity-Pair Level Denoise for Distant Relation Extraction","gpt2":"Entity-pair Level Denoising for Relation Extraction","t5_small":"Entity-Pair Level Denoise for Distant-Supervised Relation Extraction"},"4":{"bart_xsum":"Entity Linking via Dual Encoding and Zero-Rank Re-ranking","pegasus_xsum":"Entity Linking with a Dual Parser","gpt2":"Unsupervised Entity Linking via Dual Encoders","bart_cnn":"Dual Encoders for Entity Linking in Dense Two-Towered Datasets: An Unsupervised Negative Mining","t5_small":"Learning Entity Linking with Dual Encoders"},"5":{"t5_small":"Affect Lexicons for Word-Level Emotions Detection","bart_xsum":"Are Affect Lexicons useful for detecting emotions at sentence level?","bart_base":"Are Affect Lexicons Useful for Sentence Level Emotion Detection?","bart_cnn":"Are Word-Level Affect Lexicons Useful in Detecting Emotional Sentences at Sentence Level? A Case Study on Text Classification","pegasus_xsum":"Are Word-Level Affect Lexicons Useful for Detecting Emotions in Text?"},"6":{"pegasus_xsum":"Adapting Feature-Based Relation Extraction Systems with Word Embeddings and Clustering","bart_base":"Domain Adaptation for Relation Extraction Using Word Embeddings and Word Clustering","t5_small":"Word Embeddings and Clustering for Relative Extraction","gpt2":"Adapting Word Embeddings and Clustering for Out-of-Domain Relation Extraction","bart_xsum":"Domain Adaptation of Feature-based Relation Extraction with Word Embeddings and Clustering"},"7":{"pegasus_xsum":"Sequence Level Training for Generative Adversarial Learning","gpt2":"Sequence Level Training for Natural Language Generation","bart_base":"Sequence Level Training for Greedy Generation of Text","bart_xsum":"A Sequence Level Training Algorithm for Greedy Text Generation","bart_cnn":"Towards a Sequence Level Training Algorithm for Greedy Generation of Natural Language Texts using Beam Search and BLEU"},"8":{"bart_xsum":"Mixed Integer Programming for Joint Multi-Task Learning of Shared Feature Representations","t5_small":"Joint Multi-Task Learning of Shared Feature Representations","bart_base":"Multi-Task Learning of Shared Feature Representations with Alternating Minimization","bart_cnn":"Multi-task Learning of Shared Feature Representations among Tasks: A Mixed Integer Programming Algorithm and Alternating Minimization","gpt2":"Multi-Task Learning of Shared Feature Representations"},"9":{"bart_xsum":"Sparsemax: Sparse activation with smooth and convex loss.","bart_cnn":"Sparsemax: Sparsemax for Natural Language Inference with Jacobian and Attention-based Neural Networks in Backpropag","pegasus_xsum":"Sparsemax and Smoothed Convex Loss for Neural Networks","gpt2":"SparseMax: A Sparse Activation Function for Neural Networks","t5_small":"Sparsemax: A Segmentation Function for Multi-Label Classification"},"10":{"t5_small":"A Transition System for Dependency Parsing with arcs","bart_xsum":"A Transition System for Dependency Parsing with Order Swapping","gpt2":"A Transition System for Dependency Parsing","bart_base":"A Transition System for Dependency Parsing with Treebanks","pegasus_xsum":"A Transition System for Dependency Parsing with Word Swapping"},"11":{"bart_base":"Token-Based Classification of German Particle Verbs","t5_small":"Binary, token-based Classification of German Particle Verbs into Literal vs. Non-literal Use","pegasus_xsum":"A Random Forest Approach to German Particle Verb Classification","gpt2":"A Binary Classification of German Particle Verbs","bart_cnn":"A Random Forest-based Classification of Particle Verbs into Literal vs. Non-literal Usage: An Application to"},"12":{"t5_small":"Local Coherence Models for Automated Essay Scoring.","gpt2":"Neural Essay Scoring with Local Coherence","bart_base":"Learning Local Coherence for Automated Essay Scoring","bart_cnn":"Neural Coherence Modeling for Automated Essay Scoring: A Case Study on Text-Driven Neural Essay","pegasus_xsum":"A Neural Local Coherence Model for Automated Essay Scoring"},"13":{"pegasus_xsum":"Generation of Socially-Informed Timelines for Complex Events","bart_cnn":"Generating Social-Informed Timelines for Complex Events: An Optimization Framework for News Article Summarization and Comment Sum","gpt2":"Generating Social-Informed Timelines for Complex Events","bart_xsum":"Generating Social-Informed Timeline Generation for Complex Events with Article Summaries and User Comments","bart_base":"Social Timeline Generation with News Article Summaries and Comments"},"14":{"bart_base":"K-fold Cross-Validation for Bayesian Part-of-Speech Tagging","pegasus_xsum":"Bayesian Statistical Model Comparison for Part-of-Speech Tagging","t5_small":"A Bayesian Analysis of Part-of-Speech Tagger","bart_cnn":"Bayesian statistical model comparison for part-of-speech taggers using k-folding cross-validation and cross-","gpt2":"Bayesian Statistical Model Comparison Using Cross-Validation"},"15":{"t5_small":"Compositional Meaning for Intransitive Sentences with Empirical Distributional Methods","pegasus_xsum":"Modelling Compositional Meaning for Sentences with Unsupervised Learning of Words","bart_xsum":"Modelling Compositional Meaning for Sentences Using Unsupervised Learning of Word Matrices","bart_base":"An Abstract Categorical Model for Compositional Meaning of Sentences","gpt2":"Abstract Categorical Modeling of Compositional Meaning"},"16":{"bart_xsum":"A Chinese Event Extraction System with Lexical, Syntactic and Semantic Features","bart_base":"Chinese Event Extraction Using Lexical, Syntactic and Semantic Features","gpt2":"Chinese Event Extraction with Lexical, Syntactic and Semantic Features","t5_small":"Chinese Event Extraction System for Chinese Trigger Labeling","bart_cnn":"Chinese Event Extraction Using Language Specific Issues in Trigger Labeling and Argument Labeling: An Empirical Study of Chinese Trigger"},"17":{"bart_xsum":"Automatic Comment Update Based on Source Code Changes","gpt2":"Automatic Update of Natural Language Commentaries","bart_cnn":"Learning to Correlate Language Representations for Automatic Comment Retrieval of Natural Language Comments in Open-Source Software Projections","bart_base":"Automatic Update of an existing Comment Based on Changes in the Body of Code","pegasus_xsum":"Automatic Updates to Existing Natural Language Systems"},"18":{"pegasus_xsum":"TopicMF: A Novel Matrix Factorization Model for Recommendation Texts","bart_base":"TopicMF: A Matrix Factorization Model for Recommendation in Review Texts","bart_cnn":"TopicMF: A Topic Factorization Model for Recommendation in Free-Form Review Texts with High-Rank Ratings and Att","t5_small":"TopicMF: A Matrix Factorization Model for Free-form Review Texts","bart_xsum":"TopicMF: A Matrix Factorization Model for Learning Recommender Models from Free-Form Reviews"},"19":{"bart_cnn":"Self Other-Modeling for Multi-Agent Reinforcement Learning with Imperfect Information: Self-Predicting Hidden Goal Actions","t5_small":"Self Other-Modeling for Multi-Agent reinforcement learning with imperfect information","gpt2":"Self Other-Modeling for Multi-Agent Reinforcement Learning","bart_base":"Self Other-Modeling for Multi-Agent Reinforcement Learning with Imperfect Information","pegasus_xsum":"Self- Other-Modeling for Multi-Agent Reinforcement Learning with Imperfect Information"},"20":{"gpt2":"Transition-based Parsing of Discourse Trees","bart_xsum":"Pipeline-based Parsing of Discourse Trees for RST and Relation Labeling","bart_base":"Pipelined Two-Stage Parsing of Named Discourse Trees","bart_cnn":"Transition-based RST Parsing for Span, Nuclearity and Relation Labeling with Nondescourse Tree Structure","t5_small":"Transition-Based Parsing of Null Discourse Trees"},"21":{"bart_xsum":"Combinatory Categorial Grammar and Tree-Adjoining Grammar: A Weak Equivalence but an Empir","pegasus_xsum":"Lexicalized versions of the classic CCG formalism are less powerful than TAG","bart_cnn":"The Weak Equivalence of Combinatory Categorial Grammar and Tree-Adjoining Grammar: An Empirical","t5_small":"Combinatory Categorial Grammar and Tree-Adjoining Grammar","gpt2":"Weak Equivalence of CCG and Tree-Adjoining Grammar"},"22":{"gpt2":"Multiple Instance Regression for Aspect Rating Prediction","pegasus_xsum":"MultipleInstance Learning for Predicting Aspect Ratings from User-contributed Texts","bart_xsum":"Multiple-Instance Learning for Predicting Aspect Ratings from Texts","bart_base":"MultipleInstance Learning for Aspect Rating Prediction from Texts with Known Aspects","bart_cnn":"MultipleInstance Regression for Aspect Ratings and Their Impact on Sentiment Analysis from Texts with Weercontributed Multiple-"},"23":{"bart_xsum":"Text Infilling by Language Modeling: A Case Study on Short Stories, Scientific Abstracts, and Lyrics","gpt2":"Text Infilling with Language Modeling","pegasus_xsum":"Text Infilling by Language Modeling","bart_base":"Text Infilling by Language Modeling","t5_small":"Language Modeling for Infilling Texts"},"24":{"t5_small":"Multi-Label Emotion Classification with Sentiment Classification","bart_cnn":"Transfer Learning for Multi-label Emotion Classification with a Dual Attention Mechanism via a Dual Neural Network Based on Sentence Represent","gpt2":"Transfer Learning for Multi-label Emotion Classification","bart_xsum":"Transfer Learning for Multi-label Emotion Classification with the Help of Sentiment Classification","bart_base":"Transfer Learning for Multi-Label Emotion Classification"},"25":{"bart_cnn":"Identification and Verification of Simple Claims about Statistical Properties Using a Distantly Supervised Baseline Based on Knowledge Base and Raw","bart_xsum":"Identification and Verification of Simple Statistical Claims without Supervision","t5_small":"Identification and Verification of Simple Statistical Properties about Countries","gpt2":"Identification and Verification of Simple Claims about Statistical Properties","bart_base":"Automatic Identification and Verification of Simple Claims about Statistical Properties"},"26":{"bart_base":"Character-Level Recurrent Neural Tagging for Morphological Tagging","bart_cnn":"Learning Character-Level Recurrent Neural Tagging for High-resource Languages and Low-Resource Languages using Transfer Learning in NLP","t5_small":"Transfer Learning for Morphological Tagging in High-Resource Languages","gpt2":"Learning Joint Character Representations for Morphological Tagging","pegasus_xsum":"Transfer Learning for Character-Level Morphological Tagging"},"27":{"bart_xsum":"Disconnected Recurrent Neural Network for Text Categorization","gpt2":"Disconnected Recurrent Neural Network for Text Categorization","t5_small":"Disconnected Recurrent Neural Network for Text Categorization","bart_base":"Connected Recurrent Neural Network for Text Categorization","pegasus_xsum":"Disconnected Recurrent Neural Network for Text Categorization"},"28":{"t5_small":"CIFAR-10 with Hyperparameters of Deep Convolutional Network","pegasus_xsum":"Bounds on the Error of Convolutional Networks","bart_base":"Bounds on Generalization Error of Convolutional Networks","gpt2":"Bounds on the Generalization Error of Convolutional Networks","bart_cnn":"Bounds on the Generalization Error of Convolutional Networks with Applications to CIFAR-10 Bounds and Hidden Feature Maps"},"29":{"gpt2":"Inverse Reinforcement Learning with Logical Conjunctions","bart_xsum":"Inverse Reinforcement Learning with Component Features.","bart_base":"Inverse Reinforcement Learning with Component Features.","t5_small":"Inverse Reinforcement Learning with Logical Combinations","pegasus_xsum":"Inverse Reinforcement Learning by Constructing Reward Features"},"30":{"bart_xsum":"Learning to Generate Responses from Twitter Conversations","pegasus_xsum":"Dynamic-Context Generative Models for End-to-End Neural Response Generation","bart_cnn":"Dynamic-Context Generative Models for Unstructured Twitter Conversations with Context-sensitive Machine Translation and Information Retrieval Systems","gpt2":"End-to-End Response Generation with Neural Networks","t5_small":"Neural Response Generation for Unstructured Twitter Conversations"},"31":{"bart_cnn":"A Two-Layer Model for Pronoun Coreference Resolution Based on External Knowledge and Contextual Information Leveraging Context and External","bart_xsum":"A Two-Layer Model for Pronoun Coreference Resolution with Knowledge Attention","pegasus_xsum":"A Two-Layer Model for Pronoun Coreference Resolution with Context and External Knowledge","gpt2":"A Two-Layer Model for Pronominal Expressions","t5_small":"A Two-layer Pronoun Coreference Resolution with Knowledge Attention"},"32":{"t5_small":"Learning Chinese Word Representations by Character Glyphens","pegasus_xsum":"Convolutional Auto-encoder for Chinese Word Representations","bart_xsum":"Learning Chinese Word Representations with Character Glyphs","gpt2":"Learning Chinese Word Representations with Character Glyphs","bart_base":"Learning Chinese Word Representations by Character G glyphs"},"33":{"bart_cnn":"DRaiL: An Open-Source Declarative Framework for Deep Relational Modeling with Symbolic and Symbolic Represent","bart_xsum":"DRaiL: A Deep Relational Modeling Framework for Natural Language Processing","gpt2":"DRaiL: Deep Relational Models for Natural Language Tasks","bart_base":"DRaiL: A Deep Relational Model for Natural Language Processing","pegasus_xsum":"DRaiL: An Open-Source Declarative Framework for Combining Symbolic and Neural Models"},"34":{"bart_cnn":"AMR Parsing for English and Parallel Corpora: A Qualitative Analysis of the Role of AMR Parsers in AM","bart_base":"Improving AMR Parsing for English and Parallel Corpora Using AMR Annotations","gpt2":"Learning Semantic Parsers for English Using Parallel Corpora","pegasus_xsum":"Learning Abstract Meaning Representations for Sentences Written in Other Languages","bart_xsum":"Learning AMR Parsers for Languages Other than English"},"35":{"pegasus_xsum":"A Two-Stage Pretraining-Based Transformer-based Sequence Generation Method","gpt2":"Towards Text Generation with Pretraining-Based Encoders","bart_base":"Pretraining-based Encoder-Decoder for Text Summarization","t5_small":"Pretraining-based encoder-decoder framework for text generation","bart_cnn":"Pretraining-based Encoder-decoder for Textual Summarization Using Transformer-based Neural Network Representation"},"36":{"t5_small":"Sentential Annotation for Argument Mining in Web Texts","gpt2":"Sentential Annotation of Web Texts","bart_cnn":"Sentential Annotation for Argument Mining on Web Texts Using Multi-Task Learning and Bidirectional Long Short-Term Memory","pegasus_xsum":"Sentential Argument Mining for Heterogeneous Web Texts","bart_base":"Argument Mining for Web Texts with Topic Relevance"},"37":{"t5_small":"Contextual Action Language Models for Text-based Games","bart_cnn":"CALM: A Contextual Action Language Model for Text-Based Games in Natural Language and Social Media Using Reinforcement Learning","gpt2":"Contextual Action Language Model","pegasus_xsum":"Contextual Action Language Model for Text-Based Games","bart_xsum":"CALM: Contextual Action Language Model for Text-based Games"},"38":{"gpt2":"Landmark-based Spectral Clustering for Large Scale Clustering","bart_xsum":"Landmark-based Spectral Clustering for Large-Scale Problem Resolution","bart_cnn":"Landmark-based Spectral Clustering: Landmark-Based Representation and Algorithm for Large-Scale Clust","t5_small":"Landmark-based Spectral Clustering for Large Scale Clustering","bart_base":"Landmark-based Spectral Clustering for Large Scale Problems"},"39":{"bart_xsum":"Scalable Deep Non-parametric Generative Models via Multilayer Perceptron Reparametrizing","gpt2":"Deep Nonparametric Generative Models","t5_small":"Scalable Deep Non-parametric Generative Models","bart_cnn":"A Scalable Variational Model for Deep Gaussian Processes with a Multilayer Perceptron Reparametrized","pegasus_xsum":"Scaling Non-Parametric Generative Models"},"40":{"bart_base":"Domain Adaptation for Automated Essay Scoring","bart_cnn":"Domain Adaptation for Automated Essay Scoring Using Bayesian Linear-Ridge Regression using Domain Adaptation and Domain","t5_small":"Domain Adaptation for Automated Essay Scoring","bart_xsum":"Domain Adaptation for Automated Essay Scoring","gpt2":"Domain Adaptation for Automatic Essay Scoring"},"41":{"bart_cnn":"Automatic Classification of Evidence-Based Medical Abstracts Using PICO Criteria and Statistical Relational Learning with Kernels for Clinical","bart_xsum":"Automatic Classification of Abstracts in Evidence-Based Medicine Using kLog","pegasus_xsum":"Classification of Medical Abstracts Using k-Log Kernels","bart_base":"Automatic Labeling of Medical Abstracts in Evidence-Based medicine","t5_small":"Automatic Annotation of Sentences in Evidence-Based Medicine"},"42":{"pegasus_xsum":"KB-UN: Integrating Open Information Extraction Systems into a Unified Knowledge Repository","bart_xsum":"KB-UNIFY: Unified Disambiguation and Alignment of Heterogeneous Knowledge Bases","t5_small":"KB-UNIFY: Unified and Fully Disambiguated Knowledge Repository for Open Information Extraction","bart_base":"KB-UNIFY: A Unified and Fully Disambiguated Knowledge Base for Open Information Extraction","bart_cnn":"KB-UNIFY: Jointly Disambiguating Relation Argument Pairs and Sense-Based Representations for Open Information"},"43":{"t5_small":"Dependency Parsing for Preprocessing Preprocessing Statistical Machine Translation Systems","gpt2":"Parse-Based Preference Reordering for Statistical Machine Translation","bart_base":"A Dependency Parser for Statistical Machine Translation","bart_cnn":"Precedence Reordering Based on a Dependency Parser for Statistical Machine Translation of Subject-Object-Verb Ordering","bart_xsum":"A Dependency Parser for Pre-processing Reordering in Statistical Machine Translation"},"44":{"bart_xsum":"Collecting Responses to Hate: A Case Study on Unsupervised Generation of Textual Response to Hateful Content","t5_small":"Using Textual Responses to Hass in Online Discussion","bart_base":"Collecting Responses to Hate with Unsupervised Language Models","bart_cnn":"Automation Strategies to Collect Responses to Hate in Online Discussions: An Empirical Study of the Role of Content Moder","pegasus_xsum":"Collecting Responses to Hate on Social Media"},"45":{"t5_small":"Variational Inference for Text Corpora Modeling","bart_base":"Variational Inference of Text Corpora with Topic Models","bart_xsum":"Modeling Text Corpora with Topic Models and Variational Inference","pegasus_xsum":"Topic Models for Text Corpora with Metadata","bart_cnn":"Neural Variational Inference for Document Collection Modelling with Topic Models: An Exploration of US News Articles about US Immigration Data"},"46":{"gpt2":"Unsupervised Anaphoric Zero Pronoun Resolution with Deep Neural Networks","t5_small":"Supervised Zero Pronoun Resolution with Deep Neural Networks","pegasus_xsum":"Supervised Anaphoric Zero Pronoun Resolution with Neural Networks","bart_cnn":"Supervised Zero Pronoun Resolution with Deep Lexical Features and Word Embeddings via Word Representation Leveraging Neural Networks","bart_xsum":"Supervised Anaphoric Zero Pronoun Resolution with Deep Neural Networks"},"47":{"bart_base":"Transformer-Based Abstractive Document Summarization with Topic Assistant","bart_cnn":"Topic Assistant for Transformer Summarization: A Plug-and-play Model and a Task-Oriented Topic Assistant","pegasus_xsum":"Topic Assistant: A Plug-and-play Transformer for Abstractive Document Summarization","gpt2":"A Topic Assistant for Abstractive Document Summarization","bart_xsum":"Topic Assistant: A Plug-and-Play Topic Model for Abstractive Document Summarization"},"48":{"gpt2":"Non-Projective Dependency Parsing with Projectivization","bart_xsum":"Projective Dependency Parsing with SVM Classifiers.","t5_small":"SVM Classifiers for Deterministic Parsing","bart_base":"Predicting the Next Action of a Deterministic Parser using SVM Classifiers","pegasus_xsum":"Predicting the Next Action of a Deterministic Parser using SVMs"},"49":{"gpt2":"Learning Label Trees for Large Scale Classification","t5_small":"Learning Label Trees for Large Scale Image Classification","bart_base":"Learning Label Trees for Large Scale Image Classification with Many Classes","bart_cnn":"Efficient Learning of Label Trees for Large Scale Classification with Many Classifiers and More Balanced Tree Structure Tasks. A Case Study","pegasus_xsum":"Learning a Label Tree for Large Scale Image Classification"}},"3":{"0":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"bart_xsum_best":0.0,"bart_xsum_worst":2.0,"bart_xsum_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.0,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333},"1":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_base_best":1.0,"bart_base_worst":1.0,"bart_base_bws":0.0,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.0,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333},"2":{"human_title_selected_best":0.0,"human_title_selected_worst":0.0,"human_title_bws":0.0,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"bart_cnn_best":2.0,"bart_cnn_worst":1.0,"bart_cnn_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333,"bart_base_best":1.0,"bart_base_worst":1.0,"bart_base_bws":0.0,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333},"3":{"human_title_selected_best":1.0,"human_title_selected_worst":1.0,"human_title_bws":0.0,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667},"4":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"t5_small_best":1.0,"t5_small_worst":1.0,"t5_small_bws":0.0},"5":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":2.0,"bart_xsum_bws":-0.6666666667,"bart_base_best":1.0,"bart_base_worst":1.0,"bart_base_bws":0.0,"bart_cnn_best":2.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.0},"6":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"bart_base_best":2.0,"bart_base_worst":0.0,"bart_base_bws":0.6666666667,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":2.0,"bart_xsum_bws":-0.6666666667},"7":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333},"8":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"bart_base_best":2.0,"bart_base_worst":1.0,"bart_base_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0},"9":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333},"10":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"bart_xsum_best":1.0,"bart_xsum_worst":1.0,"bart_xsum_bws":0.0,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667},"11":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":2.0,"gpt2_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333},"12":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"bart_base_best":1.0,"bart_base_worst":0.0,"bart_base_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333},"13":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.0,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333},"14":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.0,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"gpt2_best":2.0,"gpt2_worst":1.0,"gpt2_bws":0.3333333333},"15":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":2.0,"gpt2_bws":-0.6666666667},"16":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"bart_base_best":2.0,"bart_base_worst":0.0,"bart_base_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667},"17":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667},"18":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667},"19":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"gpt2_best":1.0,"gpt2_worst":1.0,"gpt2_bws":0.0,"bart_base_best":1.0,"bart_base_worst":0.0,"bart_base_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333},"20":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":2.0,"bart_base_bws":-0.6666666667,"bart_cnn_best":2.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.6666666667,"t5_small_best":0.0,"t5_small_worst":1.0,"t5_small_bws":-0.3333333333},"21":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.0,"t5_small_best":0.0,"t5_small_worst":0.0,"t5_small_bws":0.0,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667},"22":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.0,"bart_base_best":1.0,"bart_base_worst":1.0,"bart_base_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667},"23":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"bart_base_best":0.0,"bart_base_worst":2.0,"bart_base_bws":-0.6666666667,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333},"24":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333},"25":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667,"gpt2_best":1.0,"gpt2_worst":1.0,"gpt2_bws":0.0,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0},"26":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":2.0,"bart_base_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":0.0},"27":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"gpt2_best":2.0,"gpt2_worst":1.0,"gpt2_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333},"28":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"gpt2_best":2.0,"gpt2_worst":1.0,"gpt2_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667},"29":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"bart_xsum_best":0.0,"bart_xsum_worst":2.0,"bart_xsum_bws":-0.6666666667,"bart_base_best":1.0,"bart_base_worst":0.0,"bart_base_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":0.0,"t5_small_bws":0.0,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333},"30":{"human_title_selected_best":0.0,"human_title_selected_worst":0.0,"human_title_bws":0.0,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667},"31":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"t5_small_best":0.0,"t5_small_worst":0.0,"t5_small_bws":0.0},"32":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"bart_base_best":0.0,"bart_base_worst":2.0,"bart_base_bws":-0.6666666667},"33":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"bart_base_best":1.0,"bart_base_worst":0.0,"bart_base_bws":0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667},"34":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_cnn_best":1.0,"bart_cnn_worst":1.0,"bart_cnn_bws":0.0,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333},"35":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":0.0,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"bart_base_best":1.0,"bart_base_worst":0.0,"bart_base_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":0.0,"t5_small_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667},"36":{"human_title_selected_best":0.0,"human_title_selected_worst":0.0,"human_title_bws":0.0,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0},"37":{"human_title_selected_best":0.0,"human_title_selected_worst":0.0,"human_title_bws":0.0,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667},"38":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"gpt2_best":0.0,"gpt2_worst":0.0,"gpt2_bws":0.0,"bart_xsum_best":0.0,"bart_xsum_worst":2.0,"bart_xsum_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"bart_base_best":1.0,"bart_base_worst":0.0,"bart_base_bws":0.3333333333},"39":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":2.0,"bart_xsum_bws":-0.6666666667,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333,"pegasus_xsum_best":2.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.6666666667},"40":{"human_title_selected_best":2.0,"human_title_selected_worst":0.0,"human_title_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":2.0,"bart_base_bws":-0.6666666667,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"t5_small_best":0.0,"t5_small_worst":0.0,"t5_small_bws":0.0,"bart_xsum_best":1.0,"bart_xsum_worst":1.0,"bart_xsum_bws":0.0,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333},"41":{"human_title_selected_best":0.0,"human_title_selected_worst":2.0,"human_title_bws":-0.6666666667,"bart_cnn_best":1.0,"bart_cnn_worst":0.0,"bart_cnn_bws":0.3333333333,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":1.0,"bart_base_bws":-0.3333333333,"t5_small_best":1.0,"t5_small_worst":1.0,"t5_small_bws":0.0},"42":{"human_title_selected_best":1.0,"human_title_selected_worst":0.0,"human_title_bws":0.3333333333,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":2.0,"pegasus_xsum_bws":-0.6666666667,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667},"43":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"t5_small_best":1.0,"t5_small_worst":0.0,"t5_small_bws":0.3333333333,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"bart_base_best":1.0,"bart_base_worst":1.0,"bart_base_bws":0.0,"bart_cnn_best":1.0,"bart_cnn_worst":1.0,"bart_cnn_bws":0.0,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333},"44":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667,"bart_base_best":1.0,"bart_base_worst":1.0,"bart_base_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333},"45":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667,"bart_base_best":0.0,"bart_base_worst":0.0,"bart_base_bws":0.0,"bart_xsum_best":1.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":0.0,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667},"46":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"gpt2_best":0.0,"gpt2_worst":1.0,"gpt2_bws":-0.3333333333,"t5_small_best":2.0,"t5_small_worst":0.0,"t5_small_bws":0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":-0.3333333333,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667},"47":{"human_title_selected_best":0.0,"human_title_selected_worst":1.0,"human_title_bws":-0.3333333333,"bart_base_best":0.0,"bart_base_worst":2.0,"bart_base_bws":-0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":1.0,"bart_cnn_bws":-0.3333333333,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.3333333333,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"bart_xsum_best":2.0,"bart_xsum_worst":0.0,"bart_xsum_bws":0.6666666667},"48":{"human_title_selected_best":0.0,"human_title_selected_worst":0.0,"human_title_bws":0.0,"gpt2_best":1.0,"gpt2_worst":0.0,"gpt2_bws":0.3333333333,"bart_xsum_best":0.0,"bart_xsum_worst":1.0,"bart_xsum_bws":-0.3333333333,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"bart_base_best":2.0,"bart_base_worst":0.0,"bart_base_bws":0.6666666667,"pegasus_xsum_best":1.0,"pegasus_xsum_worst":1.0,"pegasus_xsum_bws":0.0},"49":{"human_title_selected_best":0.0,"human_title_selected_worst":0.0,"human_title_bws":0.0,"gpt2_best":2.0,"gpt2_worst":0.0,"gpt2_bws":0.6666666667,"t5_small_best":0.0,"t5_small_worst":2.0,"t5_small_bws":-0.6666666667,"bart_base_best":2.0,"bart_base_worst":0.0,"bart_base_bws":0.6666666667,"bart_cnn_best":0.0,"bart_cnn_worst":2.0,"bart_cnn_bws":-0.6666666667,"pegasus_xsum_best":0.0,"pegasus_xsum_worst":0.0,"pegasus_xsum_bws":0.0}},"4":{"0":{"bart_xsum_bertscore":0.8495336175,"bart_xsum_bartscore":-3.7958319187,"bart_xsum_moverscore":0.5104812504,"bart_cnn_bertscore":0.8586476445,"bart_cnn_bartscore":-3.8157439232,"bart_cnn_moverscore":0.5385206289,"t5_small_bertscore":0.8544283509,"t5_small_bartscore":-3.6675417423,"t5_small_moverscore":0.5128520308,"pegasus_xsum_bertscore":0.8484686017,"pegasus_xsum_bartscore":-4.077794075,"pegasus_xsum_moverscore":0.512942836,"gpt2_bertscore":0.8431910276,"gpt2_bartscore":-4.0001707077,"gpt2_moverscore":0.5077996356},"1":{"bart_base_bertscore":0.8786065578,"bart_base_bartscore":-2.6201529503,"bart_base_moverscore":0.5584622696,"gpt2_bertscore":0.8734999299,"gpt2_bartscore":-2.350256443,"gpt2_moverscore":0.5463214893,"pegasus_xsum_bertscore":0.870218575,"pegasus_xsum_bartscore":-2.2510681152,"pegasus_xsum_moverscore":0.5342745748,"bart_xsum_bertscore":0.8626992106,"bart_xsum_bartscore":-2.5390927792,"bart_xsum_moverscore":0.543471788,"t5_small_bertscore":0.8425399065,"t5_small_bartscore":-4.0567388535,"t5_small_moverscore":0.5237891262},"2":{"bart_xsum_bertscore":0.887790978,"bart_xsum_bartscore":-1.8962483406,"bart_xsum_moverscore":0.5493637713,"bart_cnn_bertscore":0.8845931292,"bart_cnn_bartscore":-3.1046192646,"bart_cnn_moverscore":0.5764211934,"pegasus_xsum_bertscore":0.8863637447,"pegasus_xsum_bartscore":-2.2946219444,"pegasus_xsum_moverscore":0.5521015964,"bart_base_bertscore":0.8811714649,"bart_base_bartscore":-1.9575325251,"bart_base_moverscore":0.5419893866,"t5_small_bertscore":0.8807325959,"t5_small_bartscore":-3.3162822723,"t5_small_moverscore":0.5215602824},"3":{"bart_cnn_bertscore":0.833955586,"bart_cnn_bartscore":-3.0495541096,"bart_cnn_moverscore":0.5284758129,"bart_base_bertscore":0.8350874186,"bart_base_bartscore":-2.5803086758,"bart_base_moverscore":0.5061923219,"pegasus_xsum_bertscore":0.8242738247,"pegasus_xsum_bartscore":-3.1636252403,"pegasus_xsum_moverscore":0.5016569945,"gpt2_bertscore":0.8158958554,"gpt2_bartscore":-3.5381398201,"gpt2_moverscore":0.4982028081,"t5_small_bertscore":0.8350874186,"t5_small_bartscore":-2.5803086758,"t5_small_moverscore":0.5061923219},"4":{"bart_xsum_bertscore":0.823949635,"bart_xsum_bartscore":-4.3909378052,"bart_xsum_moverscore":0.4989831284,"pegasus_xsum_bertscore":0.8376330137,"pegasus_xsum_bartscore":-3.8331565857,"pegasus_xsum_moverscore":0.5072108879,"gpt2_bertscore":0.8372391462,"gpt2_bartscore":-3.9956183434,"gpt2_moverscore":0.4955986895,"bart_cnn_bertscore":0.821167767,"bart_cnn_bartscore":-3.7149300575,"bart_cnn_moverscore":0.4965737876,"t5_small_bertscore":0.8355795145,"t5_small_bartscore":-2.7271943092,"t5_small_moverscore":0.524049299},"5":{"t5_small_bertscore":0.8520236611,"t5_small_bartscore":-3.356477499,"t5_small_moverscore":0.503913382,"bart_xsum_bertscore":0.8706796169,"bart_xsum_bartscore":-1.5444196463,"bart_xsum_moverscore":0.5284203048,"bart_base_bertscore":0.8633634448,"bart_base_bartscore":-2.6508646011,"bart_base_moverscore":0.5230903235,"bart_cnn_bertscore":0.8645538092,"bart_cnn_bartscore":-2.5461101532,"bart_cnn_moverscore":0.5429267607,"pegasus_xsum_bertscore":0.8362259865,"pegasus_xsum_bartscore":-4.4428172112,"pegasus_xsum_moverscore":0.4888081364},"6":{"pegasus_xsum_bertscore":0.8613924384,"pegasus_xsum_bartscore":-1.9947621822,"pegasus_xsum_moverscore":0.5410953536,"bart_base_bertscore":0.8584913611,"bart_base_bartscore":-2.1897265911,"bart_base_moverscore":0.5397862913,"t5_small_bertscore":0.8663136959,"t5_small_bartscore":-3.2756500244,"t5_small_moverscore":0.5418716375,"gpt2_bertscore":0.8405109048,"gpt2_bartscore":-2.1333448887,"gpt2_moverscore":0.5134858723,"bart_xsum_bertscore":0.8648331761,"bart_xsum_bartscore":-2.1817162037,"bart_xsum_moverscore":0.5414213306},"7":{"pegasus_xsum_bertscore":0.8257209659,"pegasus_xsum_bartscore":-4.4184012413,"pegasus_xsum_moverscore":0.4854973904,"gpt2_bertscore":0.8292062879,"gpt2_bartscore":-4.5976028442,"gpt2_moverscore":0.4882030846,"bart_base_bertscore":0.8509582281,"bart_base_bartscore":-4.2110629082,"bart_base_moverscore":0.4944373593,"bart_xsum_bertscore":0.8291145563,"bart_xsum_bartscore":-4.2186121941,"bart_xsum_moverscore":0.5088785458,"bart_cnn_bertscore":0.8524447083,"bart_cnn_bartscore":-3.3874745369,"bart_cnn_moverscore":0.5159448093},"8":{"bart_xsum_bertscore":0.8415510058,"bart_xsum_bartscore":-4.0775351524,"bart_xsum_moverscore":0.5070062735,"t5_small_bertscore":0.8547862172,"t5_small_bartscore":-3.0797324181,"t5_small_moverscore":0.5192693152,"bart_base_bertscore":0.8562777638,"bart_base_bartscore":-3.2774107456,"bart_base_moverscore":0.505032341,"bart_cnn_bertscore":0.8461010456,"bart_cnn_bartscore":-2.6519806385,"bart_cnn_moverscore":0.5165852084,"gpt2_bertscore":0.8541832566,"gpt2_bartscore":-2.4916265011,"gpt2_moverscore":0.5358347716},"9":{"bart_xsum_bertscore":0.8443396688,"bart_xsum_bartscore":-3.4679896832,"bart_xsum_moverscore":0.5241697262,"bart_cnn_bertscore":0.8487730026,"bart_cnn_bartscore":-2.8566775322,"bart_cnn_moverscore":0.5110701961,"pegasus_xsum_bertscore":0.8537012339,"pegasus_xsum_bartscore":-3.95413661,"pegasus_xsum_moverscore":0.5317369005,"gpt2_bertscore":0.8367628455,"gpt2_bartscore":-3.5803842545,"gpt2_moverscore":0.5160864325,"t5_small_bertscore":0.8476507068,"t5_small_bartscore":-3.6586937904,"t5_small_moverscore":0.5167847465},"10":{"t5_small_bertscore":0.8470192552,"t5_small_bartscore":-3.5022277832,"t5_small_moverscore":0.5155136912,"bart_xsum_bertscore":0.8534858227,"bart_xsum_bartscore":-1.9783338308,"bart_xsum_moverscore":0.5183780493,"gpt2_bertscore":0.8487731218,"gpt2_bartscore":-3.895832777,"gpt2_moverscore":0.5087589965,"bart_base_bertscore":0.8522314429,"bart_base_bartscore":-2.8977797031,"bart_base_moverscore":0.505807668,"pegasus_xsum_bertscore":0.8548885584,"pegasus_xsum_bartscore":-3.1265108585,"pegasus_xsum_moverscore":0.5166888686},"11":{"bart_base_bertscore":0.8352602124,"bart_base_bartscore":-2.5038628578,"bart_base_moverscore":0.5265714506,"t5_small_bertscore":0.8462309241,"t5_small_bartscore":-2.7934043407,"t5_small_moverscore":0.5246321869,"pegasus_xsum_bertscore":0.8638968468,"pegasus_xsum_bartscore":-1.8630105257,"pegasus_xsum_moverscore":0.5498978081,"gpt2_bertscore":0.8374056816,"gpt2_bartscore":-3.2036724091,"gpt2_moverscore":0.5220158126,"bart_cnn_bertscore":0.8368361592,"bart_cnn_bartscore":-3.3028481007,"bart_cnn_moverscore":0.5142491635},"12":{"t5_small_bertscore":0.8671901226,"t5_small_bartscore":-1.9276218414,"t5_small_moverscore":0.5084778652,"gpt2_bertscore":0.8519392014,"gpt2_bartscore":-2.6895031929,"gpt2_moverscore":0.5231741719,"bart_base_bertscore":0.8329424858,"bart_base_bartscore":-3.2564153671,"bart_base_moverscore":0.4975400014,"bart_cnn_bertscore":0.8573155403,"bart_cnn_bartscore":-2.6077766418,"bart_cnn_moverscore":0.5032985721,"pegasus_xsum_bertscore":0.8531660438,"pegasus_xsum_bartscore":-3.4348201752,"pegasus_xsum_moverscore":0.5291623391},"13":{"pegasus_xsum_bertscore":0.8469957709,"pegasus_xsum_bartscore":-2.7521941662,"pegasus_xsum_moverscore":0.509991007,"bart_cnn_bertscore":0.8409448266,"bart_cnn_bartscore":-2.6571354866,"bart_cnn_moverscore":0.5513416765,"gpt2_bertscore":0.8511157632,"gpt2_bartscore":-3.0550222397,"gpt2_moverscore":0.5100199969,"bart_xsum_bertscore":0.857337296,"bart_xsum_bartscore":-2.5466971397,"bart_xsum_moverscore":0.5057538245,"bart_base_bertscore":0.8579454422,"bart_base_bartscore":-2.8237557411,"bart_base_moverscore":0.5417674788},"14":{"bart_base_bertscore":0.8652992845,"bart_base_bartscore":-3.8355956078,"bart_base_moverscore":0.5337079802,"pegasus_xsum_bertscore":0.8625382781,"pegasus_xsum_bartscore":-2.3891925812,"pegasus_xsum_moverscore":0.5264496253,"t5_small_bertscore":0.8670583963,"t5_small_bartscore":-2.2083542347,"t5_small_moverscore":0.5246487988,"bart_cnn_bertscore":0.8535614014,"bart_cnn_bartscore":-2.801459074,"bart_cnn_moverscore":0.5171186965,"gpt2_bertscore":0.8769419789,"gpt2_bartscore":-2.3042862415,"gpt2_moverscore":0.5467329884},"15":{"t5_small_bertscore":0.8528432846,"t5_small_bartscore":-2.8488130569,"t5_small_moverscore":0.5143496474,"pegasus_xsum_bertscore":0.8658958673,"pegasus_xsum_bartscore":-2.0893170834,"pegasus_xsum_moverscore":0.5180748984,"bart_xsum_bertscore":0.8433367014,"bart_xsum_bartscore":-4.550801754,"bart_xsum_moverscore":0.5176870974,"bart_base_bertscore":0.8663439751,"bart_base_bartscore":-2.1116838455,"bart_base_moverscore":0.5280736646,"gpt2_bertscore":0.8527121544,"gpt2_bartscore":-2.4812407494,"gpt2_moverscore":0.5157876926},"16":{"bart_xsum_bertscore":0.8774687052,"bart_xsum_bartscore":-1.75857234,"bart_xsum_moverscore":0.5240430389,"bart_base_bertscore":0.8685839176,"bart_base_bartscore":-2.0628006458,"bart_base_moverscore":0.5163249046,"gpt2_bertscore":0.8676721454,"gpt2_bartscore":-2.2925806046,"gpt2_moverscore":0.5135566887,"t5_small_bertscore":0.8555195332,"t5_small_bartscore":-2.8051748276,"t5_small_moverscore":0.5039163097,"bart_cnn_bertscore":0.861487627,"bart_cnn_bartscore":-3.7636721134,"bart_cnn_moverscore":0.5229976335},"17":{"bart_xsum_bertscore":0.8439386487,"bart_xsum_bartscore":-3.0670166016,"bart_xsum_moverscore":0.5004392561,"gpt2_bertscore":0.8319403529,"gpt2_bartscore":-4.759645462,"gpt2_moverscore":0.4922258883,"bart_cnn_bertscore":0.8528865576,"bart_cnn_bartscore":-3.1776885986,"bart_cnn_moverscore":0.5157014141,"bart_base_bertscore":0.8438130021,"bart_base_bartscore":-3.579438448,"bart_base_moverscore":0.5298628723,"pegasus_xsum_bertscore":0.861212194,"pegasus_xsum_bartscore":-2.3643772602,"pegasus_xsum_moverscore":0.52573805},"18":{"pegasus_xsum_bertscore":0.81659168,"pegasus_xsum_bartscore":-3.554059267,"pegasus_xsum_moverscore":0.5063797967,"bart_base_bertscore":0.8482875228,"bart_base_bartscore":-3.0902013779,"bart_base_moverscore":0.5206066459,"bart_cnn_bertscore":0.846082449,"bart_cnn_bartscore":-2.8812601566,"bart_cnn_moverscore":0.522799222,"t5_small_bertscore":0.8434232473,"t5_small_bartscore":-3.6464750767,"t5_small_moverscore":0.5333041376,"bart_xsum_bertscore":0.8555787802,"bart_xsum_bartscore":-2.4678065777,"bart_xsum_moverscore":0.5168010169},"19":{"bart_cnn_bertscore":0.8583076,"bart_cnn_bartscore":-2.8800418377,"bart_cnn_moverscore":0.5366577071,"t5_small_bertscore":0.871714592,"t5_small_bartscore":-2.352701664,"t5_small_moverscore":0.5205630268,"gpt2_bertscore":0.8594589233,"gpt2_bartscore":-2.1647052765,"gpt2_moverscore":0.5068679508,"bart_base_bertscore":0.8579295874,"bart_base_bartscore":-2.1279110909,"bart_base_moverscore":0.5205630268,"pegasus_xsum_bertscore":0.8396472335,"pegasus_xsum_bartscore":-4.2617230415,"pegasus_xsum_moverscore":0.5133706858},"20":{"gpt2_bertscore":0.8443670869,"gpt2_bartscore":-3.1390967369,"gpt2_moverscore":0.5021175901,"bart_xsum_bertscore":0.842196703,"bart_xsum_bartscore":-3.1835308075,"bart_xsum_moverscore":0.5216123788,"bart_base_bertscore":0.833966434,"bart_base_bartscore":-3.4654533863,"bart_base_moverscore":0.5042764264,"bart_cnn_bertscore":0.8381336331,"bart_cnn_bartscore":-3.7130448818,"bart_cnn_moverscore":0.5240520865,"t5_small_bertscore":0.853510201,"t5_small_bartscore":-3.6454944611,"t5_small_moverscore":0.5155255831},"21":{"bart_xsum_bertscore":0.8424309492,"bart_xsum_bartscore":-2.1471557617,"bart_xsum_moverscore":0.5212708268,"pegasus_xsum_bertscore":0.8607985973,"pegasus_xsum_bartscore":-1.9023892879,"pegasus_xsum_moverscore":0.5389443252,"bart_cnn_bertscore":0.8571727276,"bart_cnn_bartscore":-1.5830150843,"bart_cnn_moverscore":0.531492499,"t5_small_bertscore":0.8506295085,"t5_small_bartscore":-1.3857660294,"t5_small_moverscore":0.4993160437,"gpt2_bertscore":0.8590990901,"gpt2_bartscore":-2.3238198757,"gpt2_moverscore":0.5110086402},"22":{"gpt2_bertscore":0.8348371387,"gpt2_bartscore":-4.1733603477,"gpt2_moverscore":0.5174571357,"pegasus_xsum_bertscore":0.8234495521,"pegasus_xsum_bartscore":-4.2524113655,"pegasus_xsum_moverscore":0.5044602013,"bart_xsum_bertscore":0.8482390046,"bart_xsum_bartscore":-3.247698307,"bart_xsum_moverscore":0.5211361426,"bart_base_bertscore":0.8454300165,"bart_base_bartscore":-3.1698083878,"bart_base_moverscore":0.5207989359,"bart_cnn_bertscore":0.8468863964,"bart_cnn_bartscore":-3.5823686123,"bart_cnn_moverscore":0.5225328734},"23":{"bart_xsum_bertscore":0.8282399178,"bart_xsum_bartscore":-3.6333014965,"bart_xsum_moverscore":0.5025668944,"gpt2_bertscore":0.8488640189,"gpt2_bartscore":-2.4022386074,"gpt2_moverscore":0.5224044141,"pegasus_xsum_bertscore":0.8344413638,"pegasus_xsum_bartscore":-3.8071060181,"pegasus_xsum_moverscore":0.4947413615,"bart_base_bertscore":0.8366302252,"bart_base_bartscore":-2.8919947147,"bart_base_moverscore":0.4959640863,"t5_small_bertscore":0.836630404,"t5_small_bartscore":-2.8919947147,"t5_small_moverscore":0.4959640863},"24":{"t5_small_bertscore":0.8681910634,"t5_small_bartscore":-3.4966514111,"t5_small_moverscore":0.5394232128,"bart_cnn_bertscore":0.8586543202,"bart_cnn_bartscore":-2.8848040104,"bart_cnn_moverscore":0.5041785681,"gpt2_bertscore":0.8622639179,"gpt2_bartscore":-3.4545667171,"gpt2_moverscore":0.5538685843,"bart_xsum_bertscore":0.874712944,"bart_xsum_bartscore":-3.7350232601,"bart_xsum_moverscore":0.5168003545,"bart_base_bertscore":0.8837081194,"bart_base_bartscore":-2.9177627563,"bart_base_moverscore":0.5396813898},"25":{"bart_cnn_bertscore":0.8482580185,"bart_cnn_bartscore":-2.5171537399,"bart_cnn_moverscore":0.5493200794,"bart_xsum_bertscore":0.8433896899,"bart_xsum_bartscore":-3.3968565464,"bart_xsum_moverscore":0.5144724666,"t5_small_bertscore":0.8385513425,"t5_small_bartscore":-3.5011427402,"t5_small_moverscore":0.5171401802,"gpt2_bertscore":0.8410636783,"gpt2_bartscore":-2.6980655193,"gpt2_moverscore":0.5183862039,"bart_base_bertscore":0.8410634995,"bart_base_bartscore":-2.9407646656,"bart_base_moverscore":0.5183862039},"26":{"bart_base_bertscore":0.8497838974,"bart_base_bartscore":-3.2515616417,"bart_base_moverscore":0.5157117537,"bart_cnn_bertscore":0.8799037933,"bart_cnn_bartscore":-2.8128724098,"bart_cnn_moverscore":0.5759554987,"t5_small_bertscore":0.8646218777,"t5_small_bartscore":-3.1743240356,"t5_small_moverscore":0.5396361376,"gpt2_bertscore":0.8402597308,"gpt2_bartscore":-3.4757723808,"gpt2_moverscore":0.5076437901,"pegasus_xsum_bertscore":0.847382009,"pegasus_xsum_bartscore":-4.0057063103,"pegasus_xsum_moverscore":0.5193274237},"27":{"bart_xsum_bertscore":0.848290503,"bart_xsum_bartscore":-2.682623148,"bart_xsum_moverscore":0.5086911523,"gpt2_bertscore":0.848290503,"gpt2_bartscore":-3.1667115688,"gpt2_moverscore":0.5086911523,"t5_small_bertscore":0.848290503,"t5_small_bartscore":-2.682623148,"t5_small_moverscore":0.5086911523,"bart_base_bertscore":0.8512022495,"bart_base_bartscore":-2.8328816891,"bart_base_moverscore":0.5051016507,"pegasus_xsum_bertscore":0.848290503,"pegasus_xsum_bartscore":-2.682623148,"pegasus_xsum_moverscore":0.5086911523},"28":{"t5_small_bertscore":0.8707299829,"t5_small_bartscore":-2.5371367931,"t5_small_moverscore":0.5140664544,"pegasus_xsum_bertscore":0.8723821044,"pegasus_xsum_bartscore":-2.2739274502,"pegasus_xsum_moverscore":0.5331604534,"bart_base_bertscore":0.8733096123,"bart_base_bartscore":-2.7069356441,"bart_base_moverscore":0.5117965003,"gpt2_bertscore":0.8796161413,"gpt2_bartscore":-1.9070670605,"gpt2_moverscore":0.5294156538,"bart_cnn_bertscore":0.8812163472,"bart_cnn_bartscore":-2.2491896152,"bart_cnn_moverscore":0.5389984317},"29":{"gpt2_bertscore":0.8206763268,"gpt2_bartscore":-2.5632135868,"gpt2_moverscore":0.4939396377,"bart_xsum_bertscore":0.8209111094,"bart_xsum_bartscore":-3.1314275265,"bart_xsum_moverscore":0.4952604659,"bart_base_bertscore":0.8319789767,"bart_base_bartscore":-2.2686254978,"bart_base_moverscore":0.4987655953,"t5_small_bertscore":0.8319789767,"t5_small_bartscore":-2.2686254978,"t5_small_moverscore":0.4987655953,"pegasus_xsum_bertscore":0.820114255,"pegasus_xsum_bartscore":-2.9853947163,"pegasus_xsum_moverscore":0.4934468757},"30":{"bart_xsum_bertscore":0.8469656706,"bart_xsum_bartscore":-3.1484298706,"bart_xsum_moverscore":0.4978483365,"pegasus_xsum_bertscore":0.8770156503,"pegasus_xsum_bartscore":-3.7651743889,"pegasus_xsum_moverscore":0.534693682,"bart_cnn_bertscore":0.8927406073,"bart_cnn_bartscore":-2.797157526,"bart_cnn_moverscore":0.5582831354,"gpt2_bertscore":0.866807878,"gpt2_bartscore":-3.5523424149,"gpt2_moverscore":0.5118439912,"t5_small_bertscore":0.8758634329,"t5_small_bartscore":-3.2870364189,"t5_small_moverscore":0.5155244098},"31":{"bart_cnn_bertscore":0.8705776334,"bart_cnn_bartscore":-2.850502491,"bart_cnn_moverscore":0.5654468652,"bart_xsum_bertscore":0.874979794,"bart_xsum_bartscore":-2.5928368568,"bart_xsum_moverscore":0.5315851868,"pegasus_xsum_bertscore":0.8810817003,"pegasus_xsum_bartscore":-2.0981600285,"pegasus_xsum_moverscore":0.5490025701,"gpt2_bertscore":0.8576949835,"gpt2_bartscore":-2.924939394,"gpt2_moverscore":0.5066060606,"t5_small_bertscore":0.8438516855,"t5_small_bartscore":-2.5286579132,"t5_small_moverscore":0.5281423816},"32":{"t5_small_bertscore":0.8424543738,"t5_small_bartscore":-3.4472846985,"t5_small_moverscore":0.524557744,"pegasus_xsum_bertscore":0.8502164483,"pegasus_xsum_bartscore":-3.8056046963,"pegasus_xsum_moverscore":0.5276893454,"bart_xsum_bertscore":0.868304193,"bart_xsum_bartscore":-2.2641274929,"bart_xsum_moverscore":0.5140134801,"gpt2_bertscore":0.8589553237,"gpt2_bartscore":-2.6333725452,"gpt2_moverscore":0.5230861771,"bart_base_bertscore":0.8589553237,"bart_base_bartscore":-2.8228971958,"bart_base_moverscore":0.5230861771},"33":{"bart_cnn_bertscore":0.8353241682,"bart_cnn_bartscore":-4.6781182289,"bart_cnn_moverscore":0.5117872302,"bart_xsum_bertscore":0.8544940352,"bart_xsum_bartscore":-2.5461239815,"bart_xsum_moverscore":0.5430375742,"gpt2_bertscore":0.8564044833,"gpt2_bartscore":-2.1599025726,"gpt2_moverscore":0.5284224888,"bart_base_bertscore":0.8507292867,"bart_base_bartscore":-2.9662439823,"bart_base_moverscore":0.5204351684,"pegasus_xsum_bertscore":0.854039073,"pegasus_xsum_bartscore":-2.4347229004,"pegasus_xsum_moverscore":0.52329693},"34":{"bart_cnn_bertscore":0.8490822315,"bart_cnn_bartscore":-2.9956417084,"bart_cnn_moverscore":0.5416047452,"bart_base_bertscore":0.8385124803,"bart_base_bartscore":-3.3690867424,"bart_base_moverscore":0.5280557353,"gpt2_bertscore":0.838267386,"gpt2_bartscore":-3.8187170029,"gpt2_moverscore":0.5185314285,"pegasus_xsum_bertscore":0.8128473759,"pegasus_xsum_bartscore":-4.2311558723,"pegasus_xsum_moverscore":0.4927290067,"bart_xsum_bertscore":0.8502987623,"bart_xsum_bartscore":-3.8885219097,"bart_xsum_moverscore":0.5154596474},"35":{"pegasus_xsum_bertscore":0.8359262943,"pegasus_xsum_bartscore":-3.2927708626,"pegasus_xsum_moverscore":0.4936692098,"gpt2_bertscore":0.8561435938,"gpt2_bartscore":-3.2812712193,"gpt2_moverscore":0.5037259355,"bart_base_bertscore":0.8334531188,"bart_base_bartscore":-3.55940485,"bart_base_moverscore":0.4887984253,"t5_small_bertscore":0.845108211,"t5_small_bartscore":-2.0151469707,"t5_small_moverscore":0.4967010676,"bart_cnn_bertscore":0.8632298708,"bart_cnn_bartscore":-1.5442458391,"bart_cnn_moverscore":0.4973203585},"36":{"t5_small_bertscore":0.846584022,"t5_small_bartscore":-3.7645082474,"t5_small_moverscore":0.5093860974,"gpt2_bertscore":0.8326331973,"gpt2_bartscore":-4.1543655396,"gpt2_moverscore":0.4883191097,"bart_cnn_bertscore":0.8252484202,"bart_cnn_bartscore":-4.269557476,"bart_cnn_moverscore":0.4971833262,"pegasus_xsum_bertscore":0.8608145118,"pegasus_xsum_bartscore":-2.9543788433,"pegasus_xsum_moverscore":0.5347797632,"bart_base_bertscore":0.8408335447,"bart_base_bartscore":-3.3980710506,"bart_base_moverscore":0.5053091579},"37":{"t5_small_bertscore":0.8521587849,"t5_small_bartscore":-2.6604893208,"t5_small_moverscore":0.5002512131,"bart_cnn_bertscore":0.8452253342,"bart_cnn_bartscore":-2.9644193649,"bart_cnn_moverscore":0.5254722268,"gpt2_bertscore":0.8298243284,"gpt2_bartscore":-2.2012536526,"gpt2_moverscore":0.4763663064,"pegasus_xsum_bertscore":0.8507897854,"pegasus_xsum_bartscore":-2.4326164722,"pegasus_xsum_moverscore":0.5010045813,"bart_xsum_bertscore":0.8536093831,"bart_xsum_bartscore":-2.2316720486,"bart_xsum_moverscore":0.509579091},"38":{"gpt2_bertscore":0.8522146344,"gpt2_bartscore":-1.7310314178,"gpt2_moverscore":0.498218149,"bart_xsum_bertscore":0.8561927676,"bart_xsum_bartscore":-2.1943538189,"bart_xsum_moverscore":0.5097704614,"bart_cnn_bertscore":0.8553366661,"bart_cnn_bartscore":-2.1605305672,"bart_cnn_moverscore":0.5195677114,"t5_small_bertscore":0.852214694,"t5_small_bartscore":-1.5106146336,"t5_small_moverscore":0.498218149,"bart_base_bertscore":0.8528027534,"bart_base_bartscore":-2.4854578972,"bart_base_moverscore":0.4971475892},"39":{"bart_xsum_bertscore":0.8668097258,"bart_xsum_bartscore":-2.35247159,"bart_xsum_moverscore":0.5171574033,"gpt2_bertscore":0.8530034423,"gpt2_bartscore":-3.5493516922,"gpt2_moverscore":0.4850733185,"t5_small_bertscore":0.8620704412,"t5_small_bartscore":-2.345143795,"t5_small_moverscore":0.4990888177,"bart_cnn_bertscore":0.8677952886,"bart_cnn_bartscore":-2.5522534847,"bart_cnn_moverscore":0.5341812826,"pegasus_xsum_bertscore":0.8562350273,"pegasus_xsum_bartscore":-3.2391724586,"pegasus_xsum_moverscore":0.4922542195},"40":{"bart_base_bertscore":0.8433095813,"bart_base_bartscore":-2.3706462383,"bart_base_moverscore":0.4938884619,"bart_cnn_bertscore":0.8451887369,"bart_cnn_bartscore":-2.5807142258,"bart_cnn_moverscore":0.5214267757,"t5_small_bertscore":0.8433095813,"t5_small_bartscore":-2.3706462383,"t5_small_moverscore":0.4938884619,"bart_xsum_bertscore":0.8433095813,"bart_xsum_bartscore":-2.3706462383,"bart_xsum_moverscore":0.4938884619,"gpt2_bertscore":0.8415830731,"gpt2_bartscore":-2.9237039089,"gpt2_moverscore":0.4919809677},"41":{"bart_cnn_bertscore":0.8562602401,"bart_cnn_bartscore":-2.837458849,"bart_cnn_moverscore":0.5452744311,"bart_xsum_bertscore":0.8648011684,"bart_xsum_bartscore":-2.2500948906,"bart_xsum_moverscore":0.5221797783,"pegasus_xsum_bertscore":0.8560605645,"pegasus_xsum_bartscore":-3.8179633617,"pegasus_xsum_moverscore":0.5049397718,"bart_base_bertscore":0.8560510278,"bart_base_bartscore":-2.6071047783,"bart_base_moverscore":0.5158065854,"t5_small_bertscore":0.8527100682,"t5_small_bartscore":-2.7361195087,"t5_small_moverscore":0.5145628728},"42":{"pegasus_xsum_bertscore":0.8555977345,"pegasus_xsum_bartscore":-2.5741152763,"pegasus_xsum_moverscore":0.5302299268,"bart_xsum_bertscore":0.8474509716,"bart_xsum_bartscore":-2.1807386875,"bart_xsum_moverscore":0.537465244,"t5_small_bertscore":0.8592894077,"t5_small_bartscore":-1.7803932428,"t5_small_moverscore":0.5339067123,"bart_base_bertscore":0.8206448555,"bart_base_bartscore":-4.3610482216,"bart_base_moverscore":0.5135526203,"bart_cnn_bertscore":0.8641557693,"bart_cnn_bartscore":-1.8710056543,"bart_cnn_moverscore":0.5372975283},"43":{"t5_small_bertscore":0.8454937339,"t5_small_bartscore":-4.1952519417,"t5_small_moverscore":0.5201804585,"gpt2_bertscore":0.8551383615,"gpt2_bartscore":-2.6727416515,"gpt2_moverscore":0.5305036881,"bart_base_bertscore":0.8482090235,"bart_base_bartscore":-3.2421314716,"bart_base_moverscore":0.5174157831,"bart_cnn_bertscore":0.8475337625,"bart_cnn_bartscore":-2.7497122288,"bart_cnn_moverscore":0.5094216493,"bart_xsum_bertscore":0.8550805449,"bart_xsum_bartscore":-2.5391817093,"bart_xsum_moverscore":0.5468035268},"44":{"bart_xsum_bertscore":0.8503565788,"bart_xsum_bartscore":-2.9206902981,"bart_xsum_moverscore":0.5243240077,"t5_small_bertscore":0.8344270587,"t5_small_bartscore":-4.4234275818,"t5_small_moverscore":0.494249628,"bart_base_bertscore":0.8550501466,"bart_base_bartscore":-2.8257865906,"bart_base_moverscore":0.5115678608,"bart_cnn_bertscore":0.8379640579,"bart_cnn_bartscore":-5.2314891815,"bart_cnn_moverscore":0.5046008111,"pegasus_xsum_bertscore":0.8428795338,"pegasus_xsum_bartscore":-3.7155795097,"pegasus_xsum_moverscore":0.5371041865},"45":{"t5_small_bertscore":0.8407766223,"t5_small_bartscore":-3.572685957,"t5_small_moverscore":0.4975244182,"bart_base_bertscore":0.8453595042,"bart_base_bartscore":-4.1235513687,"bart_base_moverscore":0.4972455208,"bart_xsum_bertscore":0.8397783637,"bart_xsum_bartscore":-3.503923893,"bart_xsum_moverscore":0.5079003131,"pegasus_xsum_bertscore":0.8518351316,"pegasus_xsum_bartscore":-2.7405292988,"pegasus_xsum_moverscore":0.512997781,"bart_cnn_bertscore":0.8373824954,"bart_cnn_bartscore":-3.8228185177,"bart_cnn_moverscore":0.5010833365},"46":{"gpt2_bertscore":0.8406467438,"gpt2_bartscore":-3.0881454945,"gpt2_moverscore":0.5031549328,"t5_small_bertscore":0.8518951535,"t5_small_bartscore":-2.5179321766,"t5_small_moverscore":0.5088294334,"pegasus_xsum_bertscore":0.8434832692,"pegasus_xsum_bartscore":-2.5916960239,"pegasus_xsum_moverscore":0.5068330068,"bart_cnn_bertscore":0.8482115269,"bart_cnn_bartscore":-2.5682432652,"bart_cnn_moverscore":0.5061747498,"bart_xsum_bertscore":0.8462671041,"bart_xsum_bartscore":-3.4292490482,"bart_xsum_moverscore":0.5325117887},"47":{"bart_base_bertscore":0.8099844456,"bart_base_bartscore":-4.0481734276,"bart_base_moverscore":0.5031726844,"bart_cnn_bertscore":0.8274171948,"bart_cnn_bartscore":-2.7289068699,"bart_cnn_moverscore":0.5027591164,"pegasus_xsum_bertscore":0.8368241787,"pegasus_xsum_bartscore":-3.0687978268,"pegasus_xsum_moverscore":0.5266172151,"gpt2_bertscore":0.8403365612,"gpt2_bartscore":-2.3108067513,"gpt2_moverscore":0.5130290435,"bart_xsum_bertscore":0.8234208822,"bart_xsum_bartscore":-3.6255483627,"bart_xsum_moverscore":0.4946231319},"48":{"gpt2_bertscore":0.8590996861,"gpt2_bartscore":-4.0719590187,"gpt2_moverscore":0.5137559675,"bart_xsum_bertscore":0.8614172935,"bart_xsum_bartscore":-2.2755560875,"bart_xsum_moverscore":0.5209935026,"t5_small_bertscore":0.8352868557,"t5_small_bartscore":-3.6230068207,"t5_small_moverscore":0.5236643,"bart_base_bertscore":0.8574666381,"bart_base_bartscore":-3.0245771408,"bart_base_moverscore":0.5063472423,"pegasus_xsum_bertscore":0.8773747087,"pegasus_xsum_bartscore":-2.1398639679,"pegasus_xsum_moverscore":0.5585441054},"49":{"gpt2_bertscore":0.8621393442,"gpt2_bartscore":-3.8185436726,"gpt2_moverscore":0.5077184336,"t5_small_bertscore":0.8655979037,"t5_small_bartscore":-3.3371090889,"t5_small_moverscore":0.5112545487,"bart_base_bertscore":0.8714956641,"bart_base_bartscore":-3.0146062374,"bart_base_moverscore":0.5243292453,"bart_cnn_bertscore":0.8539739847,"bart_cnn_bartscore":-4.0811867714,"bart_cnn_moverscore":0.5239048696,"pegasus_xsum_bertscore":0.872569859,"pegasus_xsum_bartscore":-3.6484210491,"pegasus_xsum_moverscore":0.5559388204}}}